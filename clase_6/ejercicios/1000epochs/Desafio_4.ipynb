{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc6qGGCW2agk"
      },
      "source": [
        "<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n",
        "\n",
        "# **Desafío 4:** *LSTM Bot QA*\n",
        "## Procesamiento del Lenguaje Natural I\n",
        "### Carrera de Especialización en Inteligencia Artificial - Cohorte 17\n",
        "##### Docentes: Rodrigo Cárdenas / Nicolás  Vattuone\n",
        "#### Autor: Alejandro Lloveras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJeNRwkG2agl"
      },
      "source": [
        "### Consigna\n",
        "- Construir un BOT para responder a preguntas del usuario (QA).\n",
        "- Utilizar datos disponibles del challenge **ConvAI2** *(Conversational Intelligence Challenge 2)* de conversaciones en inglés.\n",
        "- Preparar los embeddings para transformar los tokens de entrada en vectores\n",
        "- Entrenar un modelo basado en el esquema encoder-decoder utilizando los datos generados.\n",
        "- Experimentar el funcionamiento del modelo. Realizar la inferencia de los modelos por separado de encoder y decoder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqO0PRcFsPTe"
      },
      "source": [
        "### Datos\n",
        "El objecto es utilizar datos disponibles del challenge **ConvAI2** *(Conversational Intelligence Challenge 2)* de conversaciones en inglés. Se construirá un BOT para responder a preguntas del usuario (QA).\n",
        "\n",
        "[Dataset](http://convai.io/data/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoaoF6Zh2agm"
      },
      "source": [
        "### Inicialización"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bDFC0I3j9oFD"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --no-cache-dir gdown --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cq3YXak9sGHd"
      },
      "outputs": [],
      "source": [
        "# Librerías estándar\n",
        "import os\n",
        "import re\n",
        "import logging\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "from io import StringIO\n",
        "\n",
        "# Manipulación y Análisis de Datos\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# PyTorch Data Handling\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Preprocesamiento de Texto\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer, one_hot\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "\n",
        "# División de Datos\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Utilidades Keras/TensorFlow\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Construcción de Modelos y Capas Core (TensorFlow/Keras)\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Activation, Dropout, Dense, Embedding, Flatten, Input, LSTM, SimpleRNN\n",
        "from tensorflow.keras.models import Model, Sequential"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vELUyQ8bDHj6",
        "outputId": "943ca389-d4f2-473a-9e88-b66bf29f11f9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cuda = torch.cuda.is_available()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-KZy5SyDHj6",
        "outputId": "69a5394d-cf8f-489b-d859-8609b81431fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ],
      "source": [
        "# torchsummar actualmente tiene un problema con las LSTM, por eso\n",
        "# se utiliza torchinfo, un fork del proyecto original con el bug solucionado\n",
        "!pip3 install torchinfo\n",
        "from torchinfo import summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGgRcirwDHj7",
        "outputId": "94d2e8e8-34f7-4324-c87e-e37af6b68ec6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-04-24 03:17:10--  http://torch_helpers.py/\n",
            "Resolving torch_helpers.py (torch_helpers.py)... failed: Name or service not known.\n",
            "wget: unable to resolve host address ‘torch_helpers.py’\n",
            "--2025-04-24 03:17:10--  https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/scripts/torch_helpers.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 23883 (23K) [text/plain]\n",
            "Saving to: ‘torch_helpers.py’\n",
            "\n",
            "torch_helpers.py    100%[===================>]  23.32K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2025-04-24 03:17:11 (18.2 MB/s) - ‘torch_helpers.py’ saved [23883/23883]\n",
            "\n",
            "FINISHED --2025-04-24 03:17:11--\n",
            "Total wall clock time: 0.4s\n",
            "Downloaded: 1 files, 23K in 0.001s (18.2 MB/s)\n"
          ]
        }
      ],
      "source": [
        "import platform\n",
        "\n",
        "if os.access('torch_helpers.py', os.F_OK) is False:\n",
        "    if platform.system() == 'Windows':\n",
        "        !curl !wget https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/scripts/torch_helpers.py > torch_helpers.py\n",
        "    else:\n",
        "        !wget torch_helpers.py https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/scripts/torch_helpers.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoRzdNBEDHj8"
      },
      "outputs": [],
      "source": [
        "def sequence_acc(y_pred, y_test):\n",
        "    y_pred_tag = y_pred.data.max(dim=-1,keepdim=True)[1]\n",
        "    y_test_tag = y_test.data.max(dim=-1,keepdim=True)[1]\n",
        "\n",
        "    batch_size = y_pred_tag.shape[0]\n",
        "    batch_acc = torch.zeros(batch_size)\n",
        "    for b in range(batch_size):\n",
        "        correct_results_sum = (y_pred_tag[b] == y_test_tag[b]).sum().float()\n",
        "        batch_acc[b] = correct_results_sum / y_pred_tag[b].shape[0]\n",
        "\n",
        "    correct_results_sum = batch_acc.sum().float()\n",
        "    acc = correct_results_sum / batch_size\n",
        "    return acc\n",
        "\n",
        "def train(model, train_loader, valid_loader, optimizer, criterion, epochs=100):\n",
        "    # Defino listas para realizar graficas de los resultados\n",
        "    train_loss = []\n",
        "    train_accuracy = []\n",
        "    valid_loss = []\n",
        "    valid_accuracy = []\n",
        "\n",
        "    # Defino mi loop de entrenamiento\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        epoch_train_loss = 0.0\n",
        "        epoch_train_accuracy = 0.0\n",
        "\n",
        "        for train_encoder_input, train_decoder_input, train_target in train_loader:\n",
        "            # Seteo los gradientes en cero ya que, por defecto, PyTorch los va acumulando\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = model(train_encoder_input.to(device), train_decoder_input.to(device))\n",
        "\n",
        "            # Computo el error de la salida comparando contra las etiquetas\n",
        "            # por cada token en cada batch (sequence_loss)\n",
        "            loss = 0\n",
        "            for t in range(train_decoder_input.shape[1]):\n",
        "                loss += criterion(output[:, t, :], train_target[:, t, :])\n",
        "\n",
        "            # Almaceno el error del batch para luego tener el error promedio de la epoca\n",
        "            epoch_train_loss += loss.item()\n",
        "\n",
        "            # Computo el nuevo set de gradientes a lo largo de toda la red\n",
        "            loss.backward()\n",
        "\n",
        "            # Realizo el paso de optimizacion actualizando los parametros de toda la red\n",
        "            optimizer.step()\n",
        "\n",
        "            # Calculo el accuracy del batch\n",
        "            accuracy = sequence_acc(output, train_target)\n",
        "            # Almaceno el accuracy del batch para luego tener el accuracy promedio de la epoca\n",
        "            epoch_train_accuracy += accuracy.item()\n",
        "\n",
        "        # Calculo la media de error para la epoca de entrenamiento.\n",
        "        # La longitud de train_loader es igual a la cantidad de batches dentro de una epoca.\n",
        "        epoch_train_loss = epoch_train_loss / len(train_loader)\n",
        "        train_loss.append(epoch_train_loss)\n",
        "        epoch_train_accuracy = epoch_train_accuracy / len(train_loader)\n",
        "        train_accuracy.append(epoch_train_accuracy)\n",
        "\n",
        "        # Realizo el paso de validación computando error y accuracy, y\n",
        "        # almacenando los valores para imprimirlos y graficarlos\n",
        "        #valid_encoder_input, valid_decoder_input, valid_target = iter(valid_loader).next()\n",
        "        valid_encoder_input, valid_decoder_input, valid_target = next(iter(valid_loader))\n",
        "        output = model(valid_encoder_input.to(device), valid_decoder_input.to(device))\n",
        "\n",
        "        epoch_valid_loss = 0\n",
        "        for t in range(train_decoder_input.shape[1]):\n",
        "                epoch_valid_loss += criterion(output[:, t, :], valid_target[:, t, :])\n",
        "        epoch_valid_loss = epoch_valid_loss.item()\n",
        "\n",
        "        valid_loss.append(epoch_valid_loss)\n",
        "\n",
        "        # Calculo el accuracy de la epoch\n",
        "        epoch_valid_accuracy = sequence_acc(output, valid_target).item()\n",
        "        valid_accuracy.append(epoch_valid_accuracy)\n",
        "\n",
        "        print(f\"Epoch: {epoch+1}/{epochs} - Train loss {epoch_train_loss:.3f} - Train accuracy {epoch_train_accuracy:.3f} - Valid Loss {epoch_valid_loss:.3f} - Valid accuracy {epoch_valid_accuracy:.3f}\")\n",
        "\n",
        "    history = {\n",
        "        \"loss\": train_loss,\n",
        "        \"accuracy\": train_accuracy,\n",
        "        \"val_loss\": valid_loss,\n",
        "        \"val_accuracy\": valid_accuracy,\n",
        "    }\n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_kn51y2JJpq"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, valid_loader, optimizer, criterion, epochs=100):\n",
        "    # Defino listas para realizar graficas de los resultados\n",
        "    train_loss = []\n",
        "    train_accuracy = []\n",
        "    valid_loss = []\n",
        "    valid_accuracy = []\n",
        "\n",
        "    # Defino mi loop de entrenamiento\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        epoch_train_loss = 0.0\n",
        "        epoch_train_accuracy = 0.0\n",
        "\n",
        "        # Inicia el bucle de entrenamiento por batch\n",
        "        for train_encoder_input, train_decoder_input, train_target in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = model(train_encoder_input.to(device), train_decoder_input.to(device))\n",
        "\n",
        "            # Computo el error del batch - INEFICIENTE BUCLE t\n",
        "            loss = 0\n",
        "            for t in range(train_decoder_input.shape[1]):\n",
        "                loss += criterion(output[:, t, :], train_target[:, t, :])\n",
        "\n",
        "            epoch_train_loss += loss.item() # Acumula pérdida por batch\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Calculo el accuracy del batch\n",
        "            accuracy = sequence_acc(output, train_target)\n",
        "            epoch_train_accuracy += accuracy.item() # Acumula accuracy por batch\n",
        "\n",
        "        # Calcula la media de metrics por epoca\n",
        "        epoch_train_loss = epoch_train_loss / len(train_loader)\n",
        "        train_loss.append(epoch_train_loss)\n",
        "        epoch_train_accuracy = epoch_train_accuracy / len(train_loader)\n",
        "        train_accuracy.append(epoch_train_accuracy)\n",
        "\n",
        "        # Realizo el paso de validación - ¡¡SOLO UN BATCH!!\n",
        "        #valid_encoder_input, valid_decoder_input, valid_target = iter(valid_loader).next()\n",
        "        valid_encoder_input, valid_decoder_input, valid_target = next(iter(valid_loader)) # <= PROBLEMA MAYOR\n",
        "\n",
        "        # Falta with torch.no_grad(): y model.eval()\n",
        "        output = model(valid_encoder_input.to(device), valid_decoder_input.to(device))\n",
        "\n",
        "        epoch_valid_loss = 0 # Acumula pérdida por batch\n",
        "        for t in range(train_decoder_input.shape[1]): # INEFICIENTE BUCLE t, y usa shape[1] de train_decoder_input\n",
        "                epoch_valid_loss += criterion(output[:, t, :], valid_target[:, t, :])\n",
        "        epoch_valid_loss = epoch_valid_loss.item() # Solo tiene la pérdida del ULTIMO batch procesado (solo 1 batch)\n",
        "\n",
        "        valid_loss.append(epoch_valid_loss) # Solo añade la pérdida de UN batch\n",
        "\n",
        "        # Calculo el accuracy de la epoch - ¡¡SOLO UN BATCH!!\n",
        "        epoch_valid_accuracy = sequence_acc(output, valid_target).item() # Solo tiene el accuracy de UN batch\n",
        "        valid_accuracy.append(epoch_valid_accuracy)\n",
        "\n",
        "        print(f\"Epoch: {epoch+1}/{epochs} - Train loss {epoch_train_loss:.3f} - Train accuracy {epoch_train_accuracy:.3f} - Valid Loss {epoch_valid_loss:.3f} - Valid accuracy {epoch_valid_accuracy:.3f}\")\n",
        "\n",
        "    history = {\n",
        "        \"loss\": train_loss,\n",
        "        \"accuracy\": train_accuracy,\n",
        "        \"val_loss\": valid_loss,\n",
        "        \"val_accuracy\": valid_accuracy,\n",
        "    }\n",
        "    return history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Guardado de resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Pesos del modelo guardados en model_weights.pth\n"
          ]
        }
      ],
      "source": [
        "# Definimos una ruta para guardar los pesos\n",
        "filepath = 'model_weights.pth'\n",
        "\n",
        "# Guardamos el diccionario de estados\n",
        "torch.save(model.state_dict(), filepath)\n",
        "\n",
        "print(f\"✅ Pesos del modelo guardados en {filepath}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training history saved to training_history.csv\n"
          ]
        }
      ],
      "source": [
        "# Definimos la ruta para guardar el historial en un archivo CSV\n",
        "history_filepath = 'training_history.csv'\n",
        "\n",
        "# Creamos un DataFrame de pandas\n",
        "df_history = pd.DataFrame(history1)\n",
        "\n",
        "# Guardamos el DataFrame en un archivo CSV\n",
        "df_history.to_csv(history_filepath, index=False)\n",
        "\n",
        "print(f\"✅ Historial de entrenamiento guardado en {history_filepath}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Inferencia\n",
        "Se experimenta el funcionamiento del modelo. Se realiza la inferencia de los modelos por separado de encoder y decoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model weights loaded from model_weights (2).pth\n"
          ]
        }
      ],
      "source": [
        "# Cargamos los pesos del modelo guardados\n",
        "model_weights_path = 'model_weights.pth'\n",
        "model.load_state_dict(torch.load(model_weights_path))\n",
        "\n",
        "print(f\"✅ Pesos del modelo cargados desde {model_weights_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWA4sTXH2agn"
      },
      "source": [
        "## 1. Descarga del dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHNkUaPp6aYq",
        "outputId": "31cd7065-f604-421a-bda8-13073a3517e8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1awUxYwImF84MIT5-jCaYAPe2QwSgS1hN&export=download\n",
            "To: /content/data_volunteers.json\n",
            "100%|██████████| 2.58M/2.58M [00:00<00:00, 72.4MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Descargar la carpeta de dataset\n",
        "import gdown\n",
        "if os.access('data_volunteers.json', os.F_OK) is False:\n",
        "    url = 'https://drive.google.com/uc?id=1awUxYwImF84MIT5-jCaYAPe2QwSgS1hN&export=download'\n",
        "    output = 'data_volunteers.json'\n",
        "    gdown.download(url, output, quiet=False)\n",
        "else:\n",
        "    print(\"✅ El dataset ya se encuentra descargado\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WZy1-wgG-Rp7"
      },
      "outputs": [],
      "source": [
        "# dataset_file\n",
        "import json\n",
        "\n",
        "text_file = \"data_volunteers.json\"\n",
        "with open(text_file) as f:\n",
        "    data = json.load(f) # la variable data será un diccionario"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ue5qd54S-eew",
        "outputId": "e1a3b90d-2c45-4061-bf4b-92c593c4b046"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['dialog', 'start_time', 'end_time', 'bot_profile', 'user_profile', 'eval_score', 'profile_match', 'participant1_id', 'participant2_id'])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Observar los campos disponibles en cada linea del dataset\n",
        "data[0].keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHBRAXPl-3dz",
        "outputId": "15511930-a493-4827-d818-2f2301ac7584"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "▪ Cantidad de rows utilizadas: 6033\n"
          ]
        }
      ],
      "source": [
        "chat_in = []\n",
        "chat_out = []\n",
        "\n",
        "input_sentences = []\n",
        "output_sentences = []\n",
        "output_sentences_inputs = []\n",
        "max_len = 30\n",
        "\n",
        "def clean_text(txt):\n",
        "    txt = txt.lower()\n",
        "    txt.replace(\"\\'d\", \" had\")\n",
        "    txt.replace(\"\\'s\", \" is\")\n",
        "    txt.replace(\"\\'m\", \" am\")\n",
        "    txt.replace(\"don't\", \"do not\")\n",
        "    txt = re.sub(r'\\W+', ' ', txt)\n",
        "\n",
        "    return txt\n",
        "\n",
        "for line in data:\n",
        "    for i in range(len(line['dialog'])-1):\n",
        "        # vamos separando el texto en \"preguntas\" (chat_in)\n",
        "        # y \"respuestas\" (chat_out)\n",
        "        chat_in = clean_text(line['dialog'][i]['text'])\n",
        "        chat_out = clean_text(line['dialog'][i+1]['text'])\n",
        "\n",
        "        if len(chat_in) >= max_len or len(chat_out) >= max_len:\n",
        "            continue\n",
        "\n",
        "        input_sentence, output = chat_in, chat_out\n",
        "\n",
        "        # output sentence (decoder_output) tiene <eos>\n",
        "        output_sentence = output + ' <eos>'\n",
        "        # output sentence input (decoder_input) tiene <sos>\n",
        "        output_sentence_input = '<sos> ' + output\n",
        "\n",
        "        input_sentences.append(input_sentence)\n",
        "        output_sentences.append(output_sentence)\n",
        "        output_sentences_inputs.append(output_sentence_input)\n",
        "\n",
        "print(\"▪ Cantidad de rows utilizadas:\", len(input_sentences))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07L1qj8pC_l6",
        "outputId": "8aac7725-3f3e-40da-aa1d-7bb4e69a3f6d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('hi how are you ', 'not bad and you  <eos>', '<sos> not bad and you ')"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_sentences[1], output_sentences[1], output_sentences_inputs[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8P-ynUNP5xp6"
      },
      "source": [
        "## 2. Preprocesamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "VxbAcfSp2agr"
      },
      "outputs": [],
      "source": [
        "# Definir el tamaño máximo del vocabulario\n",
        "MAX_VOCAB_SIZE = 8000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24Ore_JP2agr"
      },
      "source": [
        "### 2.1 Datos de Entrada:\n",
        "Calculo de `word2idx_inputs`, `max_input_len`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Ws7bjGj2vVI",
        "outputId": "23f35f26-d34e-450d-cd02-c7b3c042f40e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torch_helpers\n",
            "  Downloading torch_helpers-0.4.6-py3-none-any.whl.metadata (598 bytes)\n",
            "Downloading torch_helpers-0.4.6-py3-none-any.whl (5.2 kB)\n",
            "Installing collected packages: torch_helpers\n",
            "Successfully installed torch_helpers-0.4.6\n",
            "Collecting moleskin\n",
            "  Downloading moleskin-1.5.1-py3-none-any.whl.metadata (701 bytes)\n",
            "Collecting boltons\n",
            "  Downloading boltons-25.0.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
            "INFO: pip is looking at multiple versions of moleskin to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting moleskin\n",
            "  Downloading moleskin-1.5.0-py3-none-any.whl.metadata (701 bytes)\n",
            "  Downloading moleskin-1.4.0-py3-none-any.whl.metadata (735 bytes)\n",
            "  Downloading moleskin-1.3.1-py3-none-any.whl.metadata (735 bytes)\n",
            "  Downloading moleskin-1.3.0-py3-none-any.whl.metadata (701 bytes)\n",
            "  Downloading moleskin-1.2.0-py3-none-any.whl.metadata (701 bytes)\n",
            "  Downloading moleskin-1.1.1-py3-none-any.whl.metadata (701 bytes)\n",
            "  Downloading moleskin-1.1.0-py3-none-any.whl.metadata (701 bytes)\n",
            "INFO: pip is still looking at multiple versions of moleskin to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading moleskin-1.0.0-py3-none-any.whl.metadata (678 bytes)\n",
            "  Downloading moleskin-0.1.1-py3-none-any.whl.metadata (678 bytes)\n",
            "  Downloading moleskin-0.1.0-py3-none-any.whl.metadata (678 bytes)\n",
            "  Downloading moleskin-0.0.8-py3-none-any.whl.metadata (590 bytes)\n",
            "  Downloading moleskin-0.0.7-py3-none-any.whl.metadata (573 bytes)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading moleskin-0.0.6-py3-none-any.whl.metadata (573 bytes)\n",
            "  Downloading moleskin-0.0.5-py3-none-any.whl.metadata (549 bytes)\n",
            "  Downloading moleskin-0.0.4-py3-none-any.whl.metadata (549 bytes)\n",
            "  Downloading moleskin-0.0.3-py3-none-any.whl.metadata (549 bytes)\n",
            "  Downloading moleskin-0.0.2-py3-none-any.whl.metadata (549 bytes)\n",
            "  Downloading moleskin-0.0.1-py3-none-any.whl.metadata (549 bytes)\n",
            "  Downloading moleskin-0.0.0-py3-none-any.whl.metadata (549 bytes)\n",
            "\u001b[31mERROR: Cannot install moleskin==0.0.0, moleskin==0.0.1, moleskin==0.0.2, moleskin==0.0.3, moleskin==0.0.4, moleskin==0.0.5, moleskin==0.0.6, moleskin==0.0.7, moleskin==0.0.8, moleskin==0.1.0, moleskin==0.1.1, moleskin==1.0.0, moleskin==1.1.0, moleskin==1.1.1, moleskin==1.2.0, moleskin==1.3.0, moleskin==1.3.1, moleskin==1.4.0, moleskin==1.5.0 and moleskin==1.5.1 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "The conflict is caused by:\n",
            "    moleskin 1.5.1 depends on pprint\n",
            "    moleskin 1.5.0 depends on pprint\n",
            "    moleskin 1.4.0 depends on pprint\n",
            "    moleskin 1.3.1 depends on pprint\n",
            "    moleskin 1.3.0 depends on pprint\n",
            "    moleskin 1.2.0 depends on pprint\n",
            "    moleskin 1.1.1 depends on pprint\n",
            "    moleskin 1.1.0 depends on pprint\n",
            "    moleskin 1.0.0 depends on pprint\n",
            "    moleskin 0.1.1 depends on pprint\n",
            "    moleskin 0.1.0 depends on pprint\n",
            "    moleskin 0.0.8 depends on pprint\n",
            "    moleskin 0.0.7 depends on pprint\n",
            "    moleskin 0.0.6 depends on pprint\n",
            "    moleskin 0.0.5 depends on pprint\n",
            "    moleskin 0.0.4 depends on pprint\n",
            "    moleskin 0.0.3 depends on pprint\n",
            "    moleskin 0.0.2 depends on pprint\n",
            "    moleskin 0.0.1 depends on pprint\n",
            "    moleskin 0.0.0 depends on pprint\n",
            "\n",
            "To fix this you could try to:\n",
            "1. loosen the range of package versions you've specified\n",
            "2. remove package versions to allow pip to attempt to solve the dependency conflict\n",
            "\n",
            "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install --no-deps torch_helpers\n",
        "!pip install moleskin boltons termcolor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80BpTOK02agr",
        "outputId": "3e16257d-2d0c-463a-9a15-e9e134c3d0f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "▪ Palabras en el vocabulario: 1799\n",
            "▪ Oración de entrada más larga: 9\n"
          ]
        }
      ],
      "source": [
        "# Tokenizar las palabras con el Tokenizer de Keras\n",
        "# Definir una máxima cantidad de palabras a utilizar:\n",
        "# - num_words --> the maximum number of words to keep, based on word frequency.\n",
        "# - Only the most common num_words-1 words will be kept.\n",
        "input_tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
        "input_tokenizer.fit_on_texts(input_sentences)\n",
        "input_integer_seq = input_tokenizer.texts_to_sequences(input_sentences)\n",
        "\n",
        "word2idx_inputs = input_tokenizer.word_index\n",
        "print(\"▪ Palabras en el vocabulario:\", len(word2idx_inputs))\n",
        "\n",
        "max_input_len = max(len(sen) for sen in input_integer_seq)\n",
        "print(\"▪ Oración de entrada más larga:\", max_input_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70aDXToX2agr"
      },
      "source": [
        "### 2.2 Datos de Salida:\n",
        "Calculo de `word2idx_outputs`, `max_out_len`, `num_words_output`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnXtc5CR2agr",
        "outputId": "94f2d594-0b20-4e28-93a2-c314741579f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "▪ Palabras en el vocabulario: 1806\n",
            "▪ Oración de salida más larga: 10\n"
          ]
        }
      ],
      "source": [
        "# A los filtros de símbolos del Tokenizer agregamos el \"¿\",\n",
        "# sacamos los \"<>\" para que no afectar nuestros tokens\n",
        "output_tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, filters='!\"#$%&()*+,-./:;=¿?@[\\\\]^_`{|}~\\t\\n')\n",
        "output_tokenizer.fit_on_texts([\"<sos>\", \"<eos>\"] + output_sentences)\n",
        "output_integer_seq = output_tokenizer.texts_to_sequences(output_sentences)\n",
        "output_input_integer_seq = output_tokenizer.texts_to_sequences(output_sentences_inputs)\n",
        "\n",
        "word2idx_outputs = output_tokenizer.word_index\n",
        "print(\"▪ Palabras en el vocabulario:\", len(word2idx_outputs))\n",
        "\n",
        "num_words_output = min(len(word2idx_outputs) + 1, MAX_VOCAB_SIZE) # Se suma 1 por el primer <sos>\n",
        "max_out_len = max(len(sen) for sen in output_integer_seq)\n",
        "print(\"▪ Oración de salida más larga:\", max_out_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzqQe4Dm2ags"
      },
      "source": [
        "### 2.3 Secuencias:\n",
        "Calculo de `encoder_input_sequences`, `decoder_output_sequences`, `decoder_targets`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AN6OHRr2ags",
        "outputId": "cda08970-adef-4fe3-9385-6d0fd470e221"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "▪ Cantidad de filas del dataset: 6033\n",
            "▪ Forma de 'encoder_input_sequences': (6033, 9)\n",
            "▪ Forma de 'decoder_input_sequences': (6033, 10)\n"
          ]
        }
      ],
      "source": [
        "print(\"▪ Cantidad de filas del dataset:\", len(input_integer_seq))\n",
        "\n",
        "encoder_input_sequences = pad_sequences(input_integer_seq, maxlen=max_input_len)\n",
        "print(\"▪ Forma de 'encoder_input_sequences':\", encoder_input_sequences.shape)\n",
        "\n",
        "decoder_input_sequences = pad_sequences(output_input_integer_seq, maxlen=max_out_len, padding='post')\n",
        "print(\"▪ Forma de 'decoder_input_sequences':\", decoder_input_sequences.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAcrUePl3xZy",
        "outputId": "8ac4d0b1-8c18-4a22-df80-7e7dfb06665e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "▪ Forma de 'decoder_output_sequences': (6033, 10)\n"
          ]
        }
      ],
      "source": [
        "decoder_output_sequences = pad_sequences(output_integer_seq, maxlen=max_out_len, padding='post')\n",
        "print(\"▪ Forma de 'decoder_output_sequences':\", decoder_output_sequences.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KY4eluP2ags",
        "outputId": "767c1f88-de31-445b-e332-e41b650dbd33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "▪ Tamaño de entrada del Encoder: 9\n",
            "▪ Tamaño de entrada del Decoder: 10\n",
            "▪ Dimensiones de salida:  1807\n"
          ]
        }
      ],
      "source": [
        "class Data(Dataset):\n",
        "    def __init__(self, encoder_input, decoder_input, decoder_output):\n",
        "        # Convertir los arrays de numpy a tensores.\n",
        "        # pytorch espera en general entradas 32bits\n",
        "        self.encoder_inputs = torch.from_numpy(encoder_input.astype(np.int32))\n",
        "        self.decoder_inputs = torch.from_numpy(decoder_input.astype(np.int32))\n",
        "        # Transformar los datos a oneHotEncoding\n",
        "        # la loss function esperan la salida float\n",
        "        self.decoder_outputs = F.one_hot(torch.from_numpy(decoder_output).to(torch.int64), num_classes=num_words_output).float()\n",
        "\n",
        "        self.len = self.decoder_outputs.shape[0]\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        return self.encoder_inputs[index], self.decoder_inputs[index], self.decoder_outputs[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "data_set = Data(encoder_input_sequences, decoder_input_sequences, decoder_output_sequences)\n",
        "\n",
        "encoder_input_size = data_set.encoder_inputs.shape[1]\n",
        "print(\"▪ Tamaño de entrada del Encoder:\", encoder_input_size)\n",
        "\n",
        "decoder_input_size = data_set.decoder_inputs.shape[1]\n",
        "print(\"▪ Tamaño de entrada del Decoder:\", decoder_input_size)\n",
        "\n",
        "output_dim = data_set.decoder_outputs.shape[2]\n",
        "print(\"▪ Dimensiones de salida: \", output_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaVCPDO32ags"
      },
      "source": [
        "Ahora, se realiza el split del dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Auc5dFdV2ags",
        "outputId": "b443fa44-9eca-4e88-bbfa-5194386aeb3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "▪ Tamaño del conjunto de entrenamiento: 4827\n",
            "▪ Tamaño del conjunto de validacion: 1206\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "valid_set_size = int(data_set.len * 0.2)\n",
        "train_set_size = data_set.len - valid_set_size\n",
        "\n",
        "train_set = torch.utils.data.Subset(data_set, range(train_set_size))\n",
        "valid_set = torch.utils.data.Subset(data_set, range(train_set_size, data_set.len))\n",
        "\n",
        "print(\"▪ Tamaño del conjunto de entrenamiento:\", len(train_set))\n",
        "print(\"▪ Tamaño del conjunto de validacion:\", len(valid_set))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=128, shuffle=True)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=128, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CJIsLBbj6rg"
      },
      "source": [
        "## 3. Preparar los embeddings\n",
        "Se utilizan los embeddings de Glove (de 50 dim) para transformar los tokens de entrada en vectores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aO0bxZoZ2agt",
        "outputId": "2aeef9b8-54f2-498c-b760-be9b1cb6c42d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1wlDBOrxPq2-3htQ6ryVo7K1XnzLcfh4r&export=download\n",
            "From (redirected): https://drive.google.com/uc?id=1wlDBOrxPq2-3htQ6ryVo7K1XnzLcfh4r&export=download&confirm=t&uuid=bd3ba1e1-f1a9-405c-917a-57038554f8f7\n",
            "To: /content/gloveembedding.pkl\n",
            "100%|██████████| 525M/525M [00:07<00:00, 73.9MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Descargar los embeddings desde un gogle drive (es la forma más rápida)\n",
        "# NOTA: No hay garantía de que estos links perduren, en caso de que no estén\n",
        "# disponibles descargar de la página oficial como se explica en el siguiente bloque\n",
        "if os.access('gloveembedding.pkl', os.F_OK) is False:\n",
        "    url = 'https://drive.google.com/uc?id=1wlDBOrxPq2-3htQ6ryVo7K1XnzLcfh4r&export=download'\n",
        "    output = 'gloveembedding.pkl'\n",
        "    gdown.download(url, output, quiet=False)\n",
        "else:\n",
        "    print(\"✅ Los embeddings gloveembedding.pkl ya están descargados\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "-_rY0YtQ2agt"
      },
      "outputs": [],
      "source": [
        "class WordsEmbeddings(object):\n",
        "    logger = logging.getLogger(__name__)\n",
        "\n",
        "    def __init__(self):\n",
        "        # load the embeddings\n",
        "        words_embedding_pkl = Path(self.PKL_PATH)\n",
        "        if not words_embedding_pkl.is_file():\n",
        "            words_embedding_txt = Path(self.WORD_TO_VEC_MODEL_TXT_PATH)\n",
        "            assert words_embedding_txt.is_file(), 'Words embedding not available'\n",
        "            embeddings = self.convert_model_to_pickle()\n",
        "        else:\n",
        "            embeddings = self.load_model_from_pickle()\n",
        "        self.embeddings = embeddings\n",
        "        # build the vocabulary hashmap\n",
        "        index = np.arange(self.embeddings.shape[0])\n",
        "        # Dicctionarios para traducir de embedding a IDX de la palabra\n",
        "        self.word2idx = dict(zip(self.embeddings['word'], index))\n",
        "        self.idx2word = dict(zip(index, self.embeddings['word']))\n",
        "\n",
        "    def get_words_embeddings(self, words):\n",
        "        words_idxs = self.words2idxs(words)\n",
        "        return self.embeddings[words_idxs]['embedding']\n",
        "\n",
        "    def words2idxs(self, words):\n",
        "        return np.array([self.word2idx.get(word, -1) for word in words])\n",
        "\n",
        "    def idxs2words(self, idxs):\n",
        "        return np.array([self.idx2word.get(idx, '-1') for idx in idxs])\n",
        "\n",
        "    def load_model_from_pickle(self):\n",
        "        self.logger.debug(\n",
        "            'loading words embeddings from pickle {}'.format(\n",
        "                self.PKL_PATH\n",
        "            )\n",
        "        )\n",
        "        max_bytes = 2**28 - 1 # 256MB\n",
        "        bytes_in = bytearray(0)\n",
        "        input_size = os.path.getsize(self.PKL_PATH)\n",
        "        with open(self.PKL_PATH, 'rb') as f_in:\n",
        "            for _ in range(0, input_size, max_bytes):\n",
        "                bytes_in += f_in.read(max_bytes)\n",
        "        embeddings = pickle.loads(bytes_in)\n",
        "        self.logger.debug('words embeddings loaded')\n",
        "        return embeddings\n",
        "\n",
        "    def convert_model_to_pickle(self):\n",
        "        # create a numpy strctured array:\n",
        "        # word     embedding\n",
        "        # U50      np.float32[]\n",
        "        # word_1   a, b, c\n",
        "        # word_2   d, e, f\n",
        "        # ...\n",
        "        # word_n   g, h, i\n",
        "        self.logger.debug(\n",
        "            'converting and loading words embeddings from text file {}'.format(\n",
        "                self.WORD_TO_VEC_MODEL_TXT_PATH\n",
        "            )\n",
        "        )\n",
        "        structure = [('word', np.dtype('U' + str(self.WORD_MAX_SIZE))),\n",
        "                     ('embedding', np.float32, (self.N_FEATURES,))]\n",
        "        structure = np.dtype(structure)\n",
        "        # load numpy array from disk using a generator\n",
        "        with open(self.WORD_TO_VEC_MODEL_TXT_PATH, encoding=\"utf8\") as words_embeddings_txt:\n",
        "            embeddings_gen = (\n",
        "                (line.split()[0], line.split()[1:]) for line in words_embeddings_txt\n",
        "                if len(line.split()[1:]) == self.N_FEATURES\n",
        "            )\n",
        "            embeddings = np.fromiter(embeddings_gen, structure)\n",
        "        # add a null embedding\n",
        "        null_embedding = np.array(\n",
        "            [('null_embedding', np.zeros((self.N_FEATURES,), dtype=np.float32))],\n",
        "            dtype=structure\n",
        "        )\n",
        "        embeddings = np.concatenate([embeddings, null_embedding])\n",
        "        # dump numpy array to disk using pickle\n",
        "        max_bytes = 2**28 - 1 # # 256MB\n",
        "        bytes_out = pickle.dumps(embeddings, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "        with open(self.PKL_PATH, 'wb') as f_out:\n",
        "            for idx in range(0, len(bytes_out), max_bytes):\n",
        "                f_out.write(bytes_out[idx:idx+max_bytes])\n",
        "        self.logger.debug('words embeddings loaded')\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class GloveEmbeddings(WordsEmbeddings):\n",
        "    WORD_TO_VEC_MODEL_TXT_PATH = 'glove.twitter.27B.50d.txt'\n",
        "    PKL_PATH = 'gloveembedding.pkl'\n",
        "    N_FEATURES = 50\n",
        "    WORD_MAX_SIZE = 60\n",
        "\n",
        "# class FasttextEmbeddings(WordsEmbeddings):\n",
        "#     WORD_TO_VEC_MODEL_TXT_PATH = 'cc.en.300.vec'\n",
        "#     PKL_PATH = 'fasttext.pkl'\n",
        "#     N_FEATURES = 300\n",
        "#     WORD_MAX_SIZE = 60"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Ef486rMw2agt"
      },
      "outputs": [],
      "source": [
        "# Por una cuestion de RAM se utilizará los embeddings de Glove de dimension 50\n",
        "model_embeddings = GloveEmbeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7euJYgUw2agt",
        "outputId": "cdd774d9-238d-48a3-d23e-1351abb9a8c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preparando la matriz de embedding...\n",
            "▪ Cantidad de embeddings de palabras nulas: 38\n",
            "▪ Tamaño del vocabulario:  1799\n"
          ]
        }
      ],
      "source": [
        "# Crear la Embedding matrix de las secuencias en ingles\n",
        "\n",
        "print('Preparando la matriz de embedding...')\n",
        "embed_dim = model_embeddings.N_FEATURES\n",
        "words_not_found = []\n",
        "\n",
        "# word_index provieen del tokenizer\n",
        "\n",
        "nb_words = min(MAX_VOCAB_SIZE, len(word2idx_inputs)) # vocab_size\n",
        "embedding_matrix = np.zeros((nb_words, embed_dim))\n",
        "for word, i in word2idx_inputs.items():\n",
        "    if i >= nb_words:\n",
        "        continue\n",
        "    embedding_vector = model_embeddings.get_words_embeddings(word)[0]\n",
        "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
        "\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    else:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        words_not_found.append(word)\n",
        "\n",
        "print('▪ Cantidad de embeddings de palabras nulas:', np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
        "print(\"▪ Tamaño del vocabulario: \", nb_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGV2djtL2agt",
        "outputId": "1f31fd36-594d-4313-e107-614055ad26a6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1799, 50)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Dimensión de los embeddings de la secuencia en ingles\n",
        "embedding_matrix.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vKbhjtIwPgM"
      },
      "source": [
        "## 4. Entrenamiento del modelo\n",
        "Entrenar un modelo basado en el esquema encoder-decoder utilizando los datos generados en los puntos anteriores. Utilce como referencias los ejemplos vistos en clase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntSLxTaM2agu"
      },
      "outputs": [],
      "source": [
        "lstm_size = 128\n",
        "num_layers = 1\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # num_embeddings = vocab_size, definido por le Tokenizador\n",
        "        # embedding_dim = 50 --> dimensión de los embeddings utilizados\n",
        "        self.lstm_size = lstm_size\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding_dim = embed_dim\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=self.embedding_dim, padding_idx=0)\n",
        "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
        "        self.embedding.weight.requires_grad = False  # marcar como layer no entrenable (freeze)\n",
        "        self.lstm = nn.LSTM(input_size=self.embedding_dim, hidden_size=self.lstm_size, batch_first=True,\n",
        "                            num_layers=self.num_layers) # LSTM layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.embedding(x)\n",
        "        lstm_output, (ht, ct) = self.lstm(out)\n",
        "        return (ht, ct)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, output_dim):\n",
        "        super().__init__()\n",
        "        # num_embeddings = vocab_size, definido por le Tokenizador\n",
        "        # embedding_dim = 50 --> dimensión de los embeddings utilizados\n",
        "        self.lstm_size = lstm_size\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding_dim = embed_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=self.embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(input_size=self.embedding_dim, hidden_size=self.lstm_size, batch_first=True,\n",
        "                            num_layers=self.num_layers) # LSTM layer\n",
        "        self.fc1 = nn.Linear(in_features=self.lstm_size, out_features=self.output_dim) # Fully connected layer\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=1) # normalize in dim 1\n",
        "\n",
        "    def forward(self, x, prev_state):\n",
        "        out = self.embedding(x)\n",
        "        lstm_output, (ht, ct) = self.lstm(out, prev_state)\n",
        "        out = self.softmax(self.fc1(lstm_output[:,-1,:])) # take last output (last seq)\n",
        "        return out, (ht, ct)\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "        assert encoder.lstm_size == decoder.lstm_size, \\\n",
        "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
        "        assert encoder.num_layers == decoder.num_layers, \\\n",
        "            \"Encoder and decoder must have equal number of layers!\"\n",
        "\n",
        "    def forward(self, encoder_input, decoder_input):\n",
        "        batch_size = decoder_input.shape[0]\n",
        "        decoder_input_len = decoder_input.shape[1]\n",
        "        vocab_size = self.decoder.output_dim\n",
        "\n",
        "        # tensor para almacenar la salida\n",
        "        # (batch_size, sentence_len, one_hot_size)\n",
        "        outputs = torch.zeros(batch_size, decoder_input_len, vocab_size)\n",
        "\n",
        "        # ultimo hidden state del encoder, primer estado oculto del decoder\n",
        "        prev_state = self.encoder(encoder_input)\n",
        "\n",
        "        # En la primera iteracion se toma el primer token de target (<sos>)\n",
        "        input = decoder_input[:, 0:1]\n",
        "\n",
        "        for t in range(decoder_input_len):\n",
        "            # t --> token index\n",
        "\n",
        "            # utilizamos método \"teacher forcing\", es decir que durante\n",
        "            # el entrenamiento no realimentamos la salida del decoder\n",
        "            # sino el token correcto que sigue en target\n",
        "            input = decoder_input[:, t:t+1]\n",
        "\n",
        "            # ingresar cada token embedding, uno por uno junto al hidden state\n",
        "            # recibir el output del decoder (softmax)\n",
        "            output, prev_state = self.decoder(input, prev_state)\n",
        "            top1 = output.argmax(1).view(-1, 1)\n",
        "\n",
        "            # Sino se usará \"teacher forcing\" habría que descomentar\n",
        "            # esta linea.\n",
        "            # Hay ejemplos dandos vuelta en donde se utilza un random\n",
        "            # para ver en cada vuelta que técnica se aplica\n",
        "            #input = top1\n",
        "\n",
        "            # guardar cada salida (softmax)\n",
        "            outputs[:, t, :] = output\n",
        "\n",
        "        return outputs\n",
        "\n",
        "encoder = Encoder(vocab_size=nb_words)\n",
        "if cuda: encoder.cuda()\n",
        "# decoder --> vocab_size == output_dim --> porque recibe y devuelve palabras en el mismo vocabulario\n",
        "decoder = Decoder(vocab_size=num_words_output, output_dim=num_words_output)\n",
        "if cuda: decoder.cuda()\n",
        "\n",
        "model = Seq2Seq(encoder, decoder)\n",
        "if cuda: model.cuda()\n",
        "\n",
        "# Crear el optimizador la una función de error\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = torch.nn.CrossEntropyLoss()  # Para clasificación multi categórica"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgGsCI-iDHkI",
        "outputId": "5cb7f7a3-6a0b-4aa0-8e6a-2d19a5ae5909"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "Seq2Seq                                  [1, 10, 1807]             --\n",
              "├─Encoder: 1-1                           [1, 1, 128]               --\n",
              "│    └─Embedding: 2-1                    [1, 9, 50]                (89,950)\n",
              "│    └─LSTM: 2-2                         [1, 9, 128]               92,160\n",
              "├─Decoder: 1-2                           [1, 1807]                 --\n",
              "│    └─Embedding: 2-3                    [1, 1, 50]                90,350\n",
              "│    └─LSTM: 2-4                         [1, 1, 128]               92,160\n",
              "│    └─Linear: 2-5                       [1, 1807]                 233,103\n",
              "│    └─Softmax: 2-6                      [1, 1807]                 --\n",
              "├─Decoder: 1-3                           [1, 1807]                 (recursive)\n",
              "│    └─Embedding: 2-7                    [1, 1, 50]                (recursive)\n",
              "│    └─LSTM: 2-8                         [1, 1, 128]               (recursive)\n",
              "│    └─Linear: 2-9                       [1, 1807]                 (recursive)\n",
              "│    └─Softmax: 2-10                     [1, 1807]                 --\n",
              "├─Decoder: 1-4                           [1, 1807]                 (recursive)\n",
              "│    └─Embedding: 2-11                   [1, 1, 50]                (recursive)\n",
              "│    └─LSTM: 2-12                        [1, 1, 128]               (recursive)\n",
              "│    └─Linear: 2-13                      [1, 1807]                 (recursive)\n",
              "│    └─Softmax: 2-14                     [1, 1807]                 --\n",
              "├─Decoder: 1-5                           [1, 1807]                 (recursive)\n",
              "│    └─Embedding: 2-15                   [1, 1, 50]                (recursive)\n",
              "│    └─LSTM: 2-16                        [1, 1, 128]               (recursive)\n",
              "│    └─Linear: 2-17                      [1, 1807]                 (recursive)\n",
              "│    └─Softmax: 2-18                     [1, 1807]                 --\n",
              "├─Decoder: 1-6                           [1, 1807]                 (recursive)\n",
              "│    └─Embedding: 2-19                   [1, 1, 50]                (recursive)\n",
              "│    └─LSTM: 2-20                        [1, 1, 128]               (recursive)\n",
              "│    └─Linear: 2-21                      [1, 1807]                 (recursive)\n",
              "│    └─Softmax: 2-22                     [1, 1807]                 --\n",
              "├─Decoder: 1-7                           [1, 1807]                 (recursive)\n",
              "│    └─Embedding: 2-23                   [1, 1, 50]                (recursive)\n",
              "│    └─LSTM: 2-24                        [1, 1, 128]               (recursive)\n",
              "│    └─Linear: 2-25                      [1, 1807]                 (recursive)\n",
              "│    └─Softmax: 2-26                     [1, 1807]                 --\n",
              "├─Decoder: 1-8                           [1, 1807]                 (recursive)\n",
              "│    └─Embedding: 2-27                   [1, 1, 50]                (recursive)\n",
              "│    └─LSTM: 2-28                        [1, 1, 128]               (recursive)\n",
              "│    └─Linear: 2-29                      [1, 1807]                 (recursive)\n",
              "│    └─Softmax: 2-30                     [1, 1807]                 --\n",
              "├─Decoder: 1-9                           [1, 1807]                 (recursive)\n",
              "│    └─Embedding: 2-31                   [1, 1, 50]                (recursive)\n",
              "│    └─LSTM: 2-32                        [1, 1, 128]               (recursive)\n",
              "│    └─Linear: 2-33                      [1, 1807]                 (recursive)\n",
              "│    └─Softmax: 2-34                     [1, 1807]                 --\n",
              "├─Decoder: 1-10                          [1, 1807]                 (recursive)\n",
              "│    └─Embedding: 2-35                   [1, 1, 50]                (recursive)\n",
              "│    └─LSTM: 2-36                        [1, 1, 128]               (recursive)\n",
              "│    └─Linear: 2-37                      [1, 1807]                 (recursive)\n",
              "│    └─Softmax: 2-38                     [1, 1807]                 --\n",
              "├─Decoder: 1-11                          [1, 1807]                 (recursive)\n",
              "│    └─Embedding: 2-39                   [1, 1, 50]                (recursive)\n",
              "│    └─LSTM: 2-40                        [1, 1, 128]               (recursive)\n",
              "│    └─Linear: 2-41                      [1, 1807]                 (recursive)\n",
              "│    └─Softmax: 2-42                     [1, 1807]                 --\n",
              "==========================================================================================\n",
              "Total params: 597,723\n",
              "Trainable params: 507,773\n",
              "Non-trainable params: 89,950\n",
              "Total mult-adds (Units.MEGABYTES): 5.08\n",
              "==========================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 0.17\n",
              "Params size (MB): 2.39\n",
              "Estimated Total Size (MB): 2.56\n",
              "=========================================================================================="
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get the tuple for the first sample\n",
        "first_sample_data = data_set[0]\n",
        "\n",
        "# Extract the encoder and decoder inputs\n",
        "encoder_input_sample = first_sample_data[0]\n",
        "decoder_input_sample = first_sample_data[1]\n",
        "\n",
        "# Add a batch dimension (summary usually expects a batch)\n",
        "# Use .to(device) if your model is on CUDA\n",
        "encoder_input_sample = encoder_input_sample.unsqueeze(0).to(device)\n",
        "decoder_input_sample = decoder_input_sample.unsqueeze(0).to(device)\n",
        "\n",
        "# Pass the input tuple to summary\n",
        "summary(model, input_data=(encoder_input_sample, decoder_input_sample))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiRBQezZ2agu",
        "outputId": "0c546e3b-58f0-429d-f85c-cd482da0b816"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1/1000 - Train loss 70.501 - Train accuracy 0.499 - Valid Loss 69.810 - Valid accuracy 0.520\n",
            "Epoch: 2/1000 - Train loss 69.802 - Train accuracy 0.520 - Valid Loss 69.807 - Valid accuracy 0.520\n",
            "Epoch: 3/1000 - Train loss 69.794 - Train accuracy 0.521 - Valid Loss 69.794 - Valid accuracy 0.521\n",
            "Epoch: 4/1000 - Train loss 69.793 - Train accuracy 0.521 - Valid Loss 69.795 - Valid accuracy 0.521\n",
            "Epoch: 5/1000 - Train loss 69.786 - Train accuracy 0.522 - Valid Loss 69.785 - Valid accuracy 0.522\n",
            "Epoch: 6/1000 - Train loss 69.780 - Train accuracy 0.523 - Valid Loss 69.784 - Valid accuracy 0.522\n",
            "Epoch: 7/1000 - Train loss 69.775 - Train accuracy 0.523 - Valid Loss 69.775 - Valid accuracy 0.523\n",
            "Epoch: 8/1000 - Train loss 69.771 - Train accuracy 0.523 - Valid Loss 69.771 - Valid accuracy 0.523\n",
            "Epoch: 9/1000 - Train loss 69.765 - Train accuracy 0.524 - Valid Loss 69.773 - Valid accuracy 0.523\n",
            "Epoch: 10/1000 - Train loss 69.768 - Train accuracy 0.524 - Valid Loss 69.770 - Valid accuracy 0.523\n",
            "Epoch: 11/1000 - Train loss 69.765 - Train accuracy 0.524 - Valid Loss 69.775 - Valid accuracy 0.523\n",
            "Epoch: 12/1000 - Train loss 69.764 - Train accuracy 0.524 - Valid Loss 69.770 - Valid accuracy 0.523\n",
            "Epoch: 13/1000 - Train loss 69.767 - Train accuracy 0.524 - Valid Loss 69.774 - Valid accuracy 0.523\n",
            "Epoch: 14/1000 - Train loss 69.767 - Train accuracy 0.524 - Valid Loss 69.773 - Valid accuracy 0.523\n",
            "Epoch: 15/1000 - Train loss 69.764 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 16/1000 - Train loss 69.765 - Train accuracy 0.524 - Valid Loss 69.776 - Valid accuracy 0.523\n",
            "Epoch: 17/1000 - Train loss 69.768 - Train accuracy 0.524 - Valid Loss 69.771 - Valid accuracy 0.523\n",
            "Epoch: 18/1000 - Train loss 69.765 - Train accuracy 0.524 - Valid Loss 69.775 - Valid accuracy 0.523\n",
            "Epoch: 19/1000 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.774 - Valid accuracy 0.523\n",
            "Epoch: 20/1000 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.771 - Valid accuracy 0.523\n",
            "Epoch: 21/1000 - Train loss 69.766 - Train accuracy 0.524 - Valid Loss 69.773 - Valid accuracy 0.523\n",
            "Epoch: 22/1000 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.776 - Valid accuracy 0.523\n",
            "Epoch: 23/1000 - Train loss 69.766 - Train accuracy 0.524 - Valid Loss 69.775 - Valid accuracy 0.523\n",
            "Epoch: 24/1000 - Train loss 69.766 - Train accuracy 0.524 - Valid Loss 69.771 - Valid accuracy 0.523\n",
            "Epoch: 25/1000 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 26/1000 - Train loss 69.767 - Train accuracy 0.524 - Valid Loss 69.776 - Valid accuracy 0.523\n",
            "Epoch: 27/1000 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.774 - Valid accuracy 0.523\n",
            "Epoch: 28/1000 - Train loss 69.767 - Train accuracy 0.524 - Valid Loss 69.776 - Valid accuracy 0.523\n",
            "Epoch: 29/1000 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.770 - Valid accuracy 0.523\n",
            "Epoch: 30/1000 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.776 - Valid accuracy 0.523\n",
            "Epoch: 31/1000 - Train loss 69.764 - Train accuracy 0.524 - Valid Loss 69.771 - Valid accuracy 0.523\n",
            "Epoch: 32/1000 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 33/1000 - Train loss 69.766 - Train accuracy 0.524 - Valid Loss 69.772 - Valid accuracy 0.523\n",
            "Epoch: 34/1000 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.774 - Valid accuracy 0.523\n",
            "Epoch: 35/1000 - Train loss 69.766 - Train accuracy 0.524 - Valid Loss 69.774 - Valid accuracy 0.523\n",
            "Epoch: 36/1000 - Train loss 69.766 - Train accuracy 0.524 - Valid Loss 69.772 - Valid accuracy 0.523\n",
            "Epoch: 37/1000 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.776 - Valid accuracy 0.523\n",
            "Epoch: 38/1000 - Train loss 69.764 - Train accuracy 0.524 - Valid Loss 69.770 - Valid accuracy 0.523\n",
            "Epoch: 39/1000 - Train loss 69.764 - Train accuracy 0.524 - Valid Loss 69.772 - Valid accuracy 0.523\n",
            "Epoch: 40/1000 - Train loss 69.765 - Train accuracy 0.524 - Valid Loss 69.771 - Valid accuracy 0.523\n",
            "Epoch: 41/1000 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.771 - Valid accuracy 0.523\n",
            "Epoch: 42/1000 - Train loss 69.764 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 43/1000 - Train loss 69.765 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 44/1000 - Train loss 69.765 - Train accuracy 0.524 - Valid Loss 69.776 - Valid accuracy 0.523\n",
            "Epoch: 45/1000 - Train loss 69.764 - Train accuracy 0.524 - Valid Loss 69.775 - Valid accuracy 0.523\n",
            "Epoch: 46/1000 - Train loss 69.760 - Train accuracy 0.524 - Valid Loss 69.774 - Valid accuracy 0.523\n",
            "Epoch: 47/1000 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.775 - Valid accuracy 0.523\n",
            "Epoch: 48/1000 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.774 - Valid accuracy 0.523\n",
            "Epoch: 49/1000 - Train loss 69.764 - Train accuracy 0.524 - Valid Loss 69.775 - Valid accuracy 0.523\n",
            "Epoch: 50/1000 - Train loss 69.765 - Train accuracy 0.524 - Valid Loss 69.774 - Valid accuracy 0.523\n",
            "Epoch: 51/1000 - Train loss 69.764 - Train accuracy 0.524 - Valid Loss 69.773 - Valid accuracy 0.523\n",
            "Epoch: 52/1000 - Train loss 69.764 - Train accuracy 0.524 - Valid Loss 69.775 - Valid accuracy 0.523\n",
            "Epoch: 53/1000 - Train loss 69.764 - Train accuracy 0.524 - Valid Loss 69.774 - Valid accuracy 0.523\n",
            "Epoch: 54/1000 - Train loss 69.767 - Train accuracy 0.524 - Valid Loss 69.775 - Valid accuracy 0.523\n",
            "Epoch: 55/1000 - Train loss 69.767 - Train accuracy 0.524 - Valid Loss 69.776 - Valid accuracy 0.523\n",
            "Epoch: 56/1000 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.772 - Valid accuracy 0.523\n",
            "Epoch: 57/1000 - Train loss 69.764 - Train accuracy 0.524 - Valid Loss 69.774 - Valid accuracy 0.523\n",
            "Epoch: 58/1000 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.775 - Valid accuracy 0.523\n",
            "Epoch: 59/1000 - Train loss 69.764 - Train accuracy 0.524 - Valid Loss 69.772 - Valid accuracy 0.523\n",
            "Epoch: 60/1000 - Train loss 69.766 - Train accuracy 0.524 - Valid Loss 69.773 - Valid accuracy 0.523\n",
            "Epoch: 61/1000 - Train loss 69.761 - Train accuracy 0.524 - Valid Loss 69.774 - Valid accuracy 0.523\n",
            "Epoch: 62/1000 - Train loss 69.764 - Train accuracy 0.524 - Valid Loss 69.774 - Valid accuracy 0.523\n",
            "Epoch: 63/1000 - Train loss 69.765 - Train accuracy 0.524 - Valid Loss 69.774 - Valid accuracy 0.523\n",
            "Epoch: 64/1000 - Train loss 69.765 - Train accuracy 0.524 - Valid Loss 69.773 - Valid accuracy 0.523\n",
            "Epoch: 65/1000 - Train loss 69.765 - Train accuracy 0.524 - Valid Loss 69.776 - Valid accuracy 0.523\n",
            "Epoch: 66/1000 - Train loss 69.765 - Train accuracy 0.524 - Valid Loss 69.774 - Valid accuracy 0.523\n",
            "Epoch: 67/1000 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.774 - Valid accuracy 0.523\n",
            "Epoch: 68/1000 - Train loss 69.765 - Train accuracy 0.524 - Valid Loss 69.773 - Valid accuracy 0.523\n",
            "Epoch: 69/1000 - Train loss 69.765 - Train accuracy 0.524 - Valid Loss 69.775 - Valid accuracy 0.523\n",
            "Epoch: 70/1000 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.770 - Valid accuracy 0.523\n",
            "Epoch: 71/1000 - Train loss 69.765 - Train accuracy 0.524 - Valid Loss 69.775 - Valid accuracy 0.523\n",
            "Epoch: 72/1000 - Train loss 69.766 - Train accuracy 0.524 - Valid Loss 69.776 - Valid accuracy 0.523\n",
            "Epoch: 73/1000 - Train loss 69.761 - Train accuracy 0.524 - Valid Loss 69.774 - Valid accuracy 0.523\n",
            "Epoch: 74/1000 - Train loss 69.765 - Train accuracy 0.524 - Valid Loss 69.776 - Valid accuracy 0.523\n",
            "Epoch: 75/1000 - Train loss 69.769 - Train accuracy 0.523 - Valid Loss 69.773 - Valid accuracy 0.523\n",
            "Epoch: 76/1000 - Train loss 69.764 - Train accuracy 0.524 - Valid Loss 69.776 - Valid accuracy 0.523\n",
            "Epoch: 77/1000 - Train loss 69.764 - Train accuracy 0.524 - Valid Loss 69.773 - Valid accuracy 0.523\n",
            "Epoch: 78/1000 - Train loss 69.767 - Train accuracy 0.524 - Valid Loss 69.776 - Valid accuracy 0.523\n",
            "Epoch: 79/1000 - Train loss 69.766 - Train accuracy 0.524 - Valid Loss 69.774 - Valid accuracy 0.523\n",
            "Epoch: 80/1000 - Train loss 69.761 - Train accuracy 0.524 - Valid Loss 69.775 - Valid accuracy 0.523\n",
            "Epoch: 81/1000 - Train loss 69.766 - Train accuracy 0.524 - Valid Loss 69.776 - Valid accuracy 0.523\n",
            "Epoch: 82/1000 - Train loss 69.767 - Train accuracy 0.524 - Valid Loss 69.774 - Valid accuracy 0.523\n",
            "Epoch: 83/1000 - Train loss 69.765 - Train accuracy 0.524 - Valid Loss 69.775 - Valid accuracy 0.523\n",
            "Epoch: 84/1000 - Train loss 69.766 - Train accuracy 0.524 - Valid Loss 69.775 - Valid accuracy 0.523\n",
            "Epoch: 85/1000 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.774 - Valid accuracy 0.523\n",
            "Epoch: 86/1000 - Train loss 69.765 - Train accuracy 0.524 - Valid Loss 69.775 - Valid accuracy 0.523\n",
            "Epoch: 87/1000 - Train loss 69.765 - Train accuracy 0.524 - Valid Loss 69.776 - Valid accuracy 0.523\n",
            "Epoch: 88/1000 - Train loss 69.766 - Train accuracy 0.524 - Valid Loss 69.775 - Valid accuracy 0.523\n",
            "Epoch: 89/1000 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.776 - Valid accuracy 0.523\n",
            "Epoch: 90/1000 - Train loss 69.764 - Train accuracy 0.524 - Valid Loss 69.775 - Valid accuracy 0.523\n",
            "Epoch: 91/1000 - Train loss 69.764 - Train accuracy 0.524 - Valid Loss 69.775 - Valid accuracy 0.523\n",
            "Epoch: 92/1000 - Train loss 69.765 - Train accuracy 0.524 - Valid Loss 69.774 - Valid accuracy 0.523\n",
            "Epoch: 93/1000 - Train loss 69.767 - Train accuracy 0.524 - Valid Loss 69.774 - Valid accuracy 0.523\n",
            "Epoch: 94/1000 - Train loss 69.761 - Train accuracy 0.524 - Valid Loss 69.774 - Valid accuracy 0.523\n",
            "Epoch: 95/1000 - Train loss 69.761 - Train accuracy 0.524 - Valid Loss 69.774 - Valid accuracy 0.523\n",
            "Epoch: 96/1000 - Train loss 69.764 - Train accuracy 0.524 - Valid Loss 69.774 - Valid accuracy 0.523\n",
            "Epoch: 97/1000 - Train loss 69.764 - Train accuracy 0.524 - Valid Loss 69.775 - Valid accuracy 0.523\n",
            "Epoch: 98/1000 - Train loss 69.766 - Train accuracy 0.524 - Valid Loss 69.774 - Valid accuracy 0.523\n",
            "Epoch: 99/1000 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.774 - Valid accuracy 0.523\n",
            "Epoch: 100/1000 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.774 - Valid accuracy 0.523\n",
            "Epoch: 101/1000 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.775 - Valid accuracy 0.523\n",
            "Epoch: 102/1000 - Train loss 69.764 - Train accuracy 0.524 - Valid Loss 69.774 - Valid accuracy 0.523\n",
            "Epoch: 103/1000 - Train loss 69.765 - Train accuracy 0.524 - Valid Loss 69.774 - Valid accuracy 0.523\n",
            "Epoch: 104/1000 - Train loss 69.765 - Train accuracy 0.524 - Valid Loss 69.774 - Valid accuracy 0.523\n",
            "Epoch: 105/1000 - Train loss 69.765 - Train accuracy 0.524 - Valid Loss 69.774 - Valid accuracy 0.523\n",
            "Epoch: 106/1000 - Train loss 69.765 - Train accuracy 0.524 - Valid Loss 69.774 - Valid accuracy 0.523\n",
            "Epoch: 107/1000 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.774 - Valid accuracy 0.523\n",
            "Epoch: 108/1000 - Train loss 69.766 - Train accuracy 0.524 - Valid Loss 69.774 - Valid accuracy 0.523\n",
            "Epoch: 109/1000 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.774 - Valid accuracy 0.523\n",
            "Epoch: 110/1000 - Train loss 69.766 - Train accuracy 0.524 - Valid Loss 69.773 - Valid accuracy 0.523\n",
            "Epoch: 111/1000 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.773 - Valid accuracy 0.523\n",
            "Epoch: 112/1000 - Train loss 69.766 - Train accuracy 0.524 - Valid Loss 69.773 - Valid accuracy 0.523\n",
            "Epoch: 113/1000 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.773 - Valid accuracy 0.523\n",
            "Epoch: 114/1000 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.773 - Valid accuracy 0.523\n",
            "Epoch: 115/1000 - Train loss 69.764 - Train accuracy 0.524 - Valid Loss 69.773 - Valid accuracy 0.523\n",
            "Epoch: 116/1000 - Train loss 69.766 - Train accuracy 0.524 - Valid Loss 69.773 - Valid accuracy 0.523\n",
            "Epoch: 117/1000 - Train loss 69.764 - Train accuracy 0.524 - Valid Loss 69.773 - Valid accuracy 0.523\n",
            "Epoch: 118/1000 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.773 - Valid accuracy 0.523\n",
            "Epoch: 119/1000 - Train loss 69.765 - Train accuracy 0.524 - Valid Loss 69.773 - Valid accuracy 0.523\n",
            "Epoch: 120/1000 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.773 - Valid accuracy 0.523\n",
            "Epoch: 121/1000 - Train loss 69.764 - Train accuracy 0.524 - Valid Loss 69.774 - Valid accuracy 0.523\n",
            "Epoch: 122/1000 - Train loss 69.767 - Train accuracy 0.524 - Valid Loss 69.773 - Valid accuracy 0.523\n",
            "Epoch: 123/1000 - Train loss 69.764 - Train accuracy 0.524 - Valid Loss 69.774 - Valid accuracy 0.523\n",
            "Epoch: 124/1000 - Train loss 69.764 - Train accuracy 0.524 - Valid Loss 69.773 - Valid accuracy 0.523\n",
            "Epoch: 125/1000 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.772 - Valid accuracy 0.523\n",
            "Epoch: 126/1000 - Train loss 69.767 - Train accuracy 0.524 - Valid Loss 69.773 - Valid accuracy 0.523\n",
            "Epoch: 127/1000 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.773 - Valid accuracy 0.523\n",
            "Epoch: 128/1000 - Train loss 69.764 - Train accuracy 0.524 - Valid Loss 69.773 - Valid accuracy 0.523\n",
            "Epoch: 129/1000 - Train loss 69.765 - Train accuracy 0.524 - Valid Loss 69.773 - Valid accuracy 0.523\n",
            "Epoch: 130/1000 - Train loss 69.768 - Train accuracy 0.524 - Valid Loss 69.773 - Valid accuracy 0.523\n",
            "Epoch: 131/1000 - Train loss 69.764 - Train accuracy 0.524 - Valid Loss 69.775 - Valid accuracy 0.523\n",
            "Epoch: 132/1000 - Train loss 69.766 - Train accuracy 0.524 - Valid Loss 69.772 - Valid accuracy 0.523\n",
            "Epoch: 133/1000 - Train loss 69.766 - Train accuracy 0.524 - Valid Loss 69.772 - Valid accuracy 0.523\n",
            "Epoch: 134/1000 - Train loss 69.765 - Train accuracy 0.524 - Valid Loss 69.773 - Valid accuracy 0.523\n",
            "Epoch: 135/1000 - Train loss 69.766 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 136/1000 - Train loss 69.764 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 137/1000 - Train loss 69.765 - Train accuracy 0.524 - Valid Loss 69.775 - Valid accuracy 0.523\n",
            "Epoch: 138/1000 - Train loss 69.764 - Train accuracy 0.524 - Valid Loss 69.773 - Valid accuracy 0.523\n",
            "Epoch: 139/1000 - Train loss 69.764 - Train accuracy 0.524 - Valid Loss 69.773 - Valid accuracy 0.523\n",
            "Epoch: 140/1000 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.773 - Valid accuracy 0.523\n",
            "Epoch: 141/1000 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.773 - Valid accuracy 0.523\n",
            "Epoch: 142/1000 - Train loss 69.765 - Train accuracy 0.524 - Valid Loss 69.773 - Valid accuracy 0.523\n",
            "Epoch: 143/1000 - Train loss 69.765 - Train accuracy 0.524 - Valid Loss 69.773 - Valid accuracy 0.523\n",
            "Epoch: 144/1000 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.772 - Valid accuracy 0.523\n",
            "Epoch: 145/1000 - Train loss 69.765 - Train accuracy 0.524 - Valid Loss 69.772 - Valid accuracy 0.523\n",
            "Epoch: 146/1000 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.772 - Valid accuracy 0.523\n",
            "Epoch: 147/1000 - Train loss 69.764 - Train accuracy 0.524 - Valid Loss 69.772 - Valid accuracy 0.523\n",
            "Epoch: 148/1000 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.772 - Valid accuracy 0.523\n",
            "Epoch: 149/1000 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.772 - Valid accuracy 0.523\n",
            "Epoch: 150/1000 - Train loss 69.765 - Train accuracy 0.524 - Valid Loss 69.769 - Valid accuracy 0.523\n",
            "Epoch: 151/1000 - Train loss 69.767 - Train accuracy 0.524 - Valid Loss 69.771 - Valid accuracy 0.523\n",
            "Epoch: 152/1000 - Train loss 69.767 - Train accuracy 0.524 - Valid Loss 69.772 - Valid accuracy 0.523\n",
            "Epoch: 153/1000 - Train loss 69.766 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 154/1000 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 155/1000 - Train loss 69.766 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 156/1000 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 157/1000 - Train loss 69.766 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 158/1000 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 159/1000 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 160/1000 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 161/1000 - Train loss 69.766 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 162/1000 - Train loss 69.765 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 163/1000 - Train loss 69.764 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 164/1000 - Train loss 69.764 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 165/1000 - Train loss 69.765 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 166/1000 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 167/1000 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 168/1000 - Train loss 69.764 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 169/1000 - Train loss 69.764 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 170/1000 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 171/1000 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 172/1000 - Train loss 69.765 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 173/1000 - Train loss 69.765 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 174/1000 - Train loss 69.765 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 175/1000 - Train loss 69.765 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 176/1000 - Train loss 69.765 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 177/1000 - Train loss 69.764 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 178/1000 - Train loss 69.767 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 179/1000 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 180/1000 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 181/1000 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.776 - Valid accuracy 0.523\n",
            "Epoch: 182/1000 - Train loss 69.768 - Train accuracy 0.524 - Valid Loss 69.775 - Valid accuracy 0.523\n",
            "Epoch: 183/1000 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.775 - Valid accuracy 0.523\n",
            "Epoch: 184/1000 - Train loss 69.761 - Train accuracy 0.525 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 185/1000 - Train loss 69.766 - Train accuracy 0.524 - Valid Loss 69.772 - Valid accuracy 0.523\n",
            "Epoch: 186/1000 - Train loss 69.764 - Train accuracy 0.524 - Valid Loss 69.775 - Valid accuracy 0.523\n",
            "Epoch: 187/1000 - Train loss 69.764 - Train accuracy 0.524 - Valid Loss 69.775 - Valid accuracy 0.523\n",
            "Epoch: 188/1000 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.775 - Valid accuracy 0.523\n",
            "Epoch: 189/1000 - Train loss 69.764 - Train accuracy 0.524 - Valid Loss 69.774 - Valid accuracy 0.523\n",
            "Epoch: 190/1000 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.773 - Valid accuracy 0.523\n",
            "Epoch: 191/1000 - Train loss 69.765 - Train accuracy 0.524 - Valid Loss 69.773 - Valid accuracy 0.523\n",
            "Epoch: 192/1000 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.774 - Valid accuracy 0.523\n",
            "Epoch: 193/1000 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.775 - Valid accuracy 0.523\n",
            "Epoch: 194/1000 - Train loss 69.764 - Train accuracy 0.524 - Valid Loss 69.775 - Valid accuracy 0.523\n",
            "Epoch: 195/1000 - Train loss 69.764 - Train accuracy 0.524 - Valid Loss 69.775 - Valid accuracy 0.523\n",
            "Epoch: 196/1000 - Train loss 69.765 - Train accuracy 0.524 - Valid Loss 69.775 - Valid accuracy 0.523\n",
            "Epoch: 197/1000 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.775 - Valid accuracy 0.523\n",
            "Epoch: 198/1000 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.774 - Valid accuracy 0.523\n",
            "Epoch: 199/1000 - Train loss 69.765 - Train accuracy 0.524 - Valid Loss 69.774 - Valid accuracy 0.523\n",
            "Epoch: 200/1000 - Train loss 69.760 - Train accuracy 0.524 - Valid Loss 69.774 - Valid accuracy 0.523\n",
            "Epoch: 201/1000 - Train loss 69.764 - Train accuracy 0.524 - Valid Loss 69.774 - Valid accuracy 0.523\n",
            "Epoch: 202/1000 - Train loss 69.765 - Train accuracy 0.524 - Valid Loss 69.774 - Valid accuracy 0.523\n",
            "Epoch: 203/1000 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.774 - Valid accuracy 0.523\n",
            "Epoch: 204/1000 - Train loss 69.764 - Train accuracy 0.524 - Valid Loss 69.774 - Valid accuracy 0.523\n",
            "Epoch: 205/1000 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.773 - Valid accuracy 0.523\n",
            "Epoch: 206/1000 - Train loss 69.765 - Train accuracy 0.524 - Valid Loss 69.774 - Valid accuracy 0.523\n",
            "Epoch: 207/1000 - Train loss 69.764 - Train accuracy 0.524 - Valid Loss 69.773 - Valid accuracy 0.523\n",
            "Epoch: 208/1000 - Train loss 69.766 - Train accuracy 0.524 - Valid Loss 69.773 - Valid accuracy 0.523\n",
            "Epoch: 209/1000 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.774 - Valid accuracy 0.523\n",
            "Epoch: 210/1000 - Train loss 69.764 - Train accuracy 0.524 - Valid Loss 69.774 - Valid accuracy 0.523\n",
            "Epoch: 211/1000 - Train loss 69.767 - Train accuracy 0.524 - Valid Loss 69.773 - Valid accuracy 0.523\n",
            "Epoch: 212/1000 - Train loss 69.765 - Train accuracy 0.524 - Valid Loss 69.773 - Valid accuracy 0.523\n",
            "Epoch: 213/1000 - Train loss 69.761 - Train accuracy 0.524 - Valid Loss 69.773 - Valid accuracy 0.523\n",
            "Epoch: 214/1000 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.773 - Valid accuracy 0.523\n",
            "Epoch: 215/1000 - Train loss 69.764 - Train accuracy 0.524 - Valid Loss 69.773 - Valid accuracy 0.523\n",
            "Epoch: 216/1000 - Train loss 69.766 - Train accuracy 0.524 - Valid Loss 69.773 - Valid accuracy 0.523\n",
            "Epoch: 217/1000 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.773 - Valid accuracy 0.523\n",
            "Epoch: 218/1000 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.773 - Valid accuracy 0.523\n",
            "Epoch: 219/1000 - Train loss 69.766 - Train accuracy 0.524 - Valid Loss 69.773 - Valid accuracy 0.523\n",
            "Epoch: 220/1000 - Train loss 69.766 - Train accuracy 0.524 - Valid Loss 69.773 - Valid accuracy 0.523\n",
            "Epoch: 221/1000 - Train loss 69.765 - Train accuracy 0.524 - Valid Loss 69.773 - Valid accuracy 0.523\n",
            "Epoch: 222/1000 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.773 - Valid accuracy 0.523\n",
            "Epoch: 223/1000 - Train loss 69.764 - Train accuracy 0.524 - Valid Loss 69.773 - Valid accuracy 0.523\n",
            "Epoch: 224/1000 - Train loss 69.597 - Train accuracy 0.541 - Valid Loss 69.503 - Valid accuracy 0.550\n",
            "Epoch: 225/1000 - Train loss 69.456 - Train accuracy 0.555 - Valid Loss 69.503 - Valid accuracy 0.550\n",
            "Epoch: 226/1000 - Train loss 69.453 - Train accuracy 0.555 - Valid Loss 69.498 - Valid accuracy 0.551\n",
            "Epoch: 227/1000 - Train loss 69.449 - Train accuracy 0.555 - Valid Loss 69.498 - Valid accuracy 0.551\n",
            "Epoch: 228/1000 - Train loss 69.448 - Train accuracy 0.556 - Valid Loss 69.496 - Valid accuracy 0.551\n",
            "Epoch: 229/1000 - Train loss 69.450 - Train accuracy 0.555 - Valid Loss 69.496 - Valid accuracy 0.551\n",
            "Epoch: 230/1000 - Train loss 69.446 - Train accuracy 0.556 - Valid Loss 69.499 - Valid accuracy 0.551\n",
            "Epoch: 231/1000 - Train loss 69.447 - Train accuracy 0.556 - Valid Loss 69.496 - Valid accuracy 0.551\n",
            "Epoch: 232/1000 - Train loss 69.445 - Train accuracy 0.556 - Valid Loss 69.496 - Valid accuracy 0.551\n",
            "Epoch: 233/1000 - Train loss 69.443 - Train accuracy 0.556 - Valid Loss 69.496 - Valid accuracy 0.551\n",
            "Epoch: 234/1000 - Train loss 69.444 - Train accuracy 0.556 - Valid Loss 69.496 - Valid accuracy 0.551\n",
            "Epoch: 235/1000 - Train loss 69.440 - Train accuracy 0.556 - Valid Loss 69.496 - Valid accuracy 0.551\n",
            "Epoch: 236/1000 - Train loss 69.444 - Train accuracy 0.556 - Valid Loss 69.496 - Valid accuracy 0.551\n",
            "Epoch: 237/1000 - Train loss 69.441 - Train accuracy 0.556 - Valid Loss 69.496 - Valid accuracy 0.551\n",
            "Epoch: 238/1000 - Train loss 69.441 - Train accuracy 0.556 - Valid Loss 69.498 - Valid accuracy 0.551\n",
            "Epoch: 239/1000 - Train loss 69.441 - Train accuracy 0.556 - Valid Loss 69.498 - Valid accuracy 0.551\n",
            "Epoch: 240/1000 - Train loss 69.439 - Train accuracy 0.556 - Valid Loss 69.499 - Valid accuracy 0.551\n",
            "Epoch: 241/1000 - Train loss 69.443 - Train accuracy 0.556 - Valid Loss 69.501 - Valid accuracy 0.550\n",
            "Epoch: 242/1000 - Train loss 69.441 - Train accuracy 0.556 - Valid Loss 69.502 - Valid accuracy 0.550\n",
            "Epoch: 243/1000 - Train loss 69.443 - Train accuracy 0.556 - Valid Loss 69.496 - Valid accuracy 0.551\n",
            "Epoch: 244/1000 - Train loss 69.441 - Train accuracy 0.556 - Valid Loss 69.500 - Valid accuracy 0.550\n",
            "Epoch: 245/1000 - Train loss 69.445 - Train accuracy 0.556 - Valid Loss 69.501 - Valid accuracy 0.550\n",
            "Epoch: 246/1000 - Train loss 69.443 - Train accuracy 0.556 - Valid Loss 69.500 - Valid accuracy 0.550\n",
            "Epoch: 247/1000 - Train loss 69.442 - Train accuracy 0.556 - Valid Loss 69.501 - Valid accuracy 0.550\n",
            "Epoch: 248/1000 - Train loss 69.442 - Train accuracy 0.556 - Valid Loss 69.501 - Valid accuracy 0.550\n",
            "Epoch: 249/1000 - Train loss 69.442 - Train accuracy 0.556 - Valid Loss 69.501 - Valid accuracy 0.550\n",
            "Epoch: 250/1000 - Train loss 69.439 - Train accuracy 0.556 - Valid Loss 69.499 - Valid accuracy 0.551\n",
            "Epoch: 251/1000 - Train loss 69.433 - Train accuracy 0.557 - Valid Loss 69.497 - Valid accuracy 0.551\n",
            "Epoch: 252/1000 - Train loss 69.379 - Train accuracy 0.563 - Valid Loss 69.478 - Valid accuracy 0.552\n",
            "Epoch: 253/1000 - Train loss 69.359 - Train accuracy 0.565 - Valid Loss 69.477 - Valid accuracy 0.552\n",
            "Epoch: 254/1000 - Train loss 69.278 - Train accuracy 0.573 - Valid Loss 69.400 - Valid accuracy 0.560\n",
            "Epoch: 255/1000 - Train loss 69.210 - Train accuracy 0.579 - Valid Loss 69.386 - Valid accuracy 0.562\n",
            "Epoch: 256/1000 - Train loss 69.209 - Train accuracy 0.580 - Valid Loss 69.379 - Valid accuracy 0.562\n",
            "Epoch: 257/1000 - Train loss 69.205 - Train accuracy 0.580 - Valid Loss 69.379 - Valid accuracy 0.562\n",
            "Epoch: 258/1000 - Train loss 69.204 - Train accuracy 0.580 - Valid Loss 69.379 - Valid accuracy 0.562\n",
            "Epoch: 259/1000 - Train loss 69.202 - Train accuracy 0.580 - Valid Loss 69.381 - Valid accuracy 0.562\n",
            "Epoch: 260/1000 - Train loss 69.196 - Train accuracy 0.581 - Valid Loss 69.372 - Valid accuracy 0.563\n",
            "Epoch: 261/1000 - Train loss 69.182 - Train accuracy 0.582 - Valid Loss 69.355 - Valid accuracy 0.565\n",
            "Epoch: 262/1000 - Train loss 69.152 - Train accuracy 0.585 - Valid Loss 69.340 - Valid accuracy 0.566\n",
            "Epoch: 263/1000 - Train loss 69.152 - Train accuracy 0.585 - Valid Loss 69.334 - Valid accuracy 0.567\n",
            "Epoch: 264/1000 - Train loss 69.142 - Train accuracy 0.586 - Valid Loss 69.332 - Valid accuracy 0.567\n",
            "Epoch: 265/1000 - Train loss 69.135 - Train accuracy 0.587 - Valid Loss 69.316 - Valid accuracy 0.569\n",
            "Epoch: 266/1000 - Train loss 69.132 - Train accuracy 0.587 - Valid Loss 69.316 - Valid accuracy 0.569\n",
            "Epoch: 267/1000 - Train loss 68.967 - Train accuracy 0.605 - Valid Loss 68.689 - Valid accuracy 0.635\n",
            "Epoch: 268/1000 - Train loss 68.307 - Train accuracy 0.671 - Valid Loss 68.459 - Valid accuracy 0.655\n",
            "Epoch: 269/1000 - Train loss 68.268 - Train accuracy 0.674 - Valid Loss 68.458 - Valid accuracy 0.655\n",
            "Epoch: 270/1000 - Train loss 68.258 - Train accuracy 0.675 - Valid Loss 68.468 - Valid accuracy 0.654\n",
            "Epoch: 271/1000 - Train loss 68.253 - Train accuracy 0.675 - Valid Loss 68.472 - Valid accuracy 0.654\n",
            "Epoch: 272/1000 - Train loss 68.248 - Train accuracy 0.676 - Valid Loss 68.469 - Valid accuracy 0.653\n",
            "Epoch: 273/1000 - Train loss 68.243 - Train accuracy 0.676 - Valid Loss 68.473 - Valid accuracy 0.653\n",
            "Epoch: 274/1000 - Train loss 68.242 - Train accuracy 0.676 - Valid Loss 68.472 - Valid accuracy 0.653\n",
            "Epoch: 275/1000 - Train loss 68.239 - Train accuracy 0.677 - Valid Loss 68.481 - Valid accuracy 0.652\n",
            "Epoch: 276/1000 - Train loss 68.239 - Train accuracy 0.677 - Valid Loss 68.487 - Valid accuracy 0.652\n",
            "Epoch: 277/1000 - Train loss 68.234 - Train accuracy 0.677 - Valid Loss 68.487 - Valid accuracy 0.652\n",
            "Epoch: 278/1000 - Train loss 68.234 - Train accuracy 0.677 - Valid Loss 68.487 - Valid accuracy 0.652\n",
            "Epoch: 279/1000 - Train loss 68.234 - Train accuracy 0.677 - Valid Loss 68.486 - Valid accuracy 0.652\n",
            "Epoch: 280/1000 - Train loss 68.236 - Train accuracy 0.677 - Valid Loss 68.486 - Valid accuracy 0.652\n",
            "Epoch: 281/1000 - Train loss 68.231 - Train accuracy 0.677 - Valid Loss 68.487 - Valid accuracy 0.652\n",
            "Epoch: 282/1000 - Train loss 68.235 - Train accuracy 0.677 - Valid Loss 68.485 - Valid accuracy 0.652\n",
            "Epoch: 283/1000 - Train loss 68.234 - Train accuracy 0.677 - Valid Loss 68.483 - Valid accuracy 0.652\n",
            "Epoch: 284/1000 - Train loss 68.231 - Train accuracy 0.677 - Valid Loss 68.476 - Valid accuracy 0.653\n",
            "Epoch: 285/1000 - Train loss 68.230 - Train accuracy 0.677 - Valid Loss 68.475 - Valid accuracy 0.653\n",
            "Epoch: 286/1000 - Train loss 68.233 - Train accuracy 0.677 - Valid Loss 68.477 - Valid accuracy 0.653\n",
            "Epoch: 287/1000 - Train loss 68.231 - Train accuracy 0.677 - Valid Loss 68.477 - Valid accuracy 0.653\n",
            "Epoch: 288/1000 - Train loss 68.230 - Train accuracy 0.677 - Valid Loss 68.481 - Valid accuracy 0.652\n",
            "Epoch: 289/1000 - Train loss 68.230 - Train accuracy 0.677 - Valid Loss 68.472 - Valid accuracy 0.653\n",
            "Epoch: 290/1000 - Train loss 68.229 - Train accuracy 0.678 - Valid Loss 68.468 - Valid accuracy 0.654\n",
            "Epoch: 291/1000 - Train loss 68.229 - Train accuracy 0.678 - Valid Loss 68.476 - Valid accuracy 0.652\n",
            "Epoch: 292/1000 - Train loss 68.229 - Train accuracy 0.678 - Valid Loss 68.476 - Valid accuracy 0.652\n",
            "Epoch: 293/1000 - Train loss 68.228 - Train accuracy 0.678 - Valid Loss 68.478 - Valid accuracy 0.652\n",
            "Epoch: 294/1000 - Train loss 68.230 - Train accuracy 0.677 - Valid Loss 68.467 - Valid accuracy 0.655\n",
            "Epoch: 295/1000 - Train loss 68.228 - Train accuracy 0.678 - Valid Loss 68.468 - Valid accuracy 0.655\n",
            "Epoch: 296/1000 - Train loss 68.226 - Train accuracy 0.678 - Valid Loss 68.472 - Valid accuracy 0.654\n",
            "Epoch: 297/1000 - Train loss 68.226 - Train accuracy 0.678 - Valid Loss 68.463 - Valid accuracy 0.654\n",
            "Epoch: 298/1000 - Train loss 68.225 - Train accuracy 0.678 - Valid Loss 68.465 - Valid accuracy 0.654\n",
            "Epoch: 299/1000 - Train loss 68.227 - Train accuracy 0.678 - Valid Loss 68.465 - Valid accuracy 0.653\n",
            "Epoch: 300/1000 - Train loss 68.226 - Train accuracy 0.678 - Valid Loss 68.478 - Valid accuracy 0.652\n",
            "Epoch: 301/1000 - Train loss 68.226 - Train accuracy 0.678 - Valid Loss 68.477 - Valid accuracy 0.653\n",
            "Epoch: 302/1000 - Train loss 68.224 - Train accuracy 0.678 - Valid Loss 68.469 - Valid accuracy 0.653\n",
            "Epoch: 303/1000 - Train loss 68.223 - Train accuracy 0.678 - Valid Loss 68.478 - Valid accuracy 0.653\n",
            "Epoch: 304/1000 - Train loss 68.224 - Train accuracy 0.678 - Valid Loss 68.474 - Valid accuracy 0.653\n",
            "Epoch: 305/1000 - Train loss 68.223 - Train accuracy 0.678 - Valid Loss 68.474 - Valid accuracy 0.653\n",
            "Epoch: 306/1000 - Train loss 68.222 - Train accuracy 0.678 - Valid Loss 68.474 - Valid accuracy 0.653\n",
            "Epoch: 307/1000 - Train loss 68.224 - Train accuracy 0.678 - Valid Loss 68.473 - Valid accuracy 0.653\n",
            "Epoch: 308/1000 - Train loss 68.221 - Train accuracy 0.678 - Valid Loss 68.475 - Valid accuracy 0.653\n",
            "Epoch: 309/1000 - Train loss 68.221 - Train accuracy 0.678 - Valid Loss 68.481 - Valid accuracy 0.652\n",
            "Epoch: 310/1000 - Train loss 68.223 - Train accuracy 0.678 - Valid Loss 68.471 - Valid accuracy 0.653\n",
            "Epoch: 311/1000 - Train loss 68.220 - Train accuracy 0.678 - Valid Loss 68.475 - Valid accuracy 0.652\n",
            "Epoch: 312/1000 - Train loss 68.217 - Train accuracy 0.679 - Valid Loss 68.481 - Valid accuracy 0.652\n",
            "Epoch: 313/1000 - Train loss 68.222 - Train accuracy 0.678 - Valid Loss 68.491 - Valid accuracy 0.651\n",
            "Epoch: 314/1000 - Train loss 68.221 - Train accuracy 0.678 - Valid Loss 68.479 - Valid accuracy 0.653\n",
            "Epoch: 315/1000 - Train loss 68.222 - Train accuracy 0.678 - Valid Loss 68.484 - Valid accuracy 0.653\n",
            "Epoch: 316/1000 - Train loss 68.219 - Train accuracy 0.679 - Valid Loss 68.495 - Valid accuracy 0.651\n",
            "Epoch: 317/1000 - Train loss 68.224 - Train accuracy 0.678 - Valid Loss 68.493 - Valid accuracy 0.652\n",
            "Epoch: 318/1000 - Train loss 68.220 - Train accuracy 0.678 - Valid Loss 68.495 - Valid accuracy 0.650\n",
            "Epoch: 319/1000 - Train loss 68.222 - Train accuracy 0.678 - Valid Loss 68.495 - Valid accuracy 0.652\n",
            "Epoch: 320/1000 - Train loss 68.221 - Train accuracy 0.678 - Valid Loss 68.485 - Valid accuracy 0.653\n",
            "Epoch: 321/1000 - Train loss 68.219 - Train accuracy 0.678 - Valid Loss 68.484 - Valid accuracy 0.652\n",
            "Epoch: 322/1000 - Train loss 68.220 - Train accuracy 0.678 - Valid Loss 68.484 - Valid accuracy 0.652\n",
            "Epoch: 323/1000 - Train loss 68.218 - Train accuracy 0.679 - Valid Loss 68.479 - Valid accuracy 0.652\n",
            "Epoch: 324/1000 - Train loss 68.219 - Train accuracy 0.679 - Valid Loss 68.477 - Valid accuracy 0.653\n",
            "Epoch: 325/1000 - Train loss 68.220 - Train accuracy 0.678 - Valid Loss 68.479 - Valid accuracy 0.653\n",
            "Epoch: 326/1000 - Train loss 68.222 - Train accuracy 0.678 - Valid Loss 68.479 - Valid accuracy 0.653\n",
            "Epoch: 327/1000 - Train loss 68.220 - Train accuracy 0.678 - Valid Loss 68.481 - Valid accuracy 0.652\n",
            "Epoch: 328/1000 - Train loss 68.220 - Train accuracy 0.678 - Valid Loss 68.475 - Valid accuracy 0.654\n",
            "Epoch: 329/1000 - Train loss 68.218 - Train accuracy 0.679 - Valid Loss 68.480 - Valid accuracy 0.653\n",
            "Epoch: 330/1000 - Train loss 68.218 - Train accuracy 0.679 - Valid Loss 68.481 - Valid accuracy 0.653\n",
            "Epoch: 331/1000 - Train loss 68.216 - Train accuracy 0.679 - Valid Loss 68.480 - Valid accuracy 0.653\n",
            "Epoch: 332/1000 - Train loss 68.218 - Train accuracy 0.679 - Valid Loss 68.477 - Valid accuracy 0.652\n",
            "Epoch: 333/1000 - Train loss 68.218 - Train accuracy 0.679 - Valid Loss 68.478 - Valid accuracy 0.653\n",
            "Epoch: 334/1000 - Train loss 68.219 - Train accuracy 0.678 - Valid Loss 68.478 - Valid accuracy 0.653\n",
            "Epoch: 335/1000 - Train loss 68.221 - Train accuracy 0.678 - Valid Loss 68.478 - Valid accuracy 0.652\n",
            "Epoch: 336/1000 - Train loss 68.218 - Train accuracy 0.679 - Valid Loss 68.487 - Valid accuracy 0.652\n",
            "Epoch: 337/1000 - Train loss 68.218 - Train accuracy 0.679 - Valid Loss 68.479 - Valid accuracy 0.653\n",
            "Epoch: 338/1000 - Train loss 68.216 - Train accuracy 0.679 - Valid Loss 68.480 - Valid accuracy 0.652\n",
            "Epoch: 339/1000 - Train loss 68.219 - Train accuracy 0.678 - Valid Loss 68.482 - Valid accuracy 0.652\n",
            "Epoch: 340/1000 - Train loss 68.220 - Train accuracy 0.678 - Valid Loss 68.487 - Valid accuracy 0.652\n",
            "Epoch: 341/1000 - Train loss 68.217 - Train accuracy 0.679 - Valid Loss 68.487 - Valid accuracy 0.651\n",
            "Epoch: 342/1000 - Train loss 68.218 - Train accuracy 0.679 - Valid Loss 68.486 - Valid accuracy 0.652\n",
            "Epoch: 343/1000 - Train loss 68.220 - Train accuracy 0.678 - Valid Loss 68.489 - Valid accuracy 0.652\n",
            "Epoch: 344/1000 - Train loss 68.219 - Train accuracy 0.678 - Valid Loss 68.490 - Valid accuracy 0.652\n",
            "Epoch: 345/1000 - Train loss 68.217 - Train accuracy 0.679 - Valid Loss 68.487 - Valid accuracy 0.652\n",
            "Epoch: 346/1000 - Train loss 68.217 - Train accuracy 0.679 - Valid Loss 68.488 - Valid accuracy 0.652\n",
            "Epoch: 347/1000 - Train loss 68.218 - Train accuracy 0.679 - Valid Loss 68.489 - Valid accuracy 0.652\n",
            "Epoch: 348/1000 - Train loss 68.218 - Train accuracy 0.679 - Valid Loss 68.491 - Valid accuracy 0.652\n",
            "Epoch: 349/1000 - Train loss 68.218 - Train accuracy 0.679 - Valid Loss 68.488 - Valid accuracy 0.652\n",
            "Epoch: 350/1000 - Train loss 68.219 - Train accuracy 0.679 - Valid Loss 68.493 - Valid accuracy 0.652\n",
            "Epoch: 351/1000 - Train loss 68.217 - Train accuracy 0.679 - Valid Loss 68.495 - Valid accuracy 0.651\n",
            "Epoch: 352/1000 - Train loss 68.219 - Train accuracy 0.679 - Valid Loss 68.492 - Valid accuracy 0.652\n",
            "Epoch: 353/1000 - Train loss 68.215 - Train accuracy 0.679 - Valid Loss 68.489 - Valid accuracy 0.652\n",
            "Epoch: 354/1000 - Train loss 68.217 - Train accuracy 0.679 - Valid Loss 68.489 - Valid accuracy 0.652\n",
            "Epoch: 355/1000 - Train loss 68.216 - Train accuracy 0.679 - Valid Loss 68.485 - Valid accuracy 0.652\n",
            "Epoch: 356/1000 - Train loss 68.216 - Train accuracy 0.679 - Valid Loss 68.490 - Valid accuracy 0.652\n",
            "Epoch: 357/1000 - Train loss 68.217 - Train accuracy 0.679 - Valid Loss 68.489 - Valid accuracy 0.652\n",
            "Epoch: 358/1000 - Train loss 68.217 - Train accuracy 0.679 - Valid Loss 68.501 - Valid accuracy 0.651\n",
            "Epoch: 359/1000 - Train loss 68.217 - Train accuracy 0.679 - Valid Loss 68.503 - Valid accuracy 0.649\n",
            "Epoch: 360/1000 - Train loss 68.216 - Train accuracy 0.679 - Valid Loss 68.493 - Valid accuracy 0.652\n",
            "Epoch: 361/1000 - Train loss 68.218 - Train accuracy 0.679 - Valid Loss 68.492 - Valid accuracy 0.652\n",
            "Epoch: 362/1000 - Train loss 68.215 - Train accuracy 0.679 - Valid Loss 68.493 - Valid accuracy 0.652\n",
            "Epoch: 363/1000 - Train loss 68.216 - Train accuracy 0.679 - Valid Loss 68.499 - Valid accuracy 0.651\n",
            "Epoch: 364/1000 - Train loss 68.219 - Train accuracy 0.679 - Valid Loss 68.488 - Valid accuracy 0.652\n",
            "Epoch: 365/1000 - Train loss 68.218 - Train accuracy 0.679 - Valid Loss 68.487 - Valid accuracy 0.652\n",
            "Epoch: 366/1000 - Train loss 68.217 - Train accuracy 0.679 - Valid Loss 68.484 - Valid accuracy 0.653\n",
            "Epoch: 367/1000 - Train loss 68.216 - Train accuracy 0.679 - Valid Loss 68.474 - Valid accuracy 0.654\n",
            "Epoch: 368/1000 - Train loss 68.217 - Train accuracy 0.679 - Valid Loss 68.484 - Valid accuracy 0.653\n",
            "Epoch: 369/1000 - Train loss 68.219 - Train accuracy 0.679 - Valid Loss 68.479 - Valid accuracy 0.654\n",
            "Epoch: 370/1000 - Train loss 68.216 - Train accuracy 0.679 - Valid Loss 68.488 - Valid accuracy 0.652\n",
            "Epoch: 371/1000 - Train loss 68.215 - Train accuracy 0.679 - Valid Loss 68.484 - Valid accuracy 0.652\n",
            "Epoch: 372/1000 - Train loss 68.216 - Train accuracy 0.679 - Valid Loss 68.495 - Valid accuracy 0.652\n",
            "Epoch: 373/1000 - Train loss 68.215 - Train accuracy 0.679 - Valid Loss 68.491 - Valid accuracy 0.652\n",
            "Epoch: 374/1000 - Train loss 68.215 - Train accuracy 0.679 - Valid Loss 68.504 - Valid accuracy 0.650\n",
            "Epoch: 375/1000 - Train loss 68.217 - Train accuracy 0.679 - Valid Loss 68.483 - Valid accuracy 0.652\n",
            "Epoch: 376/1000 - Train loss 68.220 - Train accuracy 0.678 - Valid Loss 68.489 - Valid accuracy 0.652\n",
            "Epoch: 377/1000 - Train loss 68.220 - Train accuracy 0.679 - Valid Loss 68.500 - Valid accuracy 0.650\n",
            "Epoch: 378/1000 - Train loss 68.224 - Train accuracy 0.678 - Valid Loss 68.500 - Valid accuracy 0.650\n",
            "Epoch: 379/1000 - Train loss 68.224 - Train accuracy 0.678 - Valid Loss 68.509 - Valid accuracy 0.650\n",
            "Epoch: 380/1000 - Train loss 68.225 - Train accuracy 0.678 - Valid Loss 68.503 - Valid accuracy 0.650\n",
            "Epoch: 381/1000 - Train loss 68.224 - Train accuracy 0.678 - Valid Loss 68.490 - Valid accuracy 0.652\n",
            "Epoch: 382/1000 - Train loss 68.221 - Train accuracy 0.678 - Valid Loss 68.510 - Valid accuracy 0.648\n",
            "Epoch: 383/1000 - Train loss 68.219 - Train accuracy 0.679 - Valid Loss 68.501 - Valid accuracy 0.650\n",
            "Epoch: 384/1000 - Train loss 68.219 - Train accuracy 0.679 - Valid Loss 68.491 - Valid accuracy 0.652\n",
            "Epoch: 385/1000 - Train loss 68.219 - Train accuracy 0.679 - Valid Loss 68.508 - Valid accuracy 0.649\n",
            "Epoch: 386/1000 - Train loss 68.217 - Train accuracy 0.679 - Valid Loss 68.501 - Valid accuracy 0.651\n",
            "Epoch: 387/1000 - Train loss 68.215 - Train accuracy 0.679 - Valid Loss 68.506 - Valid accuracy 0.648\n",
            "Epoch: 388/1000 - Train loss 68.216 - Train accuracy 0.679 - Valid Loss 68.503 - Valid accuracy 0.648\n",
            "Epoch: 389/1000 - Train loss 68.115 - Train accuracy 0.689 - Valid Loss 68.419 - Valid accuracy 0.658\n",
            "Epoch: 390/1000 - Train loss 68.110 - Train accuracy 0.689 - Valid Loss 68.401 - Valid accuracy 0.661\n",
            "Epoch: 391/1000 - Train loss 68.108 - Train accuracy 0.690 - Valid Loss 68.398 - Valid accuracy 0.660\n",
            "Epoch: 392/1000 - Train loss 68.108 - Train accuracy 0.690 - Valid Loss 68.398 - Valid accuracy 0.661\n",
            "Epoch: 393/1000 - Train loss 68.106 - Train accuracy 0.690 - Valid Loss 68.393 - Valid accuracy 0.661\n",
            "Epoch: 394/1000 - Train loss 68.104 - Train accuracy 0.690 - Valid Loss 68.390 - Valid accuracy 0.662\n",
            "Epoch: 395/1000 - Train loss 68.106 - Train accuracy 0.690 - Valid Loss 68.406 - Valid accuracy 0.661\n",
            "Epoch: 396/1000 - Train loss 68.108 - Train accuracy 0.690 - Valid Loss 68.402 - Valid accuracy 0.662\n",
            "Epoch: 397/1000 - Train loss 68.104 - Train accuracy 0.690 - Valid Loss 68.405 - Valid accuracy 0.660\n",
            "Epoch: 398/1000 - Train loss 68.107 - Train accuracy 0.690 - Valid Loss 68.402 - Valid accuracy 0.659\n",
            "Epoch: 399/1000 - Train loss 68.105 - Train accuracy 0.690 - Valid Loss 68.390 - Valid accuracy 0.662\n",
            "Epoch: 400/1000 - Train loss 68.105 - Train accuracy 0.690 - Valid Loss 68.394 - Valid accuracy 0.661\n",
            "Epoch: 401/1000 - Train loss 68.105 - Train accuracy 0.690 - Valid Loss 68.392 - Valid accuracy 0.662\n",
            "Epoch: 402/1000 - Train loss 68.104 - Train accuracy 0.690 - Valid Loss 68.399 - Valid accuracy 0.660\n",
            "Epoch: 403/1000 - Train loss 68.105 - Train accuracy 0.690 - Valid Loss 68.397 - Valid accuracy 0.661\n",
            "Epoch: 404/1000 - Train loss 68.106 - Train accuracy 0.690 - Valid Loss 68.395 - Valid accuracy 0.661\n",
            "Epoch: 405/1000 - Train loss 68.104 - Train accuracy 0.690 - Valid Loss 68.405 - Valid accuracy 0.660\n",
            "Epoch: 406/1000 - Train loss 68.105 - Train accuracy 0.690 - Valid Loss 68.411 - Valid accuracy 0.659\n",
            "Epoch: 407/1000 - Train loss 68.057 - Train accuracy 0.695 - Valid Loss 68.292 - Valid accuracy 0.672\n",
            "Epoch: 408/1000 - Train loss 68.047 - Train accuracy 0.696 - Valid Loss 68.305 - Valid accuracy 0.670\n",
            "Epoch: 409/1000 - Train loss 68.044 - Train accuracy 0.696 - Valid Loss 68.294 - Valid accuracy 0.671\n",
            "Epoch: 410/1000 - Train loss 68.045 - Train accuracy 0.696 - Valid Loss 68.305 - Valid accuracy 0.670\n",
            "Epoch: 411/1000 - Train loss 68.045 - Train accuracy 0.696 - Valid Loss 68.316 - Valid accuracy 0.668\n",
            "Epoch: 412/1000 - Train loss 68.042 - Train accuracy 0.696 - Valid Loss 68.303 - Valid accuracy 0.670\n",
            "Epoch: 413/1000 - Train loss 68.041 - Train accuracy 0.696 - Valid Loss 68.309 - Valid accuracy 0.670\n",
            "Epoch: 414/1000 - Train loss 68.039 - Train accuracy 0.697 - Valid Loss 68.296 - Valid accuracy 0.670\n",
            "Epoch: 415/1000 - Train loss 68.040 - Train accuracy 0.696 - Valid Loss 68.299 - Valid accuracy 0.670\n",
            "Epoch: 416/1000 - Train loss 68.034 - Train accuracy 0.697 - Valid Loss 68.287 - Valid accuracy 0.671\n",
            "Epoch: 417/1000 - Train loss 68.033 - Train accuracy 0.697 - Valid Loss 68.281 - Valid accuracy 0.672\n",
            "Epoch: 418/1000 - Train loss 68.034 - Train accuracy 0.697 - Valid Loss 68.277 - Valid accuracy 0.672\n",
            "Epoch: 419/1000 - Train loss 68.028 - Train accuracy 0.698 - Valid Loss 68.266 - Valid accuracy 0.673\n",
            "Epoch: 420/1000 - Train loss 68.028 - Train accuracy 0.698 - Valid Loss 68.263 - Valid accuracy 0.675\n",
            "Epoch: 421/1000 - Train loss 68.029 - Train accuracy 0.697 - Valid Loss 68.279 - Valid accuracy 0.673\n",
            "Epoch: 422/1000 - Train loss 68.026 - Train accuracy 0.698 - Valid Loss 68.263 - Valid accuracy 0.675\n",
            "Epoch: 423/1000 - Train loss 68.029 - Train accuracy 0.697 - Valid Loss 68.268 - Valid accuracy 0.673\n",
            "Epoch: 424/1000 - Train loss 68.027 - Train accuracy 0.698 - Valid Loss 68.266 - Valid accuracy 0.675\n",
            "Epoch: 425/1000 - Train loss 68.027 - Train accuracy 0.698 - Valid Loss 68.292 - Valid accuracy 0.670\n",
            "Epoch: 426/1000 - Train loss 68.026 - Train accuracy 0.698 - Valid Loss 68.286 - Valid accuracy 0.672\n",
            "Epoch: 427/1000 - Train loss 68.023 - Train accuracy 0.698 - Valid Loss 68.287 - Valid accuracy 0.671\n",
            "Epoch: 428/1000 - Train loss 68.024 - Train accuracy 0.698 - Valid Loss 68.280 - Valid accuracy 0.673\n",
            "Epoch: 429/1000 - Train loss 68.023 - Train accuracy 0.698 - Valid Loss 68.269 - Valid accuracy 0.674\n",
            "Epoch: 430/1000 - Train loss 68.023 - Train accuracy 0.698 - Valid Loss 68.278 - Valid accuracy 0.673\n",
            "Epoch: 431/1000 - Train loss 68.016 - Train accuracy 0.699 - Valid Loss 68.285 - Valid accuracy 0.673\n",
            "Epoch: 432/1000 - Train loss 68.018 - Train accuracy 0.699 - Valid Loss 68.285 - Valid accuracy 0.672\n",
            "Epoch: 433/1000 - Train loss 68.014 - Train accuracy 0.699 - Valid Loss 68.295 - Valid accuracy 0.671\n",
            "Epoch: 434/1000 - Train loss 68.014 - Train accuracy 0.699 - Valid Loss 68.292 - Valid accuracy 0.671\n",
            "Epoch: 435/1000 - Train loss 68.014 - Train accuracy 0.699 - Valid Loss 68.285 - Valid accuracy 0.672\n",
            "Epoch: 436/1000 - Train loss 68.010 - Train accuracy 0.699 - Valid Loss 68.281 - Valid accuracy 0.673\n",
            "Epoch: 437/1000 - Train loss 68.009 - Train accuracy 0.700 - Valid Loss 68.276 - Valid accuracy 0.673\n",
            "Epoch: 438/1000 - Train loss 68.009 - Train accuracy 0.700 - Valid Loss 68.287 - Valid accuracy 0.673\n",
            "Epoch: 439/1000 - Train loss 68.011 - Train accuracy 0.699 - Valid Loss 68.283 - Valid accuracy 0.673\n",
            "Epoch: 440/1000 - Train loss 68.009 - Train accuracy 0.700 - Valid Loss 68.304 - Valid accuracy 0.670\n",
            "Epoch: 441/1000 - Train loss 68.010 - Train accuracy 0.699 - Valid Loss 68.297 - Valid accuracy 0.670\n",
            "Epoch: 442/1000 - Train loss 68.011 - Train accuracy 0.699 - Valid Loss 68.285 - Valid accuracy 0.672\n",
            "Epoch: 443/1000 - Train loss 68.010 - Train accuracy 0.700 - Valid Loss 68.287 - Valid accuracy 0.671\n",
            "Epoch: 444/1000 - Train loss 68.008 - Train accuracy 0.700 - Valid Loss 68.283 - Valid accuracy 0.673\n",
            "Epoch: 445/1000 - Train loss 68.007 - Train accuracy 0.700 - Valid Loss 68.300 - Valid accuracy 0.670\n",
            "Epoch: 446/1000 - Train loss 68.008 - Train accuracy 0.700 - Valid Loss 68.295 - Valid accuracy 0.670\n",
            "Epoch: 447/1000 - Train loss 68.008 - Train accuracy 0.700 - Valid Loss 68.288 - Valid accuracy 0.672\n",
            "Epoch: 448/1000 - Train loss 68.009 - Train accuracy 0.700 - Valid Loss 68.279 - Valid accuracy 0.673\n",
            "Epoch: 449/1000 - Train loss 67.963 - Train accuracy 0.704 - Valid Loss 68.208 - Valid accuracy 0.680\n",
            "Epoch: 450/1000 - Train loss 67.948 - Train accuracy 0.706 - Valid Loss 68.199 - Valid accuracy 0.680\n",
            "Epoch: 451/1000 - Train loss 67.947 - Train accuracy 0.706 - Valid Loss 68.213 - Valid accuracy 0.680\n",
            "Epoch: 452/1000 - Train loss 67.946 - Train accuracy 0.706 - Valid Loss 68.219 - Valid accuracy 0.679\n",
            "Epoch: 453/1000 - Train loss 67.945 - Train accuracy 0.706 - Valid Loss 68.221 - Valid accuracy 0.679\n",
            "Epoch: 454/1000 - Train loss 67.943 - Train accuracy 0.706 - Valid Loss 68.212 - Valid accuracy 0.680\n",
            "Epoch: 455/1000 - Train loss 67.943 - Train accuracy 0.706 - Valid Loss 68.239 - Valid accuracy 0.675\n",
            "Epoch: 456/1000 - Train loss 67.942 - Train accuracy 0.706 - Valid Loss 68.228 - Valid accuracy 0.678\n",
            "Epoch: 457/1000 - Train loss 67.942 - Train accuracy 0.706 - Valid Loss 68.226 - Valid accuracy 0.677\n",
            "Epoch: 458/1000 - Train loss 67.941 - Train accuracy 0.706 - Valid Loss 68.238 - Valid accuracy 0.677\n",
            "Epoch: 459/1000 - Train loss 67.940 - Train accuracy 0.706 - Valid Loss 68.232 - Valid accuracy 0.677\n",
            "Epoch: 460/1000 - Train loss 67.938 - Train accuracy 0.707 - Valid Loss 68.248 - Valid accuracy 0.676\n",
            "Epoch: 461/1000 - Train loss 67.936 - Train accuracy 0.707 - Valid Loss 68.226 - Valid accuracy 0.677\n",
            "Epoch: 462/1000 - Train loss 67.926 - Train accuracy 0.708 - Valid Loss 68.225 - Valid accuracy 0.678\n",
            "Epoch: 463/1000 - Train loss 67.925 - Train accuracy 0.708 - Valid Loss 68.212 - Valid accuracy 0.679\n",
            "Epoch: 464/1000 - Train loss 67.924 - Train accuracy 0.708 - Valid Loss 68.208 - Valid accuracy 0.680\n",
            "Epoch: 465/1000 - Train loss 67.924 - Train accuracy 0.708 - Valid Loss 68.210 - Valid accuracy 0.680\n",
            "Epoch: 466/1000 - Train loss 67.921 - Train accuracy 0.708 - Valid Loss 68.196 - Valid accuracy 0.681\n",
            "Epoch: 467/1000 - Train loss 67.924 - Train accuracy 0.708 - Valid Loss 68.200 - Valid accuracy 0.680\n",
            "Epoch: 468/1000 - Train loss 67.920 - Train accuracy 0.708 - Valid Loss 68.199 - Valid accuracy 0.680\n",
            "Epoch: 469/1000 - Train loss 67.922 - Train accuracy 0.708 - Valid Loss 68.199 - Valid accuracy 0.680\n",
            "Epoch: 470/1000 - Train loss 67.922 - Train accuracy 0.708 - Valid Loss 68.198 - Valid accuracy 0.680\n",
            "Epoch: 471/1000 - Train loss 67.921 - Train accuracy 0.708 - Valid Loss 68.203 - Valid accuracy 0.680\n",
            "Epoch: 472/1000 - Train loss 67.921 - Train accuracy 0.708 - Valid Loss 68.199 - Valid accuracy 0.680\n",
            "Epoch: 473/1000 - Train loss 67.921 - Train accuracy 0.708 - Valid Loss 68.203 - Valid accuracy 0.680\n",
            "Epoch: 474/1000 - Train loss 67.918 - Train accuracy 0.709 - Valid Loss 68.202 - Valid accuracy 0.680\n",
            "Epoch: 475/1000 - Train loss 67.921 - Train accuracy 0.708 - Valid Loss 68.204 - Valid accuracy 0.680\n",
            "Epoch: 476/1000 - Train loss 67.923 - Train accuracy 0.708 - Valid Loss 68.203 - Valid accuracy 0.680\n",
            "Epoch: 477/1000 - Train loss 67.920 - Train accuracy 0.708 - Valid Loss 68.219 - Valid accuracy 0.678\n",
            "Epoch: 478/1000 - Train loss 67.918 - Train accuracy 0.709 - Valid Loss 68.225 - Valid accuracy 0.678\n",
            "Epoch: 479/1000 - Train loss 67.918 - Train accuracy 0.709 - Valid Loss 68.211 - Valid accuracy 0.679\n",
            "Epoch: 480/1000 - Train loss 67.917 - Train accuracy 0.709 - Valid Loss 68.215 - Valid accuracy 0.680\n",
            "Epoch: 481/1000 - Train loss 67.917 - Train accuracy 0.709 - Valid Loss 68.216 - Valid accuracy 0.679\n",
            "Epoch: 482/1000 - Train loss 67.914 - Train accuracy 0.709 - Valid Loss 68.214 - Valid accuracy 0.679\n",
            "Epoch: 483/1000 - Train loss 67.915 - Train accuracy 0.709 - Valid Loss 68.219 - Valid accuracy 0.679\n",
            "Epoch: 484/1000 - Train loss 67.915 - Train accuracy 0.709 - Valid Loss 68.216 - Valid accuracy 0.679\n",
            "Epoch: 485/1000 - Train loss 67.914 - Train accuracy 0.709 - Valid Loss 68.217 - Valid accuracy 0.679\n",
            "Epoch: 486/1000 - Train loss 67.914 - Train accuracy 0.709 - Valid Loss 68.219 - Valid accuracy 0.679\n",
            "Epoch: 487/1000 - Train loss 67.914 - Train accuracy 0.709 - Valid Loss 68.218 - Valid accuracy 0.679\n",
            "Epoch: 488/1000 - Train loss 67.915 - Train accuracy 0.709 - Valid Loss 68.216 - Valid accuracy 0.679\n",
            "Epoch: 489/1000 - Train loss 67.913 - Train accuracy 0.709 - Valid Loss 68.216 - Valid accuracy 0.679\n",
            "Epoch: 490/1000 - Train loss 67.912 - Train accuracy 0.709 - Valid Loss 68.217 - Valid accuracy 0.679\n",
            "Epoch: 491/1000 - Train loss 67.913 - Train accuracy 0.709 - Valid Loss 68.212 - Valid accuracy 0.679\n",
            "Epoch: 492/1000 - Train loss 67.913 - Train accuracy 0.709 - Valid Loss 68.213 - Valid accuracy 0.679\n",
            "Epoch: 493/1000 - Train loss 67.914 - Train accuracy 0.709 - Valid Loss 68.214 - Valid accuracy 0.679\n",
            "Epoch: 494/1000 - Train loss 67.910 - Train accuracy 0.709 - Valid Loss 68.213 - Valid accuracy 0.679\n",
            "Epoch: 495/1000 - Train loss 67.910 - Train accuracy 0.709 - Valid Loss 68.211 - Valid accuracy 0.680\n",
            "Epoch: 496/1000 - Train loss 67.909 - Train accuracy 0.710 - Valid Loss 68.212 - Valid accuracy 0.679\n",
            "Epoch: 497/1000 - Train loss 67.905 - Train accuracy 0.710 - Valid Loss 68.200 - Valid accuracy 0.680\n",
            "Epoch: 498/1000 - Train loss 67.906 - Train accuracy 0.710 - Valid Loss 68.214 - Valid accuracy 0.680\n",
            "Epoch: 499/1000 - Train loss 67.905 - Train accuracy 0.710 - Valid Loss 68.211 - Valid accuracy 0.680\n",
            "Epoch: 500/1000 - Train loss 67.913 - Train accuracy 0.709 - Valid Loss 68.248 - Valid accuracy 0.677\n",
            "Epoch: 501/1000 - Train loss 67.909 - Train accuracy 0.710 - Valid Loss 68.235 - Valid accuracy 0.677\n",
            "Epoch: 502/1000 - Train loss 67.911 - Train accuracy 0.710 - Valid Loss 68.260 - Valid accuracy 0.674\n",
            "Epoch: 503/1000 - Train loss 67.910 - Train accuracy 0.710 - Valid Loss 68.198 - Valid accuracy 0.680\n",
            "Epoch: 504/1000 - Train loss 67.910 - Train accuracy 0.710 - Valid Loss 68.206 - Valid accuracy 0.680\n",
            "Epoch: 505/1000 - Train loss 67.904 - Train accuracy 0.710 - Valid Loss 68.217 - Valid accuracy 0.679\n",
            "Epoch: 506/1000 - Train loss 67.902 - Train accuracy 0.710 - Valid Loss 68.207 - Valid accuracy 0.681\n",
            "Epoch: 507/1000 - Train loss 67.901 - Train accuracy 0.710 - Valid Loss 68.217 - Valid accuracy 0.678\n",
            "Epoch: 508/1000 - Train loss 67.899 - Train accuracy 0.711 - Valid Loss 68.216 - Valid accuracy 0.678\n",
            "Epoch: 509/1000 - Train loss 67.901 - Train accuracy 0.710 - Valid Loss 68.208 - Valid accuracy 0.680\n",
            "Epoch: 510/1000 - Train loss 67.898 - Train accuracy 0.711 - Valid Loss 68.213 - Valid accuracy 0.679\n",
            "Epoch: 511/1000 - Train loss 67.902 - Train accuracy 0.710 - Valid Loss 68.206 - Valid accuracy 0.680\n",
            "Epoch: 512/1000 - Train loss 67.898 - Train accuracy 0.711 - Valid Loss 68.207 - Valid accuracy 0.680\n",
            "Epoch: 513/1000 - Train loss 67.900 - Train accuracy 0.710 - Valid Loss 68.210 - Valid accuracy 0.680\n",
            "Epoch: 514/1000 - Train loss 67.899 - Train accuracy 0.710 - Valid Loss 68.203 - Valid accuracy 0.680\n",
            "Epoch: 515/1000 - Train loss 67.899 - Train accuracy 0.711 - Valid Loss 68.202 - Valid accuracy 0.680\n",
            "Epoch: 516/1000 - Train loss 67.895 - Train accuracy 0.711 - Valid Loss 68.205 - Valid accuracy 0.680\n",
            "Epoch: 517/1000 - Train loss 67.896 - Train accuracy 0.711 - Valid Loss 68.204 - Valid accuracy 0.680\n",
            "Epoch: 518/1000 - Train loss 67.896 - Train accuracy 0.711 - Valid Loss 68.204 - Valid accuracy 0.680\n",
            "Epoch: 519/1000 - Train loss 67.897 - Train accuracy 0.711 - Valid Loss 68.204 - Valid accuracy 0.680\n",
            "Epoch: 520/1000 - Train loss 67.894 - Train accuracy 0.711 - Valid Loss 68.190 - Valid accuracy 0.681\n",
            "Epoch: 521/1000 - Train loss 67.895 - Train accuracy 0.711 - Valid Loss 68.193 - Valid accuracy 0.681\n",
            "Epoch: 522/1000 - Train loss 67.891 - Train accuracy 0.711 - Valid Loss 68.189 - Valid accuracy 0.681\n",
            "Epoch: 523/1000 - Train loss 67.888 - Train accuracy 0.712 - Valid Loss 68.179 - Valid accuracy 0.683\n",
            "Epoch: 524/1000 - Train loss 67.891 - Train accuracy 0.711 - Valid Loss 68.195 - Valid accuracy 0.681\n",
            "Epoch: 525/1000 - Train loss 67.885 - Train accuracy 0.712 - Valid Loss 68.184 - Valid accuracy 0.681\n",
            "Epoch: 526/1000 - Train loss 67.887 - Train accuracy 0.712 - Valid Loss 68.190 - Valid accuracy 0.681\n",
            "Epoch: 527/1000 - Train loss 67.884 - Train accuracy 0.712 - Valid Loss 68.222 - Valid accuracy 0.678\n",
            "Epoch: 528/1000 - Train loss 67.882 - Train accuracy 0.712 - Valid Loss 68.190 - Valid accuracy 0.681\n",
            "Epoch: 529/1000 - Train loss 67.881 - Train accuracy 0.712 - Valid Loss 68.238 - Valid accuracy 0.675\n",
            "Epoch: 530/1000 - Train loss 67.879 - Train accuracy 0.713 - Valid Loss 68.219 - Valid accuracy 0.680\n",
            "Epoch: 531/1000 - Train loss 67.879 - Train accuracy 0.713 - Valid Loss 68.181 - Valid accuracy 0.683\n",
            "Epoch: 532/1000 - Train loss 67.878 - Train accuracy 0.713 - Valid Loss 68.193 - Valid accuracy 0.681\n",
            "Epoch: 533/1000 - Train loss 67.878 - Train accuracy 0.713 - Valid Loss 68.202 - Valid accuracy 0.680\n",
            "Epoch: 534/1000 - Train loss 67.878 - Train accuracy 0.713 - Valid Loss 68.166 - Valid accuracy 0.684\n",
            "Epoch: 535/1000 - Train loss 67.877 - Train accuracy 0.713 - Valid Loss 68.176 - Valid accuracy 0.683\n",
            "Epoch: 536/1000 - Train loss 67.870 - Train accuracy 0.713 - Valid Loss 68.162 - Valid accuracy 0.684\n",
            "Epoch: 537/1000 - Train loss 67.867 - Train accuracy 0.714 - Valid Loss 68.185 - Valid accuracy 0.681\n",
            "Epoch: 538/1000 - Train loss 67.870 - Train accuracy 0.714 - Valid Loss 68.174 - Valid accuracy 0.683\n",
            "Epoch: 539/1000 - Train loss 67.867 - Train accuracy 0.714 - Valid Loss 68.191 - Valid accuracy 0.681\n",
            "Epoch: 540/1000 - Train loss 67.866 - Train accuracy 0.714 - Valid Loss 68.185 - Valid accuracy 0.682\n",
            "Epoch: 541/1000 - Train loss 67.863 - Train accuracy 0.714 - Valid Loss 68.186 - Valid accuracy 0.682\n",
            "Epoch: 542/1000 - Train loss 67.864 - Train accuracy 0.714 - Valid Loss 68.167 - Valid accuracy 0.685\n",
            "Epoch: 543/1000 - Train loss 67.864 - Train accuracy 0.714 - Valid Loss 68.157 - Valid accuracy 0.686\n",
            "Epoch: 544/1000 - Train loss 67.861 - Train accuracy 0.714 - Valid Loss 68.165 - Valid accuracy 0.684\n",
            "Epoch: 545/1000 - Train loss 67.858 - Train accuracy 0.715 - Valid Loss 68.180 - Valid accuracy 0.681\n",
            "Epoch: 546/1000 - Train loss 67.860 - Train accuracy 0.714 - Valid Loss 68.161 - Valid accuracy 0.684\n",
            "Epoch: 547/1000 - Train loss 67.847 - Train accuracy 0.716 - Valid Loss 68.158 - Valid accuracy 0.684\n",
            "Epoch: 548/1000 - Train loss 67.846 - Train accuracy 0.716 - Valid Loss 68.153 - Valid accuracy 0.685\n",
            "Epoch: 549/1000 - Train loss 67.844 - Train accuracy 0.716 - Valid Loss 68.167 - Valid accuracy 0.684\n",
            "Epoch: 550/1000 - Train loss 67.843 - Train accuracy 0.716 - Valid Loss 68.155 - Valid accuracy 0.685\n",
            "Epoch: 551/1000 - Train loss 67.841 - Train accuracy 0.716 - Valid Loss 68.145 - Valid accuracy 0.687\n",
            "Epoch: 552/1000 - Train loss 67.841 - Train accuracy 0.716 - Valid Loss 68.163 - Valid accuracy 0.684\n",
            "Epoch: 553/1000 - Train loss 67.838 - Train accuracy 0.717 - Valid Loss 68.171 - Valid accuracy 0.683\n",
            "Epoch: 554/1000 - Train loss 67.837 - Train accuracy 0.717 - Valid Loss 68.162 - Valid accuracy 0.685\n",
            "Epoch: 555/1000 - Train loss 67.835 - Train accuracy 0.717 - Valid Loss 68.157 - Valid accuracy 0.685\n",
            "Epoch: 556/1000 - Train loss 67.834 - Train accuracy 0.717 - Valid Loss 68.165 - Valid accuracy 0.684\n",
            "Epoch: 557/1000 - Train loss 67.836 - Train accuracy 0.717 - Valid Loss 68.166 - Valid accuracy 0.683\n",
            "Epoch: 558/1000 - Train loss 67.834 - Train accuracy 0.717 - Valid Loss 68.164 - Valid accuracy 0.684\n",
            "Epoch: 559/1000 - Train loss 67.836 - Train accuracy 0.717 - Valid Loss 68.171 - Valid accuracy 0.683\n",
            "Epoch: 560/1000 - Train loss 67.831 - Train accuracy 0.717 - Valid Loss 68.170 - Valid accuracy 0.683\n",
            "Epoch: 561/1000 - Train loss 67.834 - Train accuracy 0.717 - Valid Loss 68.175 - Valid accuracy 0.683\n",
            "Epoch: 562/1000 - Train loss 67.834 - Train accuracy 0.717 - Valid Loss 68.179 - Valid accuracy 0.682\n",
            "Epoch: 563/1000 - Train loss 67.832 - Train accuracy 0.717 - Valid Loss 68.183 - Valid accuracy 0.681\n",
            "Epoch: 564/1000 - Train loss 67.830 - Train accuracy 0.717 - Valid Loss 68.157 - Valid accuracy 0.687\n",
            "Epoch: 565/1000 - Train loss 67.832 - Train accuracy 0.717 - Valid Loss 68.175 - Valid accuracy 0.682\n",
            "Epoch: 566/1000 - Train loss 67.830 - Train accuracy 0.717 - Valid Loss 68.177 - Valid accuracy 0.682\n",
            "Epoch: 567/1000 - Train loss 67.830 - Train accuracy 0.717 - Valid Loss 68.194 - Valid accuracy 0.680\n",
            "Epoch: 568/1000 - Train loss 67.831 - Train accuracy 0.717 - Valid Loss 68.196 - Valid accuracy 0.680\n",
            "Epoch: 569/1000 - Train loss 67.829 - Train accuracy 0.718 - Valid Loss 68.195 - Valid accuracy 0.680\n",
            "Epoch: 570/1000 - Train loss 67.830 - Train accuracy 0.717 - Valid Loss 68.194 - Valid accuracy 0.680\n",
            "Epoch: 571/1000 - Train loss 67.830 - Train accuracy 0.717 - Valid Loss 68.196 - Valid accuracy 0.680\n",
            "Epoch: 572/1000 - Train loss 67.830 - Train accuracy 0.717 - Valid Loss 68.188 - Valid accuracy 0.681\n",
            "Epoch: 573/1000 - Train loss 67.829 - Train accuracy 0.717 - Valid Loss 68.195 - Valid accuracy 0.680\n",
            "Epoch: 574/1000 - Train loss 67.829 - Train accuracy 0.718 - Valid Loss 68.182 - Valid accuracy 0.682\n",
            "Epoch: 575/1000 - Train loss 67.810 - Train accuracy 0.719 - Valid Loss 68.176 - Valid accuracy 0.683\n",
            "Epoch: 576/1000 - Train loss 67.802 - Train accuracy 0.720 - Valid Loss 68.201 - Valid accuracy 0.679\n",
            "Epoch: 577/1000 - Train loss 67.800 - Train accuracy 0.721 - Valid Loss 68.201 - Valid accuracy 0.680\n",
            "Epoch: 578/1000 - Train loss 67.798 - Train accuracy 0.721 - Valid Loss 68.204 - Valid accuracy 0.680\n",
            "Epoch: 579/1000 - Train loss 67.797 - Train accuracy 0.721 - Valid Loss 68.220 - Valid accuracy 0.680\n",
            "Epoch: 580/1000 - Train loss 67.797 - Train accuracy 0.721 - Valid Loss 68.223 - Valid accuracy 0.678\n",
            "Epoch: 581/1000 - Train loss 67.796 - Train accuracy 0.721 - Valid Loss 68.200 - Valid accuracy 0.679\n",
            "Epoch: 582/1000 - Train loss 67.797 - Train accuracy 0.721 - Valid Loss 68.198 - Valid accuracy 0.680\n",
            "Epoch: 583/1000 - Train loss 67.799 - Train accuracy 0.721 - Valid Loss 68.198 - Valid accuracy 0.680\n",
            "Epoch: 584/1000 - Train loss 67.799 - Train accuracy 0.721 - Valid Loss 68.190 - Valid accuracy 0.684\n",
            "Epoch: 585/1000 - Train loss 67.797 - Train accuracy 0.721 - Valid Loss 68.173 - Valid accuracy 0.682\n",
            "Epoch: 586/1000 - Train loss 67.795 - Train accuracy 0.721 - Valid Loss 68.184 - Valid accuracy 0.683\n",
            "Epoch: 587/1000 - Train loss 67.795 - Train accuracy 0.721 - Valid Loss 68.161 - Valid accuracy 0.684\n",
            "Epoch: 588/1000 - Train loss 67.795 - Train accuracy 0.721 - Valid Loss 68.152 - Valid accuracy 0.686\n",
            "Epoch: 589/1000 - Train loss 67.791 - Train accuracy 0.721 - Valid Loss 68.165 - Valid accuracy 0.684\n",
            "Epoch: 590/1000 - Train loss 67.791 - Train accuracy 0.721 - Valid Loss 68.154 - Valid accuracy 0.684\n",
            "Epoch: 591/1000 - Train loss 67.789 - Train accuracy 0.722 - Valid Loss 68.152 - Valid accuracy 0.684\n",
            "Epoch: 592/1000 - Train loss 67.787 - Train accuracy 0.722 - Valid Loss 68.171 - Valid accuracy 0.683\n",
            "Epoch: 593/1000 - Train loss 67.786 - Train accuracy 0.722 - Valid Loss 68.175 - Valid accuracy 0.683\n",
            "Epoch: 594/1000 - Train loss 67.786 - Train accuracy 0.722 - Valid Loss 68.161 - Valid accuracy 0.684\n",
            "Epoch: 595/1000 - Train loss 67.789 - Train accuracy 0.722 - Valid Loss 68.176 - Valid accuracy 0.683\n",
            "Epoch: 596/1000 - Train loss 67.789 - Train accuracy 0.722 - Valid Loss 68.165 - Valid accuracy 0.684\n",
            "Epoch: 597/1000 - Train loss 67.786 - Train accuracy 0.722 - Valid Loss 68.163 - Valid accuracy 0.684\n",
            "Epoch: 598/1000 - Train loss 67.784 - Train accuracy 0.722 - Valid Loss 68.165 - Valid accuracy 0.684\n",
            "Epoch: 599/1000 - Train loss 67.783 - Train accuracy 0.722 - Valid Loss 68.178 - Valid accuracy 0.684\n",
            "Epoch: 600/1000 - Train loss 67.780 - Train accuracy 0.722 - Valid Loss 68.189 - Valid accuracy 0.682\n",
            "Epoch: 601/1000 - Train loss 67.778 - Train accuracy 0.723 - Valid Loss 68.189 - Valid accuracy 0.680\n",
            "Epoch: 602/1000 - Train loss 67.779 - Train accuracy 0.723 - Valid Loss 68.167 - Valid accuracy 0.684\n",
            "Epoch: 603/1000 - Train loss 67.776 - Train accuracy 0.723 - Valid Loss 68.181 - Valid accuracy 0.682\n",
            "Epoch: 604/1000 - Train loss 67.776 - Train accuracy 0.723 - Valid Loss 68.172 - Valid accuracy 0.684\n",
            "Epoch: 605/1000 - Train loss 67.775 - Train accuracy 0.723 - Valid Loss 68.173 - Valid accuracy 0.684\n",
            "Epoch: 606/1000 - Train loss 67.773 - Train accuracy 0.723 - Valid Loss 68.175 - Valid accuracy 0.683\n",
            "Epoch: 607/1000 - Train loss 67.776 - Train accuracy 0.723 - Valid Loss 68.172 - Valid accuracy 0.684\n",
            "Epoch: 608/1000 - Train loss 67.774 - Train accuracy 0.723 - Valid Loss 68.174 - Valid accuracy 0.682\n",
            "Epoch: 609/1000 - Train loss 67.775 - Train accuracy 0.723 - Valid Loss 68.173 - Valid accuracy 0.683\n",
            "Epoch: 610/1000 - Train loss 67.773 - Train accuracy 0.723 - Valid Loss 68.173 - Valid accuracy 0.683\n",
            "Epoch: 611/1000 - Train loss 67.775 - Train accuracy 0.723 - Valid Loss 68.174 - Valid accuracy 0.683\n",
            "Epoch: 612/1000 - Train loss 67.774 - Train accuracy 0.723 - Valid Loss 68.174 - Valid accuracy 0.683\n",
            "Epoch: 613/1000 - Train loss 67.773 - Train accuracy 0.723 - Valid Loss 68.174 - Valid accuracy 0.683\n",
            "Epoch: 614/1000 - Train loss 67.769 - Train accuracy 0.723 - Valid Loss 68.181 - Valid accuracy 0.682\n",
            "Epoch: 615/1000 - Train loss 67.773 - Train accuracy 0.723 - Valid Loss 68.182 - Valid accuracy 0.682\n",
            "Epoch: 616/1000 - Train loss 67.773 - Train accuracy 0.723 - Valid Loss 68.166 - Valid accuracy 0.685\n",
            "Epoch: 617/1000 - Train loss 67.771 - Train accuracy 0.723 - Valid Loss 68.175 - Valid accuracy 0.684\n",
            "Epoch: 618/1000 - Train loss 67.771 - Train accuracy 0.723 - Valid Loss 68.172 - Valid accuracy 0.684\n",
            "Epoch: 619/1000 - Train loss 67.770 - Train accuracy 0.723 - Valid Loss 68.168 - Valid accuracy 0.684\n",
            "Epoch: 620/1000 - Train loss 67.768 - Train accuracy 0.724 - Valid Loss 68.170 - Valid accuracy 0.684\n",
            "Epoch: 621/1000 - Train loss 67.767 - Train accuracy 0.724 - Valid Loss 68.169 - Valid accuracy 0.685\n",
            "Epoch: 622/1000 - Train loss 67.768 - Train accuracy 0.724 - Valid Loss 68.162 - Valid accuracy 0.685\n",
            "Epoch: 623/1000 - Train loss 67.767 - Train accuracy 0.724 - Valid Loss 68.161 - Valid accuracy 0.685\n",
            "Epoch: 624/1000 - Train loss 67.766 - Train accuracy 0.724 - Valid Loss 68.164 - Valid accuracy 0.685\n",
            "Epoch: 625/1000 - Train loss 67.767 - Train accuracy 0.724 - Valid Loss 68.176 - Valid accuracy 0.682\n",
            "Epoch: 626/1000 - Train loss 67.768 - Train accuracy 0.724 - Valid Loss 68.190 - Valid accuracy 0.680\n",
            "Epoch: 627/1000 - Train loss 67.768 - Train accuracy 0.724 - Valid Loss 68.195 - Valid accuracy 0.680\n",
            "Epoch: 628/1000 - Train loss 67.764 - Train accuracy 0.724 - Valid Loss 68.192 - Valid accuracy 0.680\n",
            "Epoch: 629/1000 - Train loss 67.766 - Train accuracy 0.724 - Valid Loss 68.196 - Valid accuracy 0.680\n",
            "Epoch: 630/1000 - Train loss 67.764 - Train accuracy 0.724 - Valid Loss 68.190 - Valid accuracy 0.680\n",
            "Epoch: 631/1000 - Train loss 67.768 - Train accuracy 0.724 - Valid Loss 68.180 - Valid accuracy 0.682\n",
            "Epoch: 632/1000 - Train loss 67.764 - Train accuracy 0.724 - Valid Loss 68.188 - Valid accuracy 0.682\n",
            "Epoch: 633/1000 - Train loss 67.766 - Train accuracy 0.724 - Valid Loss 68.173 - Valid accuracy 0.684\n",
            "Epoch: 634/1000 - Train loss 67.765 - Train accuracy 0.724 - Valid Loss 68.164 - Valid accuracy 0.687\n",
            "Epoch: 635/1000 - Train loss 67.764 - Train accuracy 0.724 - Valid Loss 68.180 - Valid accuracy 0.684\n",
            "Epoch: 636/1000 - Train loss 67.763 - Train accuracy 0.724 - Valid Loss 68.165 - Valid accuracy 0.685\n",
            "Epoch: 637/1000 - Train loss 67.763 - Train accuracy 0.724 - Valid Loss 68.185 - Valid accuracy 0.682\n",
            "Epoch: 638/1000 - Train loss 67.765 - Train accuracy 0.724 - Valid Loss 68.202 - Valid accuracy 0.679\n",
            "Epoch: 639/1000 - Train loss 67.766 - Train accuracy 0.724 - Valid Loss 68.183 - Valid accuracy 0.683\n",
            "Epoch: 640/1000 - Train loss 67.766 - Train accuracy 0.724 - Valid Loss 68.192 - Valid accuracy 0.681\n",
            "Epoch: 641/1000 - Train loss 67.769 - Train accuracy 0.724 - Valid Loss 68.133 - Valid accuracy 0.688\n",
            "Epoch: 642/1000 - Train loss 67.772 - Train accuracy 0.723 - Valid Loss 68.179 - Valid accuracy 0.682\n",
            "Epoch: 643/1000 - Train loss 67.772 - Train accuracy 0.723 - Valid Loss 68.193 - Valid accuracy 0.681\n",
            "Epoch: 644/1000 - Train loss 67.770 - Train accuracy 0.724 - Valid Loss 68.211 - Valid accuracy 0.680\n",
            "Epoch: 645/1000 - Train loss 67.771 - Train accuracy 0.723 - Valid Loss 68.184 - Valid accuracy 0.683\n",
            "Epoch: 646/1000 - Train loss 67.771 - Train accuracy 0.723 - Valid Loss 68.191 - Valid accuracy 0.680\n",
            "Epoch: 647/1000 - Train loss 67.766 - Train accuracy 0.724 - Valid Loss 68.191 - Valid accuracy 0.682\n",
            "Epoch: 648/1000 - Train loss 67.769 - Train accuracy 0.724 - Valid Loss 68.208 - Valid accuracy 0.680\n",
            "Epoch: 649/1000 - Train loss 67.765 - Train accuracy 0.724 - Valid Loss 68.194 - Valid accuracy 0.680\n",
            "Epoch: 650/1000 - Train loss 67.767 - Train accuracy 0.724 - Valid Loss 68.177 - Valid accuracy 0.682\n",
            "Epoch: 651/1000 - Train loss 67.765 - Train accuracy 0.724 - Valid Loss 68.188 - Valid accuracy 0.681\n",
            "Epoch: 652/1000 - Train loss 67.761 - Train accuracy 0.724 - Valid Loss 68.201 - Valid accuracy 0.680\n",
            "Epoch: 653/1000 - Train loss 67.760 - Train accuracy 0.724 - Valid Loss 68.220 - Valid accuracy 0.678\n",
            "Epoch: 654/1000 - Train loss 67.763 - Train accuracy 0.724 - Valid Loss 68.200 - Valid accuracy 0.680\n",
            "Epoch: 655/1000 - Train loss 67.760 - Train accuracy 0.724 - Valid Loss 68.195 - Valid accuracy 0.680\n",
            "Epoch: 656/1000 - Train loss 67.761 - Train accuracy 0.724 - Valid Loss 68.192 - Valid accuracy 0.681\n",
            "Epoch: 657/1000 - Train loss 67.762 - Train accuracy 0.724 - Valid Loss 68.199 - Valid accuracy 0.681\n",
            "Epoch: 658/1000 - Train loss 67.761 - Train accuracy 0.724 - Valid Loss 68.198 - Valid accuracy 0.680\n",
            "Epoch: 659/1000 - Train loss 67.760 - Train accuracy 0.724 - Valid Loss 68.189 - Valid accuracy 0.683\n",
            "Epoch: 660/1000 - Train loss 67.760 - Train accuracy 0.724 - Valid Loss 68.185 - Valid accuracy 0.683\n",
            "Epoch: 661/1000 - Train loss 67.762 - Train accuracy 0.724 - Valid Loss 68.189 - Valid accuracy 0.682\n",
            "Epoch: 662/1000 - Train loss 67.760 - Train accuracy 0.724 - Valid Loss 68.188 - Valid accuracy 0.681\n",
            "Epoch: 663/1000 - Train loss 67.759 - Train accuracy 0.725 - Valid Loss 68.204 - Valid accuracy 0.680\n",
            "Epoch: 664/1000 - Train loss 67.756 - Train accuracy 0.725 - Valid Loss 68.203 - Valid accuracy 0.680\n",
            "Epoch: 665/1000 - Train loss 67.758 - Train accuracy 0.725 - Valid Loss 68.206 - Valid accuracy 0.680\n",
            "Epoch: 666/1000 - Train loss 67.759 - Train accuracy 0.724 - Valid Loss 68.204 - Valid accuracy 0.678\n",
            "Epoch: 667/1000 - Train loss 67.757 - Train accuracy 0.725 - Valid Loss 68.211 - Valid accuracy 0.678\n",
            "Epoch: 668/1000 - Train loss 67.758 - Train accuracy 0.725 - Valid Loss 68.210 - Valid accuracy 0.680\n",
            "Epoch: 669/1000 - Train loss 67.756 - Train accuracy 0.725 - Valid Loss 68.210 - Valid accuracy 0.678\n",
            "Epoch: 670/1000 - Train loss 67.757 - Train accuracy 0.725 - Valid Loss 68.210 - Valid accuracy 0.679\n",
            "Epoch: 671/1000 - Train loss 67.755 - Train accuracy 0.725 - Valid Loss 68.201 - Valid accuracy 0.679\n",
            "Epoch: 672/1000 - Train loss 67.759 - Train accuracy 0.724 - Valid Loss 68.205 - Valid accuracy 0.679\n",
            "Epoch: 673/1000 - Train loss 67.757 - Train accuracy 0.725 - Valid Loss 68.201 - Valid accuracy 0.680\n",
            "Epoch: 674/1000 - Train loss 67.757 - Train accuracy 0.725 - Valid Loss 68.203 - Valid accuracy 0.679\n",
            "Epoch: 675/1000 - Train loss 67.757 - Train accuracy 0.725 - Valid Loss 68.201 - Valid accuracy 0.679\n",
            "Epoch: 676/1000 - Train loss 67.756 - Train accuracy 0.725 - Valid Loss 68.198 - Valid accuracy 0.681\n",
            "Epoch: 677/1000 - Train loss 67.756 - Train accuracy 0.725 - Valid Loss 68.201 - Valid accuracy 0.680\n",
            "Epoch: 678/1000 - Train loss 67.758 - Train accuracy 0.725 - Valid Loss 68.190 - Valid accuracy 0.682\n",
            "Epoch: 679/1000 - Train loss 67.758 - Train accuracy 0.725 - Valid Loss 68.198 - Valid accuracy 0.681\n",
            "Epoch: 680/1000 - Train loss 67.757 - Train accuracy 0.725 - Valid Loss 68.192 - Valid accuracy 0.682\n",
            "Epoch: 681/1000 - Train loss 67.755 - Train accuracy 0.725 - Valid Loss 68.194 - Valid accuracy 0.682\n",
            "Epoch: 682/1000 - Train loss 67.754 - Train accuracy 0.725 - Valid Loss 68.193 - Valid accuracy 0.682\n",
            "Epoch: 683/1000 - Train loss 67.756 - Train accuracy 0.725 - Valid Loss 68.194 - Valid accuracy 0.681\n",
            "Epoch: 684/1000 - Train loss 67.755 - Train accuracy 0.725 - Valid Loss 68.203 - Valid accuracy 0.680\n",
            "Epoch: 685/1000 - Train loss 67.756 - Train accuracy 0.725 - Valid Loss 68.205 - Valid accuracy 0.679\n",
            "Epoch: 686/1000 - Train loss 67.755 - Train accuracy 0.725 - Valid Loss 68.198 - Valid accuracy 0.681\n",
            "Epoch: 687/1000 - Train loss 67.758 - Train accuracy 0.725 - Valid Loss 68.206 - Valid accuracy 0.679\n",
            "Epoch: 688/1000 - Train loss 67.755 - Train accuracy 0.725 - Valid Loss 68.205 - Valid accuracy 0.679\n",
            "Epoch: 689/1000 - Train loss 67.756 - Train accuracy 0.725 - Valid Loss 68.209 - Valid accuracy 0.680\n",
            "Epoch: 690/1000 - Train loss 67.756 - Train accuracy 0.725 - Valid Loss 68.207 - Valid accuracy 0.680\n",
            "Epoch: 691/1000 - Train loss 67.755 - Train accuracy 0.725 - Valid Loss 68.207 - Valid accuracy 0.680\n",
            "Epoch: 692/1000 - Train loss 67.755 - Train accuracy 0.725 - Valid Loss 68.204 - Valid accuracy 0.680\n",
            "Epoch: 693/1000 - Train loss 67.756 - Train accuracy 0.725 - Valid Loss 68.211 - Valid accuracy 0.679\n",
            "Epoch: 694/1000 - Train loss 67.755 - Train accuracy 0.725 - Valid Loss 68.192 - Valid accuracy 0.680\n",
            "Epoch: 695/1000 - Train loss 67.755 - Train accuracy 0.725 - Valid Loss 68.172 - Valid accuracy 0.683\n",
            "Epoch: 696/1000 - Train loss 67.754 - Train accuracy 0.725 - Valid Loss 68.191 - Valid accuracy 0.681\n",
            "Epoch: 697/1000 - Train loss 67.755 - Train accuracy 0.725 - Valid Loss 68.143 - Valid accuracy 0.686\n",
            "Epoch: 698/1000 - Train loss 67.756 - Train accuracy 0.725 - Valid Loss 68.162 - Valid accuracy 0.684\n",
            "Epoch: 699/1000 - Train loss 67.757 - Train accuracy 0.725 - Valid Loss 68.168 - Valid accuracy 0.685\n",
            "Epoch: 700/1000 - Train loss 67.758 - Train accuracy 0.725 - Valid Loss 68.145 - Valid accuracy 0.686\n",
            "Epoch: 701/1000 - Train loss 67.758 - Train accuracy 0.725 - Valid Loss 68.149 - Valid accuracy 0.686\n",
            "Epoch: 702/1000 - Train loss 67.758 - Train accuracy 0.725 - Valid Loss 68.146 - Valid accuracy 0.686\n",
            "Epoch: 703/1000 - Train loss 67.756 - Train accuracy 0.725 - Valid Loss 68.141 - Valid accuracy 0.686\n",
            "Epoch: 704/1000 - Train loss 67.756 - Train accuracy 0.725 - Valid Loss 68.141 - Valid accuracy 0.687\n",
            "Epoch: 705/1000 - Train loss 67.755 - Train accuracy 0.725 - Valid Loss 68.145 - Valid accuracy 0.687\n",
            "Epoch: 706/1000 - Train loss 67.755 - Train accuracy 0.725 - Valid Loss 68.147 - Valid accuracy 0.686\n",
            "Epoch: 707/1000 - Train loss 67.751 - Train accuracy 0.725 - Valid Loss 68.144 - Valid accuracy 0.686\n",
            "Epoch: 708/1000 - Train loss 67.748 - Train accuracy 0.726 - Valid Loss 68.144 - Valid accuracy 0.687\n",
            "Epoch: 709/1000 - Train loss 67.747 - Train accuracy 0.726 - Valid Loss 68.139 - Valid accuracy 0.687\n",
            "Epoch: 710/1000 - Train loss 67.749 - Train accuracy 0.726 - Valid Loss 68.146 - Valid accuracy 0.686\n",
            "Epoch: 711/1000 - Train loss 67.747 - Train accuracy 0.726 - Valid Loss 68.143 - Valid accuracy 0.686\n",
            "Epoch: 712/1000 - Train loss 67.746 - Train accuracy 0.726 - Valid Loss 68.151 - Valid accuracy 0.686\n",
            "Epoch: 713/1000 - Train loss 67.746 - Train accuracy 0.726 - Valid Loss 68.146 - Valid accuracy 0.686\n",
            "Epoch: 714/1000 - Train loss 67.746 - Train accuracy 0.726 - Valid Loss 68.150 - Valid accuracy 0.686\n",
            "Epoch: 715/1000 - Train loss 67.746 - Train accuracy 0.726 - Valid Loss 68.133 - Valid accuracy 0.687\n",
            "Epoch: 716/1000 - Train loss 67.745 - Train accuracy 0.726 - Valid Loss 68.148 - Valid accuracy 0.687\n",
            "Epoch: 717/1000 - Train loss 67.745 - Train accuracy 0.726 - Valid Loss 68.133 - Valid accuracy 0.687\n",
            "Epoch: 718/1000 - Train loss 67.746 - Train accuracy 0.726 - Valid Loss 68.149 - Valid accuracy 0.686\n",
            "Epoch: 719/1000 - Train loss 67.746 - Train accuracy 0.726 - Valid Loss 68.151 - Valid accuracy 0.686\n",
            "Epoch: 720/1000 - Train loss 67.741 - Train accuracy 0.726 - Valid Loss 68.165 - Valid accuracy 0.685\n",
            "Epoch: 721/1000 - Train loss 67.743 - Train accuracy 0.726 - Valid Loss 68.164 - Valid accuracy 0.685\n",
            "Epoch: 722/1000 - Train loss 67.742 - Train accuracy 0.726 - Valid Loss 68.175 - Valid accuracy 0.682\n",
            "Epoch: 723/1000 - Train loss 67.742 - Train accuracy 0.726 - Valid Loss 68.155 - Valid accuracy 0.684\n",
            "Epoch: 724/1000 - Train loss 67.741 - Train accuracy 0.726 - Valid Loss 68.168 - Valid accuracy 0.681\n",
            "Epoch: 725/1000 - Train loss 67.741 - Train accuracy 0.726 - Valid Loss 68.154 - Valid accuracy 0.684\n",
            "Epoch: 726/1000 - Train loss 67.743 - Train accuracy 0.726 - Valid Loss 68.167 - Valid accuracy 0.684\n",
            "Epoch: 727/1000 - Train loss 67.741 - Train accuracy 0.726 - Valid Loss 68.177 - Valid accuracy 0.682\n",
            "Epoch: 728/1000 - Train loss 67.742 - Train accuracy 0.726 - Valid Loss 68.175 - Valid accuracy 0.682\n",
            "Epoch: 729/1000 - Train loss 67.742 - Train accuracy 0.726 - Valid Loss 68.165 - Valid accuracy 0.683\n",
            "Epoch: 730/1000 - Train loss 67.739 - Train accuracy 0.726 - Valid Loss 68.164 - Valid accuracy 0.682\n",
            "Epoch: 731/1000 - Train loss 67.728 - Train accuracy 0.728 - Valid Loss 68.118 - Valid accuracy 0.691\n",
            "Epoch: 732/1000 - Train loss 67.726 - Train accuracy 0.728 - Valid Loss 68.134 - Valid accuracy 0.689\n",
            "Epoch: 733/1000 - Train loss 67.722 - Train accuracy 0.728 - Valid Loss 68.130 - Valid accuracy 0.689\n",
            "Epoch: 734/1000 - Train loss 67.723 - Train accuracy 0.728 - Valid Loss 68.114 - Valid accuracy 0.688\n",
            "Epoch: 735/1000 - Train loss 67.724 - Train accuracy 0.728 - Valid Loss 68.134 - Valid accuracy 0.688\n",
            "Epoch: 736/1000 - Train loss 67.722 - Train accuracy 0.728 - Valid Loss 68.159 - Valid accuracy 0.684\n",
            "Epoch: 737/1000 - Train loss 67.723 - Train accuracy 0.728 - Valid Loss 68.142 - Valid accuracy 0.685\n",
            "Epoch: 738/1000 - Train loss 67.721 - Train accuracy 0.728 - Valid Loss 68.149 - Valid accuracy 0.687\n",
            "Epoch: 739/1000 - Train loss 67.719 - Train accuracy 0.728 - Valid Loss 68.150 - Valid accuracy 0.684\n",
            "Epoch: 740/1000 - Train loss 67.722 - Train accuracy 0.728 - Valid Loss 68.138 - Valid accuracy 0.688\n",
            "Epoch: 741/1000 - Train loss 67.721 - Train accuracy 0.728 - Valid Loss 68.143 - Valid accuracy 0.686\n",
            "Epoch: 742/1000 - Train loss 67.721 - Train accuracy 0.728 - Valid Loss 68.160 - Valid accuracy 0.684\n",
            "Epoch: 743/1000 - Train loss 67.720 - Train accuracy 0.728 - Valid Loss 68.142 - Valid accuracy 0.688\n",
            "Epoch: 744/1000 - Train loss 67.721 - Train accuracy 0.728 - Valid Loss 68.139 - Valid accuracy 0.688\n",
            "Epoch: 745/1000 - Train loss 67.720 - Train accuracy 0.728 - Valid Loss 68.136 - Valid accuracy 0.688\n",
            "Epoch: 746/1000 - Train loss 67.721 - Train accuracy 0.728 - Valid Loss 68.135 - Valid accuracy 0.688\n",
            "Epoch: 747/1000 - Train loss 67.721 - Train accuracy 0.728 - Valid Loss 68.156 - Valid accuracy 0.685\n",
            "Epoch: 748/1000 - Train loss 67.718 - Train accuracy 0.729 - Valid Loss 68.146 - Valid accuracy 0.688\n",
            "Epoch: 749/1000 - Train loss 67.720 - Train accuracy 0.728 - Valid Loss 68.150 - Valid accuracy 0.686\n",
            "Epoch: 750/1000 - Train loss 67.721 - Train accuracy 0.728 - Valid Loss 68.162 - Valid accuracy 0.685\n",
            "Epoch: 751/1000 - Train loss 67.722 - Train accuracy 0.728 - Valid Loss 68.132 - Valid accuracy 0.689\n",
            "Epoch: 752/1000 - Train loss 67.722 - Train accuracy 0.728 - Valid Loss 68.155 - Valid accuracy 0.684\n",
            "Epoch: 753/1000 - Train loss 67.721 - Train accuracy 0.728 - Valid Loss 68.148 - Valid accuracy 0.687\n",
            "Epoch: 754/1000 - Train loss 67.723 - Train accuracy 0.728 - Valid Loss 68.163 - Valid accuracy 0.684\n",
            "Epoch: 755/1000 - Train loss 67.724 - Train accuracy 0.728 - Valid Loss 68.148 - Valid accuracy 0.685\n",
            "Epoch: 756/1000 - Train loss 67.726 - Train accuracy 0.728 - Valid Loss 68.152 - Valid accuracy 0.685\n",
            "Epoch: 757/1000 - Train loss 67.727 - Train accuracy 0.728 - Valid Loss 68.145 - Valid accuracy 0.685\n",
            "Epoch: 758/1000 - Train loss 67.732 - Train accuracy 0.728 - Valid Loss 68.101 - Valid accuracy 0.691\n",
            "Epoch: 759/1000 - Train loss 67.732 - Train accuracy 0.727 - Valid Loss 68.181 - Valid accuracy 0.681\n",
            "Epoch: 760/1000 - Train loss 67.729 - Train accuracy 0.728 - Valid Loss 68.170 - Valid accuracy 0.683\n",
            "Epoch: 761/1000 - Train loss 67.727 - Train accuracy 0.728 - Valid Loss 68.128 - Valid accuracy 0.687\n",
            "Epoch: 762/1000 - Train loss 67.724 - Train accuracy 0.728 - Valid Loss 68.144 - Valid accuracy 0.686\n",
            "Epoch: 763/1000 - Train loss 67.723 - Train accuracy 0.728 - Valid Loss 68.132 - Valid accuracy 0.687\n",
            "Epoch: 764/1000 - Train loss 67.723 - Train accuracy 0.728 - Valid Loss 68.119 - Valid accuracy 0.690\n",
            "Epoch: 765/1000 - Train loss 67.720 - Train accuracy 0.728 - Valid Loss 68.132 - Valid accuracy 0.688\n",
            "Epoch: 766/1000 - Train loss 67.719 - Train accuracy 0.729 - Valid Loss 68.139 - Valid accuracy 0.686\n",
            "Epoch: 767/1000 - Train loss 67.720 - Train accuracy 0.728 - Valid Loss 68.147 - Valid accuracy 0.685\n",
            "Epoch: 768/1000 - Train loss 67.717 - Train accuracy 0.729 - Valid Loss 68.145 - Valid accuracy 0.685\n",
            "Epoch: 769/1000 - Train loss 67.719 - Train accuracy 0.728 - Valid Loss 68.143 - Valid accuracy 0.687\n",
            "Epoch: 770/1000 - Train loss 67.717 - Train accuracy 0.729 - Valid Loss 68.141 - Valid accuracy 0.687\n",
            "Epoch: 771/1000 - Train loss 67.718 - Train accuracy 0.729 - Valid Loss 68.147 - Valid accuracy 0.686\n",
            "Epoch: 772/1000 - Train loss 67.717 - Train accuracy 0.729 - Valid Loss 68.152 - Valid accuracy 0.684\n",
            "Epoch: 773/1000 - Train loss 67.716 - Train accuracy 0.729 - Valid Loss 68.164 - Valid accuracy 0.684\n",
            "Epoch: 774/1000 - Train loss 67.714 - Train accuracy 0.729 - Valid Loss 68.145 - Valid accuracy 0.687\n",
            "Epoch: 775/1000 - Train loss 67.716 - Train accuracy 0.729 - Valid Loss 68.155 - Valid accuracy 0.685\n",
            "Epoch: 776/1000 - Train loss 67.715 - Train accuracy 0.729 - Valid Loss 68.156 - Valid accuracy 0.685\n",
            "Epoch: 777/1000 - Train loss 67.715 - Train accuracy 0.729 - Valid Loss 68.153 - Valid accuracy 0.685\n",
            "Epoch: 778/1000 - Train loss 67.715 - Train accuracy 0.729 - Valid Loss 68.141 - Valid accuracy 0.687\n",
            "Epoch: 779/1000 - Train loss 67.715 - Train accuracy 0.729 - Valid Loss 68.154 - Valid accuracy 0.686\n",
            "Epoch: 780/1000 - Train loss 67.716 - Train accuracy 0.729 - Valid Loss 68.154 - Valid accuracy 0.686\n",
            "Epoch: 781/1000 - Train loss 67.716 - Train accuracy 0.729 - Valid Loss 68.153 - Valid accuracy 0.686\n",
            "Epoch: 782/1000 - Train loss 67.714 - Train accuracy 0.729 - Valid Loss 68.142 - Valid accuracy 0.687\n",
            "Epoch: 783/1000 - Train loss 67.715 - Train accuracy 0.729 - Valid Loss 68.167 - Valid accuracy 0.684\n",
            "Epoch: 784/1000 - Train loss 67.715 - Train accuracy 0.729 - Valid Loss 68.149 - Valid accuracy 0.686\n",
            "Epoch: 785/1000 - Train loss 67.716 - Train accuracy 0.729 - Valid Loss 68.137 - Valid accuracy 0.686\n",
            "Epoch: 786/1000 - Train loss 67.715 - Train accuracy 0.729 - Valid Loss 68.155 - Valid accuracy 0.684\n",
            "Epoch: 787/1000 - Train loss 67.716 - Train accuracy 0.729 - Valid Loss 68.123 - Valid accuracy 0.688\n",
            "Epoch: 788/1000 - Train loss 67.716 - Train accuracy 0.729 - Valid Loss 68.132 - Valid accuracy 0.687\n",
            "Epoch: 789/1000 - Train loss 67.717 - Train accuracy 0.729 - Valid Loss 68.146 - Valid accuracy 0.686\n",
            "Epoch: 790/1000 - Train loss 67.714 - Train accuracy 0.729 - Valid Loss 68.135 - Valid accuracy 0.688\n",
            "Epoch: 791/1000 - Train loss 67.714 - Train accuracy 0.729 - Valid Loss 68.147 - Valid accuracy 0.686\n",
            "Epoch: 792/1000 - Train loss 67.716 - Train accuracy 0.729 - Valid Loss 68.143 - Valid accuracy 0.686\n",
            "Epoch: 793/1000 - Train loss 67.714 - Train accuracy 0.729 - Valid Loss 68.148 - Valid accuracy 0.687\n",
            "Epoch: 794/1000 - Train loss 67.715 - Train accuracy 0.729 - Valid Loss 68.142 - Valid accuracy 0.686\n",
            "Epoch: 795/1000 - Train loss 67.715 - Train accuracy 0.729 - Valid Loss 68.140 - Valid accuracy 0.687\n",
            "Epoch: 796/1000 - Train loss 67.715 - Train accuracy 0.729 - Valid Loss 68.140 - Valid accuracy 0.687\n",
            "Epoch: 797/1000 - Train loss 67.714 - Train accuracy 0.729 - Valid Loss 68.164 - Valid accuracy 0.683\n",
            "Epoch: 798/1000 - Train loss 67.713 - Train accuracy 0.729 - Valid Loss 68.138 - Valid accuracy 0.688\n",
            "Epoch: 799/1000 - Train loss 67.716 - Train accuracy 0.729 - Valid Loss 68.162 - Valid accuracy 0.685\n",
            "Epoch: 800/1000 - Train loss 67.715 - Train accuracy 0.729 - Valid Loss 68.135 - Valid accuracy 0.688\n",
            "Epoch: 801/1000 - Train loss 67.714 - Train accuracy 0.729 - Valid Loss 68.130 - Valid accuracy 0.688\n",
            "Epoch: 802/1000 - Train loss 67.715 - Train accuracy 0.729 - Valid Loss 68.160 - Valid accuracy 0.684\n",
            "Epoch: 803/1000 - Train loss 67.715 - Train accuracy 0.729 - Valid Loss 68.191 - Valid accuracy 0.680\n",
            "Epoch: 804/1000 - Train loss 67.714 - Train accuracy 0.729 - Valid Loss 68.196 - Valid accuracy 0.681\n",
            "Epoch: 805/1000 - Train loss 67.714 - Train accuracy 0.729 - Valid Loss 68.207 - Valid accuracy 0.679\n",
            "Epoch: 806/1000 - Train loss 67.717 - Train accuracy 0.729 - Valid Loss 68.201 - Valid accuracy 0.680\n",
            "Epoch: 807/1000 - Train loss 67.718 - Train accuracy 0.729 - Valid Loss 68.166 - Valid accuracy 0.684\n",
            "Epoch: 808/1000 - Train loss 67.718 - Train accuracy 0.729 - Valid Loss 68.197 - Valid accuracy 0.680\n",
            "Epoch: 809/1000 - Train loss 67.718 - Train accuracy 0.729 - Valid Loss 68.173 - Valid accuracy 0.684\n",
            "Epoch: 810/1000 - Train loss 67.720 - Train accuracy 0.728 - Valid Loss 68.191 - Valid accuracy 0.682\n",
            "Epoch: 811/1000 - Train loss 67.718 - Train accuracy 0.729 - Valid Loss 68.122 - Valid accuracy 0.690\n",
            "Epoch: 812/1000 - Train loss 67.717 - Train accuracy 0.729 - Valid Loss 68.127 - Valid accuracy 0.689\n",
            "Epoch: 813/1000 - Train loss 67.720 - Train accuracy 0.728 - Valid Loss 68.137 - Valid accuracy 0.688\n",
            "Epoch: 814/1000 - Train loss 67.718 - Train accuracy 0.729 - Valid Loss 68.154 - Valid accuracy 0.684\n",
            "Epoch: 815/1000 - Train loss 67.717 - Train accuracy 0.729 - Valid Loss 68.156 - Valid accuracy 0.685\n",
            "Epoch: 816/1000 - Train loss 67.718 - Train accuracy 0.729 - Valid Loss 68.140 - Valid accuracy 0.687\n",
            "Epoch: 817/1000 - Train loss 67.717 - Train accuracy 0.729 - Valid Loss 68.168 - Valid accuracy 0.684\n",
            "Epoch: 818/1000 - Train loss 67.714 - Train accuracy 0.729 - Valid Loss 68.157 - Valid accuracy 0.686\n",
            "Epoch: 819/1000 - Train loss 67.714 - Train accuracy 0.729 - Valid Loss 68.159 - Valid accuracy 0.684\n",
            "Epoch: 820/1000 - Train loss 67.714 - Train accuracy 0.729 - Valid Loss 68.164 - Valid accuracy 0.684\n",
            "Epoch: 821/1000 - Train loss 67.713 - Train accuracy 0.729 - Valid Loss 68.159 - Valid accuracy 0.685\n",
            "Epoch: 822/1000 - Train loss 67.714 - Train accuracy 0.729 - Valid Loss 68.161 - Valid accuracy 0.684\n",
            "Epoch: 823/1000 - Train loss 67.710 - Train accuracy 0.729 - Valid Loss 68.160 - Valid accuracy 0.684\n",
            "Epoch: 824/1000 - Train loss 67.714 - Train accuracy 0.729 - Valid Loss 68.160 - Valid accuracy 0.684\n",
            "Epoch: 825/1000 - Train loss 67.712 - Train accuracy 0.729 - Valid Loss 68.164 - Valid accuracy 0.684\n",
            "Epoch: 826/1000 - Train loss 67.711 - Train accuracy 0.729 - Valid Loss 68.147 - Valid accuracy 0.686\n",
            "Epoch: 827/1000 - Train loss 67.712 - Train accuracy 0.729 - Valid Loss 68.164 - Valid accuracy 0.684\n",
            "Epoch: 828/1000 - Train loss 67.710 - Train accuracy 0.729 - Valid Loss 68.174 - Valid accuracy 0.683\n",
            "Epoch: 829/1000 - Train loss 67.712 - Train accuracy 0.729 - Valid Loss 68.155 - Valid accuracy 0.685\n",
            "Epoch: 830/1000 - Train loss 67.713 - Train accuracy 0.729 - Valid Loss 68.159 - Valid accuracy 0.684\n",
            "Epoch: 831/1000 - Train loss 67.712 - Train accuracy 0.729 - Valid Loss 68.158 - Valid accuracy 0.685\n",
            "Epoch: 832/1000 - Train loss 67.712 - Train accuracy 0.729 - Valid Loss 68.161 - Valid accuracy 0.685\n",
            "Epoch: 833/1000 - Train loss 67.710 - Train accuracy 0.729 - Valid Loss 68.162 - Valid accuracy 0.684\n",
            "Epoch: 834/1000 - Train loss 67.709 - Train accuracy 0.729 - Valid Loss 68.155 - Valid accuracy 0.685\n",
            "Epoch: 835/1000 - Train loss 67.707 - Train accuracy 0.730 - Valid Loss 68.159 - Valid accuracy 0.684\n",
            "Epoch: 836/1000 - Train loss 67.707 - Train accuracy 0.730 - Valid Loss 68.155 - Valid accuracy 0.685\n",
            "Epoch: 837/1000 - Train loss 67.707 - Train accuracy 0.730 - Valid Loss 68.149 - Valid accuracy 0.686\n",
            "Epoch: 838/1000 - Train loss 67.706 - Train accuracy 0.730 - Valid Loss 68.151 - Valid accuracy 0.686\n",
            "Epoch: 839/1000 - Train loss 67.708 - Train accuracy 0.730 - Valid Loss 68.153 - Valid accuracy 0.686\n",
            "Epoch: 840/1000 - Train loss 67.706 - Train accuracy 0.730 - Valid Loss 68.172 - Valid accuracy 0.684\n",
            "Epoch: 841/1000 - Train loss 67.706 - Train accuracy 0.730 - Valid Loss 68.176 - Valid accuracy 0.683\n",
            "Epoch: 842/1000 - Train loss 67.707 - Train accuracy 0.730 - Valid Loss 68.176 - Valid accuracy 0.683\n",
            "Epoch: 843/1000 - Train loss 67.703 - Train accuracy 0.730 - Valid Loss 68.177 - Valid accuracy 0.683\n",
            "Epoch: 844/1000 - Train loss 67.706 - Train accuracy 0.730 - Valid Loss 68.177 - Valid accuracy 0.683\n",
            "Epoch: 845/1000 - Train loss 67.706 - Train accuracy 0.730 - Valid Loss 68.175 - Valid accuracy 0.684\n",
            "Epoch: 846/1000 - Train loss 67.707 - Train accuracy 0.730 - Valid Loss 68.175 - Valid accuracy 0.684\n",
            "Epoch: 847/1000 - Train loss 67.708 - Train accuracy 0.730 - Valid Loss 68.176 - Valid accuracy 0.684\n",
            "Epoch: 848/1000 - Train loss 67.706 - Train accuracy 0.730 - Valid Loss 68.176 - Valid accuracy 0.683\n",
            "Epoch: 849/1000 - Train loss 67.708 - Train accuracy 0.730 - Valid Loss 68.179 - Valid accuracy 0.683\n",
            "Epoch: 850/1000 - Train loss 67.704 - Train accuracy 0.730 - Valid Loss 68.182 - Valid accuracy 0.683\n",
            "Epoch: 851/1000 - Train loss 67.706 - Train accuracy 0.730 - Valid Loss 68.176 - Valid accuracy 0.684\n",
            "Epoch: 852/1000 - Train loss 67.705 - Train accuracy 0.730 - Valid Loss 68.177 - Valid accuracy 0.682\n",
            "Epoch: 853/1000 - Train loss 67.706 - Train accuracy 0.730 - Valid Loss 68.183 - Valid accuracy 0.682\n",
            "Epoch: 854/1000 - Train loss 67.708 - Train accuracy 0.730 - Valid Loss 68.177 - Valid accuracy 0.683\n",
            "Epoch: 855/1000 - Train loss 67.707 - Train accuracy 0.730 - Valid Loss 68.179 - Valid accuracy 0.682\n",
            "Epoch: 856/1000 - Train loss 67.705 - Train accuracy 0.730 - Valid Loss 68.187 - Valid accuracy 0.681\n",
            "Epoch: 857/1000 - Train loss 67.703 - Train accuracy 0.730 - Valid Loss 68.186 - Valid accuracy 0.681\n",
            "Epoch: 858/1000 - Train loss 67.704 - Train accuracy 0.730 - Valid Loss 68.176 - Valid accuracy 0.681\n",
            "Epoch: 859/1000 - Train loss 67.705 - Train accuracy 0.730 - Valid Loss 68.168 - Valid accuracy 0.683\n",
            "Epoch: 860/1000 - Train loss 67.703 - Train accuracy 0.730 - Valid Loss 68.153 - Valid accuracy 0.685\n",
            "Epoch: 861/1000 - Train loss 67.704 - Train accuracy 0.730 - Valid Loss 68.151 - Valid accuracy 0.684\n",
            "Epoch: 862/1000 - Train loss 67.695 - Train accuracy 0.731 - Valid Loss 68.153 - Valid accuracy 0.684\n",
            "Epoch: 863/1000 - Train loss 67.698 - Train accuracy 0.731 - Valid Loss 68.153 - Valid accuracy 0.684\n",
            "Epoch: 864/1000 - Train loss 67.700 - Train accuracy 0.731 - Valid Loss 68.134 - Valid accuracy 0.687\n",
            "Epoch: 865/1000 - Train loss 67.701 - Train accuracy 0.730 - Valid Loss 68.145 - Valid accuracy 0.686\n",
            "Epoch: 866/1000 - Train loss 67.704 - Train accuracy 0.730 - Valid Loss 68.144 - Valid accuracy 0.686\n",
            "Epoch: 867/1000 - Train loss 67.704 - Train accuracy 0.730 - Valid Loss 68.163 - Valid accuracy 0.685\n",
            "Epoch: 868/1000 - Train loss 67.702 - Train accuracy 0.730 - Valid Loss 68.115 - Valid accuracy 0.688\n",
            "Epoch: 869/1000 - Train loss 67.701 - Train accuracy 0.730 - Valid Loss 68.106 - Valid accuracy 0.690\n",
            "Epoch: 870/1000 - Train loss 67.698 - Train accuracy 0.731 - Valid Loss 68.160 - Valid accuracy 0.684\n",
            "Epoch: 871/1000 - Train loss 67.699 - Train accuracy 0.731 - Valid Loss 68.129 - Valid accuracy 0.688\n",
            "Epoch: 872/1000 - Train loss 67.698 - Train accuracy 0.731 - Valid Loss 68.125 - Valid accuracy 0.688\n",
            "Epoch: 873/1000 - Train loss 67.696 - Train accuracy 0.731 - Valid Loss 68.127 - Valid accuracy 0.688\n",
            "Epoch: 874/1000 - Train loss 67.694 - Train accuracy 0.731 - Valid Loss 68.125 - Valid accuracy 0.688\n",
            "Epoch: 875/1000 - Train loss 67.692 - Train accuracy 0.731 - Valid Loss 68.132 - Valid accuracy 0.687\n",
            "Epoch: 876/1000 - Train loss 67.692 - Train accuracy 0.731 - Valid Loss 68.121 - Valid accuracy 0.689\n",
            "Epoch: 877/1000 - Train loss 67.691 - Train accuracy 0.731 - Valid Loss 68.124 - Valid accuracy 0.688\n",
            "Epoch: 878/1000 - Train loss 67.695 - Train accuracy 0.731 - Valid Loss 68.123 - Valid accuracy 0.689\n",
            "Epoch: 879/1000 - Train loss 67.694 - Train accuracy 0.731 - Valid Loss 68.123 - Valid accuracy 0.689\n",
            "Epoch: 880/1000 - Train loss 67.693 - Train accuracy 0.731 - Valid Loss 68.124 - Valid accuracy 0.689\n",
            "Epoch: 881/1000 - Train loss 67.693 - Train accuracy 0.731 - Valid Loss 68.119 - Valid accuracy 0.689\n",
            "Epoch: 882/1000 - Train loss 67.693 - Train accuracy 0.731 - Valid Loss 68.112 - Valid accuracy 0.690\n",
            "Epoch: 883/1000 - Train loss 67.693 - Train accuracy 0.731 - Valid Loss 68.115 - Valid accuracy 0.690\n",
            "Epoch: 884/1000 - Train loss 67.690 - Train accuracy 0.731 - Valid Loss 68.118 - Valid accuracy 0.689\n",
            "Epoch: 885/1000 - Train loss 67.690 - Train accuracy 0.731 - Valid Loss 68.116 - Valid accuracy 0.689\n",
            "Epoch: 886/1000 - Train loss 67.688 - Train accuracy 0.732 - Valid Loss 68.110 - Valid accuracy 0.690\n",
            "Epoch: 887/1000 - Train loss 67.693 - Train accuracy 0.731 - Valid Loss 68.109 - Valid accuracy 0.690\n",
            "Epoch: 888/1000 - Train loss 67.690 - Train accuracy 0.731 - Valid Loss 68.112 - Valid accuracy 0.690\n",
            "Epoch: 889/1000 - Train loss 67.692 - Train accuracy 0.731 - Valid Loss 68.111 - Valid accuracy 0.690\n",
            "Epoch: 890/1000 - Train loss 67.690 - Train accuracy 0.731 - Valid Loss 68.108 - Valid accuracy 0.690\n",
            "Epoch: 891/1000 - Train loss 67.691 - Train accuracy 0.731 - Valid Loss 68.104 - Valid accuracy 0.690\n",
            "Epoch: 892/1000 - Train loss 67.692 - Train accuracy 0.731 - Valid Loss 68.103 - Valid accuracy 0.691\n",
            "Epoch: 893/1000 - Train loss 67.690 - Train accuracy 0.731 - Valid Loss 68.103 - Valid accuracy 0.691\n",
            "Epoch: 894/1000 - Train loss 67.690 - Train accuracy 0.731 - Valid Loss 68.103 - Valid accuracy 0.691\n",
            "Epoch: 895/1000 - Train loss 67.688 - Train accuracy 0.732 - Valid Loss 68.103 - Valid accuracy 0.691\n",
            "Epoch: 896/1000 - Train loss 67.689 - Train accuracy 0.731 - Valid Loss 68.103 - Valid accuracy 0.691\n",
            "Epoch: 897/1000 - Train loss 67.690 - Train accuracy 0.731 - Valid Loss 68.104 - Valid accuracy 0.691\n",
            "Epoch: 898/1000 - Train loss 67.688 - Train accuracy 0.732 - Valid Loss 68.104 - Valid accuracy 0.691\n",
            "Epoch: 899/1000 - Train loss 67.688 - Train accuracy 0.732 - Valid Loss 68.104 - Valid accuracy 0.691\n",
            "Epoch: 900/1000 - Train loss 67.689 - Train accuracy 0.732 - Valid Loss 68.103 - Valid accuracy 0.691\n",
            "Epoch: 901/1000 - Train loss 67.690 - Train accuracy 0.731 - Valid Loss 68.105 - Valid accuracy 0.691\n",
            "Epoch: 902/1000 - Train loss 67.690 - Train accuracy 0.731 - Valid Loss 68.103 - Valid accuracy 0.691\n",
            "Epoch: 903/1000 - Train loss 67.690 - Train accuracy 0.731 - Valid Loss 68.105 - Valid accuracy 0.691\n",
            "Epoch: 904/1000 - Train loss 67.689 - Train accuracy 0.731 - Valid Loss 68.096 - Valid accuracy 0.691\n",
            "Epoch: 905/1000 - Train loss 67.691 - Train accuracy 0.731 - Valid Loss 68.095 - Valid accuracy 0.691\n",
            "Epoch: 906/1000 - Train loss 67.691 - Train accuracy 0.731 - Valid Loss 68.099 - Valid accuracy 0.691\n",
            "Epoch: 907/1000 - Train loss 67.689 - Train accuracy 0.732 - Valid Loss 68.090 - Valid accuracy 0.692\n",
            "Epoch: 908/1000 - Train loss 67.692 - Train accuracy 0.731 - Valid Loss 68.103 - Valid accuracy 0.691\n",
            "Epoch: 909/1000 - Train loss 67.691 - Train accuracy 0.731 - Valid Loss 68.112 - Valid accuracy 0.689\n",
            "Epoch: 910/1000 - Train loss 67.691 - Train accuracy 0.731 - Valid Loss 68.115 - Valid accuracy 0.689\n",
            "Epoch: 911/1000 - Train loss 67.690 - Train accuracy 0.731 - Valid Loss 68.128 - Valid accuracy 0.688\n",
            "Epoch: 912/1000 - Train loss 67.691 - Train accuracy 0.731 - Valid Loss 68.131 - Valid accuracy 0.688\n",
            "Epoch: 913/1000 - Train loss 67.690 - Train accuracy 0.731 - Valid Loss 68.116 - Valid accuracy 0.690\n",
            "Epoch: 914/1000 - Train loss 67.690 - Train accuracy 0.731 - Valid Loss 68.104 - Valid accuracy 0.689\n",
            "Epoch: 915/1000 - Train loss 67.691 - Train accuracy 0.731 - Valid Loss 68.093 - Valid accuracy 0.691\n",
            "Epoch: 916/1000 - Train loss 67.689 - Train accuracy 0.731 - Valid Loss 68.101 - Valid accuracy 0.690\n",
            "Epoch: 917/1000 - Train loss 67.691 - Train accuracy 0.731 - Valid Loss 68.132 - Valid accuracy 0.687\n",
            "Epoch: 918/1000 - Train loss 67.690 - Train accuracy 0.731 - Valid Loss 68.105 - Valid accuracy 0.689\n",
            "Epoch: 919/1000 - Train loss 67.689 - Train accuracy 0.732 - Valid Loss 68.110 - Valid accuracy 0.689\n",
            "Epoch: 920/1000 - Train loss 67.691 - Train accuracy 0.731 - Valid Loss 68.086 - Valid accuracy 0.691\n",
            "Epoch: 921/1000 - Train loss 67.691 - Train accuracy 0.731 - Valid Loss 68.097 - Valid accuracy 0.691\n",
            "Epoch: 922/1000 - Train loss 67.691 - Train accuracy 0.731 - Valid Loss 68.113 - Valid accuracy 0.689\n",
            "Epoch: 923/1000 - Train loss 67.694 - Train accuracy 0.731 - Valid Loss 68.099 - Valid accuracy 0.691\n",
            "Epoch: 924/1000 - Train loss 67.692 - Train accuracy 0.731 - Valid Loss 68.094 - Valid accuracy 0.691\n",
            "Epoch: 925/1000 - Train loss 67.693 - Train accuracy 0.731 - Valid Loss 68.123 - Valid accuracy 0.687\n",
            "Epoch: 926/1000 - Train loss 67.693 - Train accuracy 0.731 - Valid Loss 68.125 - Valid accuracy 0.688\n",
            "Epoch: 927/1000 - Train loss 67.693 - Train accuracy 0.731 - Valid Loss 68.116 - Valid accuracy 0.690\n",
            "Epoch: 928/1000 - Train loss 67.692 - Train accuracy 0.731 - Valid Loss 68.109 - Valid accuracy 0.690\n",
            "Epoch: 929/1000 - Train loss 67.694 - Train accuracy 0.731 - Valid Loss 68.122 - Valid accuracy 0.688\n",
            "Epoch: 930/1000 - Train loss 67.691 - Train accuracy 0.731 - Valid Loss 68.123 - Valid accuracy 0.689\n",
            "Epoch: 931/1000 - Train loss 67.691 - Train accuracy 0.731 - Valid Loss 68.108 - Valid accuracy 0.690\n",
            "Epoch: 932/1000 - Train loss 67.690 - Train accuracy 0.731 - Valid Loss 68.098 - Valid accuracy 0.691\n",
            "Epoch: 933/1000 - Train loss 67.690 - Train accuracy 0.731 - Valid Loss 68.124 - Valid accuracy 0.688\n",
            "Epoch: 934/1000 - Train loss 67.688 - Train accuracy 0.732 - Valid Loss 68.115 - Valid accuracy 0.690\n",
            "Epoch: 935/1000 - Train loss 67.687 - Train accuracy 0.732 - Valid Loss 68.130 - Valid accuracy 0.687\n",
            "Epoch: 936/1000 - Train loss 67.689 - Train accuracy 0.731 - Valid Loss 68.124 - Valid accuracy 0.688\n",
            "Epoch: 937/1000 - Train loss 67.688 - Train accuracy 0.732 - Valid Loss 68.136 - Valid accuracy 0.687\n",
            "Epoch: 938/1000 - Train loss 67.686 - Train accuracy 0.732 - Valid Loss 68.131 - Valid accuracy 0.688\n",
            "Epoch: 939/1000 - Train loss 67.685 - Train accuracy 0.732 - Valid Loss 68.131 - Valid accuracy 0.687\n",
            "Epoch: 940/1000 - Train loss 67.685 - Train accuracy 0.732 - Valid Loss 68.136 - Valid accuracy 0.686\n",
            "Epoch: 941/1000 - Train loss 67.684 - Train accuracy 0.732 - Valid Loss 68.129 - Valid accuracy 0.687\n",
            "Epoch: 942/1000 - Train loss 67.686 - Train accuracy 0.732 - Valid Loss 68.131 - Valid accuracy 0.687\n",
            "Epoch: 943/1000 - Train loss 67.686 - Train accuracy 0.732 - Valid Loss 68.132 - Valid accuracy 0.687\n",
            "Epoch: 944/1000 - Train loss 67.685 - Train accuracy 0.732 - Valid Loss 68.122 - Valid accuracy 0.688\n",
            "Epoch: 945/1000 - Train loss 67.683 - Train accuracy 0.732 - Valid Loss 68.119 - Valid accuracy 0.688\n",
            "Epoch: 946/1000 - Train loss 67.686 - Train accuracy 0.732 - Valid Loss 68.113 - Valid accuracy 0.690\n",
            "Epoch: 947/1000 - Train loss 67.684 - Train accuracy 0.732 - Valid Loss 68.118 - Valid accuracy 0.688\n",
            "Epoch: 948/1000 - Train loss 67.684 - Train accuracy 0.732 - Valid Loss 68.118 - Valid accuracy 0.690\n",
            "Epoch: 949/1000 - Train loss 67.685 - Train accuracy 0.732 - Valid Loss 68.121 - Valid accuracy 0.688\n",
            "Epoch: 950/1000 - Train loss 67.685 - Train accuracy 0.732 - Valid Loss 68.120 - Valid accuracy 0.689\n",
            "Epoch: 951/1000 - Train loss 67.684 - Train accuracy 0.732 - Valid Loss 68.120 - Valid accuracy 0.688\n",
            "Epoch: 952/1000 - Train loss 67.685 - Train accuracy 0.732 - Valid Loss 68.125 - Valid accuracy 0.688\n",
            "Epoch: 953/1000 - Train loss 67.685 - Train accuracy 0.732 - Valid Loss 68.126 - Valid accuracy 0.688\n",
            "Epoch: 954/1000 - Train loss 67.684 - Train accuracy 0.732 - Valid Loss 68.124 - Valid accuracy 0.688\n",
            "Epoch: 955/1000 - Train loss 67.684 - Train accuracy 0.732 - Valid Loss 68.127 - Valid accuracy 0.688\n",
            "Epoch: 956/1000 - Train loss 67.684 - Train accuracy 0.732 - Valid Loss 68.121 - Valid accuracy 0.688\n",
            "Epoch: 957/1000 - Train loss 67.682 - Train accuracy 0.732 - Valid Loss 68.129 - Valid accuracy 0.688\n",
            "Epoch: 958/1000 - Train loss 67.683 - Train accuracy 0.732 - Valid Loss 68.127 - Valid accuracy 0.688\n",
            "Epoch: 959/1000 - Train loss 67.684 - Train accuracy 0.732 - Valid Loss 68.126 - Valid accuracy 0.688\n",
            "Epoch: 960/1000 - Train loss 67.682 - Train accuracy 0.732 - Valid Loss 68.126 - Valid accuracy 0.688\n",
            "Epoch: 961/1000 - Train loss 67.684 - Train accuracy 0.732 - Valid Loss 68.118 - Valid accuracy 0.688\n",
            "Epoch: 962/1000 - Train loss 67.684 - Train accuracy 0.732 - Valid Loss 68.123 - Valid accuracy 0.688\n",
            "Epoch: 963/1000 - Train loss 67.684 - Train accuracy 0.732 - Valid Loss 68.120 - Valid accuracy 0.688\n",
            "Epoch: 964/1000 - Train loss 67.683 - Train accuracy 0.732 - Valid Loss 68.126 - Valid accuracy 0.688\n",
            "Epoch: 965/1000 - Train loss 67.684 - Train accuracy 0.732 - Valid Loss 68.118 - Valid accuracy 0.689\n",
            "Epoch: 966/1000 - Train loss 67.683 - Train accuracy 0.732 - Valid Loss 68.125 - Valid accuracy 0.688\n",
            "Epoch: 967/1000 - Train loss 67.683 - Train accuracy 0.732 - Valid Loss 68.121 - Valid accuracy 0.688\n",
            "Epoch: 968/1000 - Train loss 67.682 - Train accuracy 0.732 - Valid Loss 68.117 - Valid accuracy 0.688\n",
            "Epoch: 969/1000 - Train loss 67.681 - Train accuracy 0.732 - Valid Loss 68.122 - Valid accuracy 0.688\n",
            "Epoch: 970/1000 - Train loss 67.684 - Train accuracy 0.732 - Valid Loss 68.131 - Valid accuracy 0.688\n",
            "Epoch: 971/1000 - Train loss 67.684 - Train accuracy 0.732 - Valid Loss 68.130 - Valid accuracy 0.687\n",
            "Epoch: 972/1000 - Train loss 67.681 - Train accuracy 0.732 - Valid Loss 68.130 - Valid accuracy 0.688\n",
            "Epoch: 973/1000 - Train loss 67.684 - Train accuracy 0.732 - Valid Loss 68.126 - Valid accuracy 0.688\n",
            "Epoch: 974/1000 - Train loss 67.683 - Train accuracy 0.732 - Valid Loss 68.132 - Valid accuracy 0.687\n",
            "Epoch: 975/1000 - Train loss 67.683 - Train accuracy 0.732 - Valid Loss 68.138 - Valid accuracy 0.687\n",
            "Epoch: 976/1000 - Train loss 67.681 - Train accuracy 0.732 - Valid Loss 68.127 - Valid accuracy 0.688\n",
            "Epoch: 977/1000 - Train loss 67.682 - Train accuracy 0.732 - Valid Loss 68.122 - Valid accuracy 0.688\n",
            "Epoch: 978/1000 - Train loss 67.683 - Train accuracy 0.732 - Valid Loss 68.104 - Valid accuracy 0.691\n",
            "Epoch: 979/1000 - Train loss 67.680 - Train accuracy 0.732 - Valid Loss 68.118 - Valid accuracy 0.688\n",
            "Epoch: 980/1000 - Train loss 67.682 - Train accuracy 0.732 - Valid Loss 68.111 - Valid accuracy 0.690\n",
            "Epoch: 981/1000 - Train loss 67.684 - Train accuracy 0.732 - Valid Loss 68.122 - Valid accuracy 0.687\n",
            "Epoch: 982/1000 - Train loss 67.683 - Train accuracy 0.732 - Valid Loss 68.126 - Valid accuracy 0.688\n",
            "Epoch: 983/1000 - Train loss 67.682 - Train accuracy 0.732 - Valid Loss 68.131 - Valid accuracy 0.688\n",
            "Epoch: 984/1000 - Train loss 67.681 - Train accuracy 0.732 - Valid Loss 68.127 - Valid accuracy 0.688\n",
            "Epoch: 985/1000 - Train loss 67.683 - Train accuracy 0.732 - Valid Loss 68.119 - Valid accuracy 0.690\n",
            "Epoch: 986/1000 - Train loss 67.683 - Train accuracy 0.732 - Valid Loss 68.126 - Valid accuracy 0.688\n",
            "Epoch: 987/1000 - Train loss 67.682 - Train accuracy 0.732 - Valid Loss 68.131 - Valid accuracy 0.688\n",
            "Epoch: 988/1000 - Train loss 67.682 - Train accuracy 0.732 - Valid Loss 68.132 - Valid accuracy 0.688\n",
            "Epoch: 989/1000 - Train loss 67.680 - Train accuracy 0.732 - Valid Loss 68.134 - Valid accuracy 0.688\n",
            "Epoch: 990/1000 - Train loss 67.684 - Train accuracy 0.732 - Valid Loss 68.130 - Valid accuracy 0.688\n",
            "Epoch: 991/1000 - Train loss 67.684 - Train accuracy 0.732 - Valid Loss 68.123 - Valid accuracy 0.689\n",
            "Epoch: 992/1000 - Train loss 67.684 - Train accuracy 0.732 - Valid Loss 68.133 - Valid accuracy 0.687\n",
            "Epoch: 993/1000 - Train loss 67.687 - Train accuracy 0.732 - Valid Loss 68.111 - Valid accuracy 0.689\n",
            "Epoch: 994/1000 - Train loss 67.689 - Train accuracy 0.732 - Valid Loss 68.106 - Valid accuracy 0.690\n",
            "Epoch: 995/1000 - Train loss 67.692 - Train accuracy 0.731 - Valid Loss 68.102 - Valid accuracy 0.691\n",
            "Epoch: 996/1000 - Train loss 67.693 - Train accuracy 0.731 - Valid Loss 68.108 - Valid accuracy 0.690\n",
            "Epoch: 997/1000 - Train loss 67.692 - Train accuracy 0.731 - Valid Loss 68.131 - Valid accuracy 0.688\n",
            "Epoch: 998/1000 - Train loss 67.691 - Train accuracy 0.731 - Valid Loss 68.116 - Valid accuracy 0.690\n",
            "Epoch: 999/1000 - Train loss 67.694 - Train accuracy 0.731 - Valid Loss 68.137 - Valid accuracy 0.687\n",
            "Epoch: 1000/1000 - Train loss 67.693 - Train accuracy 0.731 - Valid Loss 68.115 - Valid accuracy 0.690\n"
          ]
        }
      ],
      "source": [
        "history1 = train(model,\n",
        "                train_loader,\n",
        "                valid_loader,\n",
        "                optimizer,\n",
        "                criterion,\n",
        "                epochs=1000\n",
        "                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "1r0u-4VI2agu",
        "outputId": "ebf57456-c8b3-47bc-b5a1-b6f1ae61b6f8"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "epoch_count = range(1, len(history1['accuracy']) + 1)\n",
        "sns.lineplot(x=epoch_count,  y=history1['accuracy'], label='train')\n",
        "sns.lineplot(x=epoch_count,  y=history1['val_accuracy'], label='valid')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LS4auqBHVDr"
      },
      "source": [
        "### Guardado de resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Pesos del modelo guardados en model_weights.pth\n"
          ]
        }
      ],
      "source": [
        "# Definimos una ruta para guardar los pesos\n",
        "filepath = 'model_weights.pth'\n",
        "\n",
        "# Guardamos el diccionario de estados\n",
        "torch.save(model.state_dict(), filepath)\n",
        "\n",
        "print(f\"✅ Pesos del modelo guardados en {filepath}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Historial de entrenamiento guardado en training_history.csv\n"
          ]
        }
      ],
      "source": [
        "# Definimos la ruta para guardar el historial en un archivo CSV\n",
        "history_filepath = 'training_history.csv'\n",
        "\n",
        "# Creamos un DataFrame de pandas\n",
        "df_history = pd.DataFrame(history1)\n",
        "\n",
        "# Guardamos el DataFrame en un archivo CSV\n",
        "df_history.to_csv(history_filepath, index=False)\n",
        "\n",
        "print(f\"✅ Historial de entrenamiento guardado en {history_filepath}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Inferencia\n",
        "Se experimenta el funcionamiento del modelo. Se realiza la inferencia de los modelos por separado de encoder y decoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Pesos del modelo cargados desde model_weights.pth\n"
          ]
        }
      ],
      "source": [
        "# Cargamos los pesos del modelo guardados\n",
        "model_weights_path = 'model_weights.pth'\n",
        "model.load_state_dict(torch.load(model_weights_path))\n",
        "\n",
        "print(f\"✅ Pesos del modelo cargados desde {model_weights_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "PnS51WFF2agu"
      },
      "outputs": [],
      "source": [
        "# Armar lo conversores de indice a palabra:\n",
        "idx2word_input = {v:k for k, v in word2idx_inputs.items()}\n",
        "idx2word_target = {v:k for k, v in word2idx_outputs.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "OUEdMsga2agv"
      },
      "outputs": [],
      "source": [
        "def translate_sentence(input_seq):\n",
        "    # Se transforma la sequencia de entrada a los stados \"h\" y \"c\" de la LSTM\n",
        "    # para enviar la primera vez al decoder\"\n",
        "    prev_state = model.encoder(encoder_sequence_test_tensor.to(device))\n",
        "\n",
        "    # Se inicializa la secuencia de entrada al decoder como \"<sos>\"\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = word2idx_outputs['<sos>']\n",
        "    target_seq_tensor = torch.from_numpy(target_seq.astype(np.int32))\n",
        "\n",
        "    # Se obtiene el indice que finaliza la inferencia\n",
        "    eos = word2idx_outputs['<eos>']\n",
        "\n",
        "    output_sentence = []\n",
        "    for _ in range(max_out_len):\n",
        "        # Predicción del próximo elemento\n",
        "        output, new_prev_state = model.decoder(target_seq_tensor.to(device), prev_state)\n",
        "        top1 = output.argmax(1).view(-1, 1)\n",
        "        idx = int(top1.cpu())\n",
        "\n",
        "        # Si es \"end of sentece <eos>\" se acaba\n",
        "        if eos == idx:\n",
        "            break\n",
        "\n",
        "        # Transformar ídx a palabra\n",
        "        word = ''\n",
        "        if idx > 0:\n",
        "            word = idx2word_target[idx]\n",
        "            output_sentence.append(word)\n",
        "\n",
        "        # Actualizar los estados dado la ultimo prediccion\n",
        "        prev_state = new_prev_state\n",
        "\n",
        "        # Actualizar secuencia de entrada con la salida (re-alimentacion)\n",
        "        target_seq_tensor = top1\n",
        "\n",
        "    return ' '.join(output_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LrxZnms0GLu"
      },
      "source": [
        "### Test 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHbAli6b2agv",
        "outputId": "e94e47c0-8619-40d9-e706-85590d1123a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "▪ Input: My mother say hi.\n",
            "▪ Representacion en vector de tokens de ids [36, 356, 113, 11]\n",
            "▪ Padding del vector: [[  0   0   0   0   0  36 356 113  11]]\n",
            "\n",
            "1. Etapa Encoder:\n",
            "▪ Index/token de salida: 3\n",
            "▪ Palabra de salida: i\n",
            "\n",
            "2. Etapa Decoder:\n",
            "▪ Response: i am a vegan\n"
          ]
        }
      ],
      "source": [
        "input_test = \"My mother say hi.\"\n",
        "print('▪ Input:', input_test)\n",
        "integer_seq_test = input_tokenizer.texts_to_sequences([input_test])[0]\n",
        "print(\"▪ Representacion en vector de tokens de ids\", integer_seq_test)\n",
        "encoder_sequence_test = pad_sequences([integer_seq_test], maxlen=max_input_len)\n",
        "print(\"▪ Padding del vector:\", encoder_sequence_test)\n",
        "\n",
        "# ENCODER STEP\n",
        "encoder_sequence_test_tensor = torch.from_numpy(encoder_sequence_test.astype(np.int32))\n",
        "\n",
        "# Se obtiene la salida del encoder (el estado oculto para el decoder)\n",
        "prev_state = model.encoder(encoder_sequence_test_tensor.to(device))\n",
        "\n",
        "# Se inicializa la secuencia de entrada al decoder como \"<sos>\"\n",
        "target_seq = np.zeros((1, 1))\n",
        "target_seq[0, 0] = word2idx_outputs['<sos>']\n",
        "target_seq_tensor = torch.from_numpy(target_seq.astype(np.int32))\n",
        "\n",
        "# Se obtiene la primera palabra de la secuencia de salida del decoder\n",
        "output, prev_state = model.decoder(target_seq_tensor.to(device), prev_state)\n",
        "\n",
        "print(\"\\n1. Etapa Encoder:\")\n",
        "top1 = output.argmax(1).view(-1, 1)\n",
        "idx = int(top1.cpu())\n",
        "print(\"▪ Index/token de salida:\", idx)\n",
        "\n",
        "word = idx2word_target[idx]\n",
        "print(\"▪ Palabra de salida:\", word)\n",
        "\n",
        "print(\"\\n2. Etapa Decoder:\")\n",
        "\n",
        "integer_seq_test = input_tokenizer.texts_to_sequences([input_test])[0]\n",
        "encoder_sequence_test = pad_sequences([integer_seq_test], maxlen=max_input_len)\n",
        "encoder_sequence_test_tensor = torch.from_numpy(encoder_sequence_test.astype(np.int32))\n",
        "\n",
        "translation = translate_sentence(encoder_sequence_test)\n",
        "print('▪ Response:', translation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZgJsNjS0xu1"
      },
      "source": [
        "### Test 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXxMjybI0xu2",
        "outputId": "2198db08-5271-4a2e-d08c-13ef7242a5e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "▪ Input: Your mother say bye.\n",
            "▪ Representacion en vector de tokens de ids [21, 356, 113, 58]\n",
            "▪ Padding del vector: [[  0   0   0   0   0  21 356 113  58]]\n",
            "\n",
            "1. Etapa Encoder:\n",
            "▪ Index/token de salida: 3\n",
            "▪ Palabra de salida: i\n",
            "\n",
            "2. Etapa Decoder:\n",
            "▪ Response: i am a vegan\n"
          ]
        }
      ],
      "source": [
        "input_test = \"Your mother say bye.\"\n",
        "print('▪ Input:', input_test)\n",
        "integer_seq_test = input_tokenizer.texts_to_sequences([input_test])[0]\n",
        "print(\"▪ Representacion en vector de tokens de ids\", integer_seq_test)\n",
        "encoder_sequence_test = pad_sequences([integer_seq_test], maxlen=max_input_len)\n",
        "print(\"▪ Padding del vector:\", encoder_sequence_test)\n",
        "\n",
        "# ENCODER STEP\n",
        "encoder_sequence_test_tensor = torch.from_numpy(encoder_sequence_test.astype(np.int32))\n",
        "\n",
        "# Se obtiene la salida del encoder (el estado oculto para el decoder)\n",
        "prev_state = model.encoder(encoder_sequence_test_tensor.to(device))\n",
        "\n",
        "# Se inicializa la secuencia de entrada al decoder como \"<sos>\"\n",
        "target_seq = np.zeros((1, 1))\n",
        "target_seq[0, 0] = word2idx_outputs['<sos>']\n",
        "target_seq_tensor = torch.from_numpy(target_seq.astype(np.int32))\n",
        "\n",
        "# Se obtiene la primera palabra de la secuencia de salida del decoder\n",
        "output, prev_state = model.decoder(target_seq_tensor.to(device), prev_state)\n",
        "\n",
        "print(\"\\n1. Etapa Encoder:\")\n",
        "top1 = output.argmax(1).view(-1, 1)\n",
        "idx = int(top1.cpu())\n",
        "print(\"▪ Index/token de salida:\", idx)\n",
        "\n",
        "word = idx2word_target[idx]\n",
        "print(\"▪ Palabra de salida:\", word)\n",
        "\n",
        "print(\"\\n2. Etapa Decoder:\")\n",
        "\n",
        "integer_seq_test = input_tokenizer.texts_to_sequences([input_test])[0]\n",
        "encoder_sequence_test = pad_sequences([integer_seq_test], maxlen=max_input_len)\n",
        "encoder_sequence_test_tensor = torch.from_numpy(encoder_sequence_test.astype(np.int32))\n",
        "\n",
        "translation = translate_sentence(encoder_sequence_test)\n",
        "print('▪ Response:', translation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpoxEbnH1IG8"
      },
      "source": [
        "### Test 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4EYrXuD1IG9",
        "outputId": "db0c9aa4-0a3d-4c90-d285-8abc2f65090b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "▪ Input: What do you want from me?\n",
            "▪ Representacion en vector de tokens de ids [4, 3, 2, 92, 39, 35]\n",
            "▪ Padding del vector: [[ 0  0  0  4  3  2 92 39 35]]\n",
            "\n",
            "1. Etapa Encoder:\n",
            "▪ Index/token de salida: 3\n",
            "▪ Palabra de salida: i\n",
            "\n",
            "2. Etapa Decoder:\n",
            "▪ Response: i am a vegan\n"
          ]
        }
      ],
      "source": [
        "input_test = \"What do you want from me?\"\n",
        "print('▪ Input:', input_test)\n",
        "integer_seq_test = input_tokenizer.texts_to_sequences([input_test])[0]\n",
        "print(\"▪ Representacion en vector de tokens de ids\", integer_seq_test)\n",
        "encoder_sequence_test = pad_sequences([integer_seq_test], maxlen=max_input_len)\n",
        "print(\"▪ Padding del vector:\", encoder_sequence_test)\n",
        "\n",
        "# ENCODER STEP\n",
        "encoder_sequence_test_tensor = torch.from_numpy(encoder_sequence_test.astype(np.int32))\n",
        "\n",
        "# Se obtiene la salida del encoder (el estado oculto para el decoder)\n",
        "prev_state = model.encoder(encoder_sequence_test_tensor.to(device))\n",
        "\n",
        "# Se inicializa la secuencia de entrada al decoder como \"<sos>\"\n",
        "target_seq = np.zeros((1, 1))\n",
        "target_seq[0, 0] = word2idx_outputs['<sos>']\n",
        "target_seq_tensor = torch.from_numpy(target_seq.astype(np.int32))\n",
        "\n",
        "# Se obtiene la primera palabra de la secuencia de salida del decoder\n",
        "output, prev_state = model.decoder(target_seq_tensor.to(device), prev_state)\n",
        "\n",
        "print(\"\\n1. Etapa Encoder:\")\n",
        "top1 = output.argmax(1).view(-1, 1)\n",
        "idx = int(top1.cpu())\n",
        "print(\"▪ Index/token de salida:\", idx)\n",
        "\n",
        "word = idx2word_target[idx]\n",
        "print(\"▪ Palabra de salida:\", word)\n",
        "\n",
        "print(\"\\n2. Etapa Decoder:\")\n",
        "\n",
        "integer_seq_test = input_tokenizer.texts_to_sequences([input_test])[0]\n",
        "encoder_sequence_test = pad_sequences([integer_seq_test], maxlen=max_input_len)\n",
        "encoder_sequence_test_tensor = torch.from_numpy(encoder_sequence_test.astype(np.int32))\n",
        "\n",
        "translation = translate_sentence(encoder_sequence_test)\n",
        "print('▪ Response:', translation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QIUJy4P1vkF"
      },
      "source": [
        "### Test 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prwZ3vK11R5L",
        "outputId": "2067fe70-86f5-47cf-bbd1-a7e9adbba8c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "▪ Input: my teek hu\n",
            "▪ Response: i am a vegan\n"
          ]
        }
      ],
      "source": [
        "i = np.random.choice(len(input_sentences))\n",
        "input_seq = encoder_input_sequences[i:i+1]\n",
        "encoder_sequence_test_tensor = torch.from_numpy(input_seq.astype(np.int32))\n",
        "translation = translate_sentence(encoder_sequence_test_tensor)\n",
        "print('▪ Input:', input_sentences[i])\n",
        "print('▪ Response:', translation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4J739D-c2agw",
        "outputId": "1a60b3a4-b186-4b27-d8d4-fe1ebd9ea60a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "▪ Input: so what \n",
            "▪ Response: i am a vegan \n",
            "\n",
            "▪ Input: do you have any pets \n",
            "▪ Response: i am a vegan \n",
            "\n",
            "▪ Input: you ask me my favorite color\n",
            "▪ Response: i am a vegan \n",
            "\n",
            "▪ Input: what is your favorite game \n",
            "▪ Response: i am a vegan \n",
            "\n",
            "▪ Input: i take photos of many things\n",
            "▪ Response: i am a vegan \n",
            "\n",
            "▪ Input: me to\n",
            "▪ Response: i am a vegan \n",
            "\n",
            "▪ Input: yes i m do you like bees \n",
            "▪ Response: i am a vegan \n",
            "\n",
            "▪ Input: what do you do for a living \n",
            "▪ Response: i am a vegan \n",
            "\n",
            "▪ Input: what is your favorite book \n",
            "▪ Response: i am a vegan \n",
            "\n",
            "▪ Input: ok\n",
            "▪ Response: i am a vegan \n",
            "\n",
            "▪ Input: hi how are you today\n",
            "▪ Response: i am a vegan \n",
            "\n",
            "▪ Input: nice\n",
            "▪ Response: i am a vegan \n",
            "\n",
            "▪ Input: what do you do for a living \n",
            "▪ Response: i am a vegan \n",
            "\n",
            "▪ Input: where it is\n",
            "▪ Response: i am a vegan \n",
            "\n",
            "▪ Input: what do you do for a living \n",
            "▪ Response: i am a vegan \n",
            "\n",
            "▪ Input: what do you do for a living \n",
            "▪ Response: i am a vegan \n",
            "\n",
            "▪ Input: currently nthg\n",
            "▪ Response: i am a vegan \n",
            "\n",
            "▪ Input: oh i see so you are a girl \n",
            "▪ Response: i am a vegan \n",
            "\n",
            "▪ Input: nice\n",
            "▪ Response: i am a vegan \n",
            "\n",
            "▪ Input: complain\n",
            "▪ Response: i am a vegan \n",
            "\n"
          ]
        }
      ],
      "source": [
        "for n in range(20):\n",
        "  i = np.random.choice(len(input_sentences))\n",
        "  input_seq = encoder_input_sequences[i:i+1]\n",
        "  encoder_sequence_test_tensor = torch.from_numpy(input_seq.astype(np.int32))\n",
        "  translation = translate_sentence(encoder_sequence_test_tensor)\n",
        "  print('▪ Input:', input_sentences[i])\n",
        "  print('▪ Response:', translation,\"\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv310",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
