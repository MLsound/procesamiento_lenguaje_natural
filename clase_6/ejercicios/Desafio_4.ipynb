{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc6qGGCW2agk"
      },
      "source": [
        "<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n",
        "\n",
        "# **Desafío 4:** *LSTM Bot QA*\n",
        "## Procesamiento del Lenguaje Natural I\n",
        "### Carrera de Especialización en Inteligencia Artificial - Cohorte 17\n",
        "##### Docentes: Rodrigo Cárdenas / Nicolás  Vattuone\n",
        "#### Autor: Alejandro Lloveras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJeNRwkG2agl"
      },
      "source": [
        "### Consigna\n",
        "- Construir un BOT para responder a preguntas del usuario (QA).\n",
        "- Utilizar datos disponibles del challenge **ConvAI2** *(Conversational Intelligence Challenge 2)* de conversaciones en inglés.\n",
        "- Preparar los embeddings para transformar los tokens de entrada en vectores\n",
        "- Entrenar un modelo basado en el esquema encoder-decoder utilizando los datos generados.\n",
        "- Experimentar el funcionamiento del modelo. Realizar la inferencia de los modelos por separado de encoder y decoder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqO0PRcFsPTe"
      },
      "source": [
        "### Datos\n",
        "El objecto es utilizar datos disponibles del challenge **ConvAI2** *(Conversational Intelligence Challenge 2)* de conversaciones en inglés. Se construirá un BOT para responder a preguntas del usuario (QA).\n",
        "\n",
        "[Dataset](http://convai.io/data/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoaoF6Zh2agm"
      },
      "source": [
        "### Inicialización"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bDFC0I3j9oFD"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --no-cache-dir gdown --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cq3YXak9sGHd"
      },
      "outputs": [],
      "source": [
        "# Librerías estándar\n",
        "import os\n",
        "import re\n",
        "import logging\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "from io import StringIO\n",
        "\n",
        "# Manipulación y Análisis de Datos\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# PyTorch Data Handling\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Preprocesamiento de Texto\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer, one_hot\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "\n",
        "# División de Datos\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Utilidades Keras/TensorFlow\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Construcción de Modelos y Capas Core (TensorFlow/Keras)\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Activation, Dropout, Dense, Embedding, Flatten, Input, LSTM, SimpleRNN\n",
        "from tensorflow.keras.models import Model, Sequential"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vELUyQ8bDHj6",
        "outputId": "7b0d4f61-4133-4c3b-8dff-59ca330b975d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cuda = torch.cuda.is_available()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-KZy5SyDHj6",
        "outputId": "a9afb7f3-ea31-4a87-bffc-283e065c3680"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ],
      "source": [
        "# torchsummar actualmente tiene un problema con las LSTM, por eso\n",
        "# se utiliza torchinfo, un fork del proyecto original con el bug solucionado\n",
        "!pip3 install torchinfo\n",
        "from torchinfo import summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torch_helpers\n",
            "  Downloading torch_helpers-0.4.6-py3-none-any.whl.metadata (598 bytes)\n",
            "Downloading torch_helpers-0.4.6-py3-none-any.whl (5.2 kB)\n",
            "Installing collected packages: torch_helpers\n",
            "Successfully installed torch_helpers-0.4.6\n",
            "Collecting moleskin\n",
            "  Downloading moleskin-1.5.1-py3-none-any.whl.metadata (701 bytes)\n",
            "Collecting boltons\n",
            "  Downloading boltons-25.0.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
            "INFO: pip is looking at multiple versions of moleskin to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting moleskin\n",
            "  Downloading moleskin-1.5.0-py3-none-any.whl.metadata (701 bytes)\n",
            "  Downloading moleskin-1.4.0-py3-none-any.whl.metadata (735 bytes)\n",
            "  Downloading moleskin-1.3.1-py3-none-any.whl.metadata (735 bytes)\n",
            "  Downloading moleskin-1.3.0-py3-none-any.whl.metadata (701 bytes)\n",
            "  Downloading moleskin-1.2.0-py3-none-any.whl.metadata (701 bytes)\n",
            "  Downloading moleskin-1.1.1-py3-none-any.whl.metadata (701 bytes)\n",
            "  Downloading moleskin-1.1.0-py3-none-any.whl.metadata (701 bytes)\n",
            "INFO: pip is still looking at multiple versions of moleskin to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading moleskin-1.0.0-py3-none-any.whl.metadata (678 bytes)\n",
            "  Downloading moleskin-0.1.1-py3-none-any.whl.metadata (678 bytes)\n",
            "  Downloading moleskin-0.1.0-py3-none-any.whl.metadata (678 bytes)\n",
            "  Downloading moleskin-0.0.8-py3-none-any.whl.metadata (590 bytes)\n",
            "  Downloading moleskin-0.0.7-py3-none-any.whl.metadata (573 bytes)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading moleskin-0.0.6-py3-none-any.whl.metadata (573 bytes)\n",
            "  Downloading moleskin-0.0.5-py3-none-any.whl.metadata (549 bytes)\n",
            "  Downloading moleskin-0.0.4-py3-none-any.whl.metadata (549 bytes)\n",
            "  Downloading moleskin-0.0.3-py3-none-any.whl.metadata (549 bytes)\n",
            "  Downloading moleskin-0.0.2-py3-none-any.whl.metadata (549 bytes)\n",
            "  Downloading moleskin-0.0.1-py3-none-any.whl.metadata (549 bytes)\n",
            "  Downloading moleskin-0.0.0-py3-none-any.whl.metadata (549 bytes)\n",
            "\u001b[31mERROR: Cannot install moleskin==0.0.0, moleskin==0.0.1, moleskin==0.0.2, moleskin==0.0.3, moleskin==0.0.4, moleskin==0.0.5, moleskin==0.0.6, moleskin==0.0.7, moleskin==0.0.8, moleskin==0.1.0, moleskin==0.1.1, moleskin==1.0.0, moleskin==1.1.0, moleskin==1.1.1, moleskin==1.2.0, moleskin==1.3.0, moleskin==1.3.1, moleskin==1.4.0, moleskin==1.5.0 and moleskin==1.5.1 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "The conflict is caused by:\n",
            "    moleskin 1.5.1 depends on pprint\n",
            "    moleskin 1.5.0 depends on pprint\n",
            "    moleskin 1.4.0 depends on pprint\n",
            "    moleskin 1.3.1 depends on pprint\n",
            "    moleskin 1.3.0 depends on pprint\n",
            "    moleskin 1.2.0 depends on pprint\n",
            "    moleskin 1.1.1 depends on pprint\n",
            "    moleskin 1.1.0 depends on pprint\n",
            "    moleskin 1.0.0 depends on pprint\n",
            "    moleskin 0.1.1 depends on pprint\n",
            "    moleskin 0.1.0 depends on pprint\n",
            "    moleskin 0.0.8 depends on pprint\n",
            "    moleskin 0.0.7 depends on pprint\n",
            "    moleskin 0.0.6 depends on pprint\n",
            "    moleskin 0.0.5 depends on pprint\n",
            "    moleskin 0.0.4 depends on pprint\n",
            "    moleskin 0.0.3 depends on pprint\n",
            "    moleskin 0.0.2 depends on pprint\n",
            "    moleskin 0.0.1 depends on pprint\n",
            "    moleskin 0.0.0 depends on pprint\n",
            "\n",
            "To fix this you could try to:\n",
            "1. loosen the range of package versions you've specified\n",
            "2. remove package versions to allow pip to attempt to solve the dependency conflict\n",
            "\n",
            "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install --no-deps torch_helpers\n",
        "!pip install moleskin boltons termcolor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGgRcirwDHj7",
        "outputId": "6c69ff80-1172-463f-d8df-c2793971ed1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-04-23 22:35:17--  http://torch_helpers.py/\n",
            "Resolving torch_helpers.py (torch_helpers.py)... failed: Name or service not known.\n",
            "wget: unable to resolve host address ‘torch_helpers.py’\n",
            "--2025-04-23 22:35:17--  https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/scripts/torch_helpers.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 23883 (23K) [text/plain]\n",
            "Saving to: ‘torch_helpers.py’\n",
            "\n",
            "torch_helpers.py    100%[===================>]  23.32K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2025-04-23 22:35:18 (29.6 MB/s) - ‘torch_helpers.py’ saved [23883/23883]\n",
            "\n",
            "FINISHED --2025-04-23 22:35:18--\n",
            "Total wall clock time: 0.6s\n",
            "Downloaded: 1 files, 23K in 0.001s (29.6 MB/s)\n"
          ]
        }
      ],
      "source": [
        "import platform\n",
        "\n",
        "if os.access('torch_helpers.py', os.F_OK) is False:\n",
        "    if platform.system() == 'Windows':\n",
        "        !curl !wget https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/scripts/torch_helpers.py > torch_helpers.py\n",
        "    else:\n",
        "        !wget torch_helpers.py https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/scripts/torch_helpers.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Funciones necesarias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zoRzdNBEDHj8"
      },
      "outputs": [],
      "source": [
        "def sequence_acc(y_pred, y_test):\n",
        "    y_pred_tag = y_pred.data.max(dim=-1,keepdim=True)[1]\n",
        "    y_test_tag = y_test.data.max(dim=-1,keepdim=True)[1]\n",
        "\n",
        "    batch_size = y_pred_tag.shape[0]\n",
        "    batch_acc = torch.zeros(batch_size)\n",
        "    for b in range(batch_size):\n",
        "        correct_results_sum = (y_pred_tag[b] == y_test_tag[b]).sum().float()\n",
        "        batch_acc[b] = correct_results_sum / y_pred_tag[b].shape[0]\n",
        "\n",
        "    correct_results_sum = batch_acc.sum().float()\n",
        "    acc = correct_results_sum / batch_size\n",
        "    return acc\n",
        "\n",
        "def train(model, train_loader, valid_loader, optimizer, criterion, epochs=100):\n",
        "    # Defino listas para realizar graficas de los resultados\n",
        "    train_loss = []\n",
        "    train_accuracy = []\n",
        "    valid_loss = []\n",
        "    valid_accuracy = []\n",
        "\n",
        "    # Defino mi loop de entrenamiento\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        epoch_train_loss = 0.0\n",
        "        epoch_train_accuracy = 0.0\n",
        "\n",
        "        for train_encoder_input, train_decoder_input, train_target in train_loader:\n",
        "            # Seteo los gradientes en cero ya que, por defecto, PyTorch los va acumulando\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = model(train_encoder_input.to(device), train_decoder_input.to(device))\n",
        "\n",
        "            # Computo el error de la salida comparando contra las etiquetas\n",
        "            # por cada token en cada batch (sequence_loss)\n",
        "            loss = 0\n",
        "            for t in range(train_decoder_input.shape[1]):\n",
        "                loss += criterion(output[:, t, :], train_target[:, t, :])\n",
        "\n",
        "            # Almaceno el error del batch para luego tener el error promedio de la epoca\n",
        "            epoch_train_loss += loss.item()\n",
        "\n",
        "            # Computo el nuevo set de gradientes a lo largo de toda la red\n",
        "            loss.backward()\n",
        "\n",
        "            # Realizo el paso de optimizacion actualizando los parametros de toda la red\n",
        "            optimizer.step()\n",
        "\n",
        "            # Calculo el accuracy del batch\n",
        "            accuracy = sequence_acc(output, train_target)\n",
        "            # Almaceno el accuracy del batch para luego tener el accuracy promedio de la epoca\n",
        "            epoch_train_accuracy += accuracy.item()\n",
        "\n",
        "        # Calculo la media de error para la epoca de entrenamiento.\n",
        "        # La longitud de train_loader es igual a la cantidad de batches dentro de una epoca.\n",
        "        epoch_train_loss = epoch_train_loss / len(train_loader)\n",
        "        train_loss.append(epoch_train_loss)\n",
        "        epoch_train_accuracy = epoch_train_accuracy / len(train_loader)\n",
        "        train_accuracy.append(epoch_train_accuracy)\n",
        "\n",
        "        # Realizo el paso de validación computando error y accuracy, y\n",
        "        # almacenando los valores para imprimirlos y graficarlos\n",
        "        #valid_encoder_input, valid_decoder_input, valid_target = iter(valid_loader).next()\n",
        "        valid_encoder_input, valid_decoder_input, valid_target = next(iter(valid_loader))\n",
        "        output = model(valid_encoder_input.to(device), valid_decoder_input.to(device))\n",
        "\n",
        "        epoch_valid_loss = 0\n",
        "        for t in range(train_decoder_input.shape[1]):\n",
        "                epoch_valid_loss += criterion(output[:, t, :], valid_target[:, t, :])\n",
        "        epoch_valid_loss = epoch_valid_loss.item()\n",
        "\n",
        "        valid_loss.append(epoch_valid_loss)\n",
        "\n",
        "        # Calculo el accuracy de la epoch\n",
        "        epoch_valid_accuracy = sequence_acc(output, valid_target).item()\n",
        "        valid_accuracy.append(epoch_valid_accuracy)\n",
        "\n",
        "        print(f\"Epoch: {epoch+1}/{epochs} - Train loss {epoch_train_loss:.3f} - Train accuracy {epoch_train_accuracy:.3f} - Valid Loss {epoch_valid_loss:.3f} - Valid accuracy {epoch_valid_accuracy:.3f}\")\n",
        "\n",
        "    history = {\n",
        "        \"loss\": train_loss,\n",
        "        \"accuracy\": train_accuracy,\n",
        "        \"val_loss\": valid_loss,\n",
        "        \"val_accuracy\": valid_accuracy,\n",
        "    }\n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "o_kn51y2JJpq"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, valid_loader, optimizer, criterion, epochs=100):\n",
        "    # Defino listas para realizar graficas de los resultados\n",
        "    train_loss = []\n",
        "    train_accuracy = []\n",
        "    valid_loss = []\n",
        "    valid_accuracy = []\n",
        "\n",
        "    # Defino mi loop de entrenamiento\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        epoch_train_loss = 0.0\n",
        "        epoch_train_accuracy = 0.0\n",
        "\n",
        "        # Inicia el bucle de entrenamiento por batch\n",
        "        for train_encoder_input, train_decoder_input, train_target in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = model(train_encoder_input.to(device), train_decoder_input.to(device))\n",
        "\n",
        "            # Computo el error del batch - INEFICIENTE BUCLE t\n",
        "            loss = 0\n",
        "            for t in range(train_decoder_input.shape[1]):\n",
        "                loss += criterion(output[:, t, :], train_target[:, t, :])\n",
        "\n",
        "            epoch_train_loss += loss.item() # Acumula pérdida por batch\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Calculo el accuracy del batch\n",
        "            accuracy = sequence_acc(output, train_target)\n",
        "            epoch_train_accuracy += accuracy.item() # Acumula accuracy por batch\n",
        "\n",
        "        # Calcula la media de metrics por epoca\n",
        "        epoch_train_loss = epoch_train_loss / len(train_loader)\n",
        "        train_loss.append(epoch_train_loss)\n",
        "        epoch_train_accuracy = epoch_train_accuracy / len(train_loader)\n",
        "        train_accuracy.append(epoch_train_accuracy)\n",
        "\n",
        "        # Realizo el paso de validación - ¡¡SOLO UN BATCH!!\n",
        "        #valid_encoder_input, valid_decoder_input, valid_target = iter(valid_loader).next()\n",
        "        valid_encoder_input, valid_decoder_input, valid_target = next(iter(valid_loader)) # <= PROBLEMA MAYOR\n",
        "\n",
        "        # Falta with torch.no_grad(): y model.eval()\n",
        "        output = model(valid_encoder_input.to(device), valid_decoder_input.to(device))\n",
        "\n",
        "        epoch_valid_loss = 0 # Acumula pérdida por batch\n",
        "        for t in range(train_decoder_input.shape[1]): # INEFICIENTE BUCLE t, y usa shape[1] de train_decoder_input\n",
        "                epoch_valid_loss += criterion(output[:, t, :], valid_target[:, t, :])\n",
        "        epoch_valid_loss = epoch_valid_loss.item() # Solo tiene la pérdida del ULTIMO batch procesado (solo 1 batch)\n",
        "\n",
        "        valid_loss.append(epoch_valid_loss) # Solo añade la pérdida de UN batch\n",
        "\n",
        "        # Calculo el accuracy de la epoch - ¡¡SOLO UN BATCH!!\n",
        "        epoch_valid_accuracy = sequence_acc(output, valid_target).item() # Solo tiene el accuracy de UN batch\n",
        "        valid_accuracy.append(epoch_valid_accuracy)\n",
        "\n",
        "        print(f\"Epoch: {epoch+1}/{epochs} - Train loss {epoch_train_loss:.3f} - Train accuracy {epoch_train_accuracy:.3f} - Valid Loss {epoch_valid_loss:.3f} - Valid accuracy {epoch_valid_accuracy:.3f}\")\n",
        "\n",
        "    history = {\n",
        "        \"loss\": train_loss,\n",
        "        \"accuracy\": train_accuracy,\n",
        "        \"val_loss\": valid_loss,\n",
        "        \"val_accuracy\": valid_accuracy,\n",
        "    }\n",
        "    return history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWA4sTXH2agn"
      },
      "source": [
        "## 1. Descarga del dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHNkUaPp6aYq",
        "outputId": "8191dfa3-a298-4a13-f96c-969e888b284c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1awUxYwImF84MIT5-jCaYAPe2QwSgS1hN&export=download\n",
            "To: /content/data_volunteers.json\n",
            "100%|██████████| 2.58M/2.58M [00:00<00:00, 85.9MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Descargar la carpeta de dataset\n",
        "import gdown\n",
        "if os.access('data_volunteers.json', os.F_OK) is False:\n",
        "    url = 'https://drive.google.com/uc?id=1awUxYwImF84MIT5-jCaYAPe2QwSgS1hN&export=download'\n",
        "    output = 'data_volunteers.json'\n",
        "    gdown.download(url, output, quiet=False)\n",
        "else:\n",
        "    print(\"✅ El dataset ya se encuentra descargado\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZy1-wgG-Rp7"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "text_file = \"data_volunteers.json\"\n",
        "with open(text_file) as f:\n",
        "    data = json.load(f) # la variable data será un diccionario"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ue5qd54S-eew",
        "outputId": "589c5fc7-75e4-4846-b0f2-0ce39cfc074b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['dialog', 'start_time', 'end_time', 'bot_profile', 'user_profile', 'eval_score', 'profile_match', 'participant1_id', 'participant2_id'])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Observamos los campos disponibles en cada linea del dataset\n",
        "data[0].keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHBRAXPl-3dz",
        "outputId": "0b929f5e-cff9-4b0b-f5f9-8b8ec7f290ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "▪ Cantidad de rows utilizadas: 6033\n"
          ]
        }
      ],
      "source": [
        "chat_in = []\n",
        "chat_out = []\n",
        "\n",
        "input_sentences = []\n",
        "output_sentences = []\n",
        "output_sentences_inputs = []\n",
        "max_len = 30\n",
        "\n",
        "def clean_text(txt):\n",
        "    txt = txt.lower()\n",
        "    txt.replace(\"\\'d\", \" had\")\n",
        "    txt.replace(\"\\'s\", \" is\")\n",
        "    txt.replace(\"\\'m\", \" am\")\n",
        "    txt.replace(\"don't\", \"do not\")\n",
        "    txt = re.sub(r'\\W+', ' ', txt)\n",
        "\n",
        "    return txt\n",
        "\n",
        "for line in data:\n",
        "    for i in range(len(line['dialog'])-1):\n",
        "        # vamos separando el texto en \"preguntas\" (chat_in)\n",
        "        # y \"respuestas\" (chat_out)\n",
        "        chat_in = clean_text(line['dialog'][i]['text'])\n",
        "        chat_out = clean_text(line['dialog'][i+1]['text'])\n",
        "\n",
        "        if len(chat_in) >= max_len or len(chat_out) >= max_len:\n",
        "            continue\n",
        "\n",
        "        input_sentence, output = chat_in, chat_out\n",
        "\n",
        "        # output sentence (decoder_output) tiene <eos>\n",
        "        output_sentence = output + ' <eos>'\n",
        "        # output sentence input (decoder_input) tiene <sos>\n",
        "        output_sentence_input = '<sos> ' + output\n",
        "\n",
        "        input_sentences.append(input_sentence)\n",
        "        output_sentences.append(output_sentence)\n",
        "        output_sentences_inputs.append(output_sentence_input)\n",
        "\n",
        "print(\"▪ Cantidad de rows utilizadas:\", len(input_sentences))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07L1qj8pC_l6",
        "outputId": "36567b2e-4049-46af-d1a8-4bd175798b6a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('hi how are you ', 'not bad and you  <eos>', '<sos> not bad and you ')"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_sentences[1], output_sentences[1], output_sentences_inputs[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8P-ynUNP5xp6"
      },
      "source": [
        "## 2. Preprocesamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxbAcfSp2agr"
      },
      "outputs": [],
      "source": [
        "# Definimos el tamaño máximo del vocabulario\n",
        "MAX_VOCAB_SIZE = 8000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24Ore_JP2agr"
      },
      "source": [
        "### 2.1 Datos de Entrada:\n",
        "Calculo de `word2idx_inputs`, `max_input_len`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80BpTOK02agr",
        "outputId": "c546f433-716a-4078-c26c-1b32a0a9aebf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "▪ Palabras en el vocabulario: 1799\n",
            "▪ Oración de entrada más larga: 9\n"
          ]
        }
      ],
      "source": [
        "# Tokenizamos las palabras con el Tokenizer de Keras\n",
        "# Definir una máxima cantidad de palabras a utilizar:\n",
        "# - num_words --> the maximum number of words to keep, based on word frequency.\n",
        "# - Only the most common num_words-1 words will be kept.\n",
        "input_tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
        "input_tokenizer.fit_on_texts(input_sentences)\n",
        "input_integer_seq = input_tokenizer.texts_to_sequences(input_sentences)\n",
        "\n",
        "word2idx_inputs = input_tokenizer.word_index\n",
        "print(\"▪ Palabras en el vocabulario:\", len(word2idx_inputs))\n",
        "\n",
        "max_input_len = max(len(sen) for sen in input_integer_seq)\n",
        "print(\"▪ Oración de entrada más larga:\", max_input_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70aDXToX2agr"
      },
      "source": [
        "### 2.2 Datos de Salida:\n",
        "Calculo de `word2idx_outputs`, `max_out_len`, `num_words_output`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnXtc5CR2agr",
        "outputId": "c996f3a0-ce65-4544-c1c6-865ea27b34bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "▪ Palabras en el vocabulario: 1806\n",
            "▪ Oración de salida más larga: 10\n"
          ]
        }
      ],
      "source": [
        "# A los filtros de símbolos del Tokenizer agregamos el \"¿\",\n",
        "# sacamos los \"<>\" para que no afectar nuestros tokens\n",
        "output_tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, filters='!\"#$%&()*+,-./:;=¿?@[\\\\]^_`{|}~\\t\\n')\n",
        "output_tokenizer.fit_on_texts([\"<sos>\", \"<eos>\"] + output_sentences)\n",
        "output_integer_seq = output_tokenizer.texts_to_sequences(output_sentences)\n",
        "output_input_integer_seq = output_tokenizer.texts_to_sequences(output_sentences_inputs)\n",
        "\n",
        "word2idx_outputs = output_tokenizer.word_index\n",
        "print(\"▪ Palabras en el vocabulario:\", len(word2idx_outputs))\n",
        "\n",
        "num_words_output = min(len(word2idx_outputs) + 1, MAX_VOCAB_SIZE) # Se suma 1 por el primer <sos>\n",
        "max_out_len = max(len(sen) for sen in output_integer_seq)\n",
        "print(\"▪ Oración de salida más larga:\", max_out_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzqQe4Dm2ags"
      },
      "source": [
        "### 2.3 Secuencias:\n",
        "Calculo de `encoder_input_sequences`, `decoder_output_sequences`, `decoder_targets`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AN6OHRr2ags",
        "outputId": "0ebcf725-00b9-474d-ba39-96b9f0239f89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "▪ Cantidad de filas del dataset: 6033\n",
            "▪ Forma de 'encoder_input_sequences': (6033, 9)\n",
            "▪ Forma de 'decoder_input_sequences': (6033, 10)\n"
          ]
        }
      ],
      "source": [
        "print(\"▪ Cantidad de filas del dataset:\", len(input_integer_seq))\n",
        "\n",
        "encoder_input_sequences = pad_sequences(input_integer_seq, maxlen=max_input_len)\n",
        "print(\"▪ Forma de 'encoder_input_sequences':\", encoder_input_sequences.shape)\n",
        "\n",
        "decoder_input_sequences = pad_sequences(output_input_integer_seq, maxlen=max_out_len, padding='post')\n",
        "print(\"▪ Forma de 'decoder_input_sequences':\", decoder_input_sequences.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAcrUePl3xZy",
        "outputId": "ed6876dd-714f-461b-c4b6-fce82af30bc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "▪ Forma de 'decoder_output_sequences': (6033, 10)\n"
          ]
        }
      ],
      "source": [
        "decoder_output_sequences = pad_sequences(output_integer_seq, maxlen=max_out_len, padding='post')\n",
        "print(\"▪ Forma de 'decoder_output_sequences':\", decoder_output_sequences.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KY4eluP2ags",
        "outputId": "79f30ac7-2a49-4fa5-9481-82de1289ca56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "▪ Tamaño de entrada del Encoder: 9\n",
            "▪ Tamaño de entrada del Decoder: 10\n",
            "▪ Dimensiones de salida:  1807\n"
          ]
        }
      ],
      "source": [
        "class Data(Dataset):\n",
        "    def __init__(self, encoder_input, decoder_input, decoder_output):\n",
        "        # Convertir los arrays de numpy a tensores.\n",
        "        # pytorch espera en general entradas 32bits\n",
        "        self.encoder_inputs = torch.from_numpy(encoder_input.astype(np.int32))\n",
        "        self.decoder_inputs = torch.from_numpy(decoder_input.astype(np.int32))\n",
        "        # Transformar los datos a oneHotEncoding\n",
        "        # la loss function esperan la salida float\n",
        "        self.decoder_outputs = F.one_hot(torch.from_numpy(decoder_output).to(torch.int64), num_classes=num_words_output).float()\n",
        "\n",
        "        self.len = self.decoder_outputs.shape[0]\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        return self.encoder_inputs[index], self.decoder_inputs[index], self.decoder_outputs[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "data_set = Data(encoder_input_sequences, decoder_input_sequences, decoder_output_sequences)\n",
        "\n",
        "encoder_input_size = data_set.encoder_inputs.shape[1]\n",
        "print(\"▪ Tamaño de entrada del Encoder:\", encoder_input_size)\n",
        "\n",
        "decoder_input_size = data_set.decoder_inputs.shape[1]\n",
        "print(\"▪ Tamaño de entrada del Decoder:\", decoder_input_size)\n",
        "\n",
        "output_dim = data_set.decoder_outputs.shape[2]\n",
        "print(\"▪ Dimensiones de salida: \", output_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaVCPDO32ags"
      },
      "source": [
        "Ahora, se realiza el split del dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Auc5dFdV2ags",
        "outputId": "f347159c-6beb-43c1-abf4-600ce8d25519"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "▪ Tamaño del conjunto de entrenamiento: 4827\n",
            "▪ Tamaño del conjunto de validacion: 1206\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "valid_set_size = int(data_set.len * 0.2)\n",
        "train_set_size = data_set.len - valid_set_size\n",
        "\n",
        "train_set = torch.utils.data.Subset(data_set, range(train_set_size))\n",
        "valid_set = torch.utils.data.Subset(data_set, range(train_set_size, data_set.len))\n",
        "\n",
        "print(\"▪ Tamaño del conjunto de entrenamiento:\", len(train_set))\n",
        "print(\"▪ Tamaño del conjunto de validacion:\", len(valid_set))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=128, shuffle=True)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=128, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CJIsLBbj6rg"
      },
      "source": [
        "## 3. Preparar los embeddings\n",
        "Se utilizan los embeddings de Glove (de 50 dim) para transformar los tokens de entrada en vectores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aO0bxZoZ2agt",
        "outputId": "dc9ce609-1fef-48c8-d6c3-0de86d1b8b24"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1wlDBOrxPq2-3htQ6ryVo7K1XnzLcfh4r&export=download\n",
            "From (redirected): https://drive.google.com/uc?id=1wlDBOrxPq2-3htQ6ryVo7K1XnzLcfh4r&export=download&confirm=t&uuid=72b64f62-4d6f-4228-9aec-d37c2023a033\n",
            "To: /content/gloveembedding.pkl\n",
            "100%|██████████| 525M/525M [00:05<00:00, 96.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Descargamos los embeddings desde un gogle drive (es la forma más rápida)\n",
        "# NOTA: No hay garantía de que estos links perduren, en caso de que no estén\n",
        "# disponibles descargar de la página oficial como se explica en el siguiente bloque\n",
        "if os.access('gloveembedding.pkl', os.F_OK) is False:\n",
        "    url = 'https://drive.google.com/uc?id=1wlDBOrxPq2-3htQ6ryVo7K1XnzLcfh4r&export=download'\n",
        "    output = 'gloveembedding.pkl'\n",
        "    gdown.download(url, output, quiet=False)\n",
        "else:\n",
        "    print(\"✅ Los embeddings gloveembedding.pkl ya están descargados\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_rY0YtQ2agt"
      },
      "outputs": [],
      "source": [
        "class WordsEmbeddings(object):\n",
        "    logger = logging.getLogger(__name__)\n",
        "\n",
        "    def __init__(self):\n",
        "        # load the embeddings\n",
        "        words_embedding_pkl = Path(self.PKL_PATH)\n",
        "        if not words_embedding_pkl.is_file():\n",
        "            words_embedding_txt = Path(self.WORD_TO_VEC_MODEL_TXT_PATH)\n",
        "            assert words_embedding_txt.is_file(), 'Words embedding not available'\n",
        "            embeddings = self.convert_model_to_pickle()\n",
        "        else:\n",
        "            embeddings = self.load_model_from_pickle()\n",
        "        self.embeddings = embeddings\n",
        "        # build the vocabulary hashmap\n",
        "        index = np.arange(self.embeddings.shape[0])\n",
        "        # Dicctionarios para traducir de embedding a IDX de la palabra\n",
        "        self.word2idx = dict(zip(self.embeddings['word'], index))\n",
        "        self.idx2word = dict(zip(index, self.embeddings['word']))\n",
        "\n",
        "    def get_words_embeddings(self, words):\n",
        "        words_idxs = self.words2idxs(words)\n",
        "        return self.embeddings[words_idxs]['embedding']\n",
        "\n",
        "    def words2idxs(self, words):\n",
        "        return np.array([self.word2idx.get(word, -1) for word in words])\n",
        "\n",
        "    def idxs2words(self, idxs):\n",
        "        return np.array([self.idx2word.get(idx, '-1') for idx in idxs])\n",
        "\n",
        "    def load_model_from_pickle(self):\n",
        "        self.logger.debug(\n",
        "            'loading words embeddings from pickle {}'.format(\n",
        "                self.PKL_PATH\n",
        "            )\n",
        "        )\n",
        "        max_bytes = 2**28 - 1 # 256MB\n",
        "        bytes_in = bytearray(0)\n",
        "        input_size = os.path.getsize(self.PKL_PATH)\n",
        "        with open(self.PKL_PATH, 'rb') as f_in:\n",
        "            for _ in range(0, input_size, max_bytes):\n",
        "                bytes_in += f_in.read(max_bytes)\n",
        "        embeddings = pickle.loads(bytes_in)\n",
        "        self.logger.debug('words embeddings loaded')\n",
        "        return embeddings\n",
        "\n",
        "    def convert_model_to_pickle(self):\n",
        "        # create a numpy strctured array:\n",
        "        # word     embedding\n",
        "        # U50      np.float32[]\n",
        "        # word_1   a, b, c\n",
        "        # word_2   d, e, f\n",
        "        # ...\n",
        "        # word_n   g, h, i\n",
        "        self.logger.debug(\n",
        "            'converting and loading words embeddings from text file {}'.format(\n",
        "                self.WORD_TO_VEC_MODEL_TXT_PATH\n",
        "            )\n",
        "        )\n",
        "        structure = [('word', np.dtype('U' + str(self.WORD_MAX_SIZE))),\n",
        "                     ('embedding', np.float32, (self.N_FEATURES,))]\n",
        "        structure = np.dtype(structure)\n",
        "        # load numpy array from disk using a generator\n",
        "        with open(self.WORD_TO_VEC_MODEL_TXT_PATH, encoding=\"utf8\") as words_embeddings_txt:\n",
        "            embeddings_gen = (\n",
        "                (line.split()[0], line.split()[1:]) for line in words_embeddings_txt\n",
        "                if len(line.split()[1:]) == self.N_FEATURES\n",
        "            )\n",
        "            embeddings = np.fromiter(embeddings_gen, structure)\n",
        "        # add a null embedding\n",
        "        null_embedding = np.array(\n",
        "            [('null_embedding', np.zeros((self.N_FEATURES,), dtype=np.float32))],\n",
        "            dtype=structure\n",
        "        )\n",
        "        embeddings = np.concatenate([embeddings, null_embedding])\n",
        "        # dump numpy array to disk using pickle\n",
        "        max_bytes = 2**28 - 1 # # 256MB\n",
        "        bytes_out = pickle.dumps(embeddings, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "        with open(self.PKL_PATH, 'wb') as f_out:\n",
        "            for idx in range(0, len(bytes_out), max_bytes):\n",
        "                f_out.write(bytes_out[idx:idx+max_bytes])\n",
        "        self.logger.debug('words embeddings loaded')\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class GloveEmbeddings(WordsEmbeddings):\n",
        "    WORD_TO_VEC_MODEL_TXT_PATH = 'glove.twitter.27B.50d.txt'\n",
        "    PKL_PATH = 'gloveembedding.pkl'\n",
        "    N_FEATURES = 50\n",
        "    WORD_MAX_SIZE = 60"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Ef486rMw2agt"
      },
      "outputs": [],
      "source": [
        "# Por una cuestion de RAM se utilizará los embeddings de Glove de dimension 50\n",
        "model_embeddings = GloveEmbeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7euJYgUw2agt",
        "outputId": "580e9456-f33a-4211-ff4c-4f6246fd8dbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preparando la matriz de embedding...\n",
            "▪ Cantidad de embeddings de palabras nulas: 38\n",
            "▪ Tamaño del vocabulario:  1799\n"
          ]
        }
      ],
      "source": [
        "# Creamos la Embedding matrix de las secuencias en ingles\n",
        "\n",
        "print('Preparando la matriz de embedding...')\n",
        "embed_dim = model_embeddings.N_FEATURES\n",
        "words_not_found = []\n",
        "\n",
        "# word_index provieen del tokenizer\n",
        "\n",
        "nb_words = min(MAX_VOCAB_SIZE, len(word2idx_inputs)) # vocab_size\n",
        "embedding_matrix = np.zeros((nb_words, embed_dim))\n",
        "for word, i in word2idx_inputs.items():\n",
        "    if i >= nb_words:\n",
        "        continue\n",
        "    embedding_vector = model_embeddings.get_words_embeddings(word)[0]\n",
        "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
        "\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    else:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        words_not_found.append(word)\n",
        "\n",
        "print('▪ Cantidad de embeddings de palabras nulas:', np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
        "print(\"▪ Tamaño del vocabulario: \", nb_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGV2djtL2agt",
        "outputId": "018cc352-822c-4141-843a-b57d97934646"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1799, 50)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Dimensión de los embeddings de la secuencia en ingles\n",
        "embedding_matrix.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vKbhjtIwPgM"
      },
      "source": [
        "## 4. Entrenamiento del modelo\n",
        "Se etrena un modelo basado con esquema encoder-decoder utilizando los datos generados en los puntos anteriores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntSLxTaM2agu"
      },
      "outputs": [],
      "source": [
        "lstm_size = 128\n",
        "num_layers = 1\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # num_embeddings = vocab_size, definido por le Tokenizador\n",
        "        # embedding_dim = 50 --> dimensión de los embeddings utilizados\n",
        "        self.lstm_size = lstm_size\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding_dim = embed_dim\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=self.embedding_dim, padding_idx=0)\n",
        "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
        "        self.embedding.weight.requires_grad = False  # marcar como layer no entrenable (freeze)\n",
        "        self.lstm = nn.LSTM(input_size=self.embedding_dim, hidden_size=self.lstm_size, batch_first=True,\n",
        "                            num_layers=self.num_layers) # LSTM layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.embedding(x)\n",
        "        lstm_output, (ht, ct) = self.lstm(out)\n",
        "        return (ht, ct)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, output_dim):\n",
        "        super().__init__()\n",
        "        # num_embeddings = vocab_size, definido por le Tokenizador\n",
        "        # embedding_dim = 50 --> dimensión de los embeddings utilizados\n",
        "        self.lstm_size = lstm_size\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding_dim = embed_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=self.embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(input_size=self.embedding_dim, hidden_size=self.lstm_size, batch_first=True,\n",
        "                            num_layers=self.num_layers) # LSTM layer\n",
        "        self.fc1 = nn.Linear(in_features=self.lstm_size, out_features=self.output_dim) # Fully connected layer\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=1) # normalize in dim 1\n",
        "\n",
        "    def forward(self, x, prev_state):\n",
        "        out = self.embedding(x)\n",
        "        lstm_output, (ht, ct) = self.lstm(out, prev_state)\n",
        "        out = self.softmax(self.fc1(lstm_output[:,-1,:])) # take last output (last seq)\n",
        "        return out, (ht, ct)\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "        assert encoder.lstm_size == decoder.lstm_size, \\\n",
        "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
        "        assert encoder.num_layers == decoder.num_layers, \\\n",
        "            \"Encoder and decoder must have equal number of layers!\"\n",
        "\n",
        "    def forward(self, encoder_input, decoder_input):\n",
        "        batch_size = decoder_input.shape[0]\n",
        "        decoder_input_len = decoder_input.shape[1]\n",
        "        vocab_size = self.decoder.output_dim\n",
        "\n",
        "        # tensor para almacenar la salida\n",
        "        # (batch_size, sentence_len, one_hot_size)\n",
        "        outputs = torch.zeros(batch_size, decoder_input_len, vocab_size)\n",
        "\n",
        "        # ultimo hidden state del encoder, primer estado oculto del decoder\n",
        "        prev_state = self.encoder(encoder_input)\n",
        "\n",
        "        # En la primera iteracion se toma el primer token de target (<sos>)\n",
        "        input = decoder_input[:, 0:1]\n",
        "\n",
        "        for t in range(decoder_input_len):\n",
        "            # t --> token index\n",
        "\n",
        "            # utilizamos método \"teacher forcing\", es decir que durante\n",
        "            # el entrenamiento no realimentamos la salida del decoder\n",
        "            # sino el token correcto que sigue en target\n",
        "            input = decoder_input[:, t:t+1]\n",
        "\n",
        "            # ingresar cada token embedding, uno por uno junto al hidden state\n",
        "            # recibir el output del decoder (softmax)\n",
        "            output, prev_state = self.decoder(input, prev_state)\n",
        "            top1 = output.argmax(1).view(-1, 1)\n",
        "\n",
        "            # Sino se usará \"teacher forcing\" habría que descomentar\n",
        "            # esta linea.\n",
        "            # Hay ejemplos dandos vuelta en donde se utilza un random\n",
        "            # para ver en cada vuelta que técnica se aplica\n",
        "            #input = top1\n",
        "\n",
        "            # guardar cada salida (softmax)\n",
        "            outputs[:, t, :] = output\n",
        "\n",
        "        return outputs\n",
        "\n",
        "encoder = Encoder(vocab_size=nb_words)\n",
        "if cuda: encoder.cuda()\n",
        "# decoder --> vocab_size == output_dim --> porque recibe y devuelve palabras en el mismo vocabulario\n",
        "decoder = Decoder(vocab_size=num_words_output, output_dim=num_words_output)\n",
        "if cuda: decoder.cuda()\n",
        "\n",
        "model = Seq2Seq(encoder, decoder)\n",
        "if cuda: model.cuda()\n",
        "\n",
        "# Crear el optimizador la una función de error\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = torch.nn.CrossEntropyLoss()  # Para clasificación multi categórica"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgGsCI-iDHkI",
        "outputId": "f2d6f8dd-c30f-4377-fbc2-fbfffaa38203"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "Seq2Seq                                  [1, 10, 1807]             --\n",
              "├─Encoder: 1-1                           [1, 1, 128]               --\n",
              "│    └─Embedding: 2-1                    [1, 9, 50]                (89,950)\n",
              "│    └─LSTM: 2-2                         [1, 9, 128]               92,160\n",
              "├─Decoder: 1-2                           [1, 1807]                 --\n",
              "│    └─Embedding: 2-3                    [1, 1, 50]                90,350\n",
              "│    └─LSTM: 2-4                         [1, 1, 128]               92,160\n",
              "│    └─Linear: 2-5                       [1, 1807]                 233,103\n",
              "│    └─Softmax: 2-6                      [1, 1807]                 --\n",
              "├─Decoder: 1-3                           [1, 1807]                 (recursive)\n",
              "│    └─Embedding: 2-7                    [1, 1, 50]                (recursive)\n",
              "│    └─LSTM: 2-8                         [1, 1, 128]               (recursive)\n",
              "│    └─Linear: 2-9                       [1, 1807]                 (recursive)\n",
              "│    └─Softmax: 2-10                     [1, 1807]                 --\n",
              "├─Decoder: 1-4                           [1, 1807]                 (recursive)\n",
              "│    └─Embedding: 2-11                   [1, 1, 50]                (recursive)\n",
              "│    └─LSTM: 2-12                        [1, 1, 128]               (recursive)\n",
              "│    └─Linear: 2-13                      [1, 1807]                 (recursive)\n",
              "│    └─Softmax: 2-14                     [1, 1807]                 --\n",
              "├─Decoder: 1-5                           [1, 1807]                 (recursive)\n",
              "│    └─Embedding: 2-15                   [1, 1, 50]                (recursive)\n",
              "│    └─LSTM: 2-16                        [1, 1, 128]               (recursive)\n",
              "│    └─Linear: 2-17                      [1, 1807]                 (recursive)\n",
              "│    └─Softmax: 2-18                     [1, 1807]                 --\n",
              "├─Decoder: 1-6                           [1, 1807]                 (recursive)\n",
              "│    └─Embedding: 2-19                   [1, 1, 50]                (recursive)\n",
              "│    └─LSTM: 2-20                        [1, 1, 128]               (recursive)\n",
              "│    └─Linear: 2-21                      [1, 1807]                 (recursive)\n",
              "│    └─Softmax: 2-22                     [1, 1807]                 --\n",
              "├─Decoder: 1-7                           [1, 1807]                 (recursive)\n",
              "│    └─Embedding: 2-23                   [1, 1, 50]                (recursive)\n",
              "│    └─LSTM: 2-24                        [1, 1, 128]               (recursive)\n",
              "│    └─Linear: 2-25                      [1, 1807]                 (recursive)\n",
              "│    └─Softmax: 2-26                     [1, 1807]                 --\n",
              "├─Decoder: 1-8                           [1, 1807]                 (recursive)\n",
              "│    └─Embedding: 2-27                   [1, 1, 50]                (recursive)\n",
              "│    └─LSTM: 2-28                        [1, 1, 128]               (recursive)\n",
              "│    └─Linear: 2-29                      [1, 1807]                 (recursive)\n",
              "│    └─Softmax: 2-30                     [1, 1807]                 --\n",
              "├─Decoder: 1-9                           [1, 1807]                 (recursive)\n",
              "│    └─Embedding: 2-31                   [1, 1, 50]                (recursive)\n",
              "│    └─LSTM: 2-32                        [1, 1, 128]               (recursive)\n",
              "│    └─Linear: 2-33                      [1, 1807]                 (recursive)\n",
              "│    └─Softmax: 2-34                     [1, 1807]                 --\n",
              "├─Decoder: 1-10                          [1, 1807]                 (recursive)\n",
              "│    └─Embedding: 2-35                   [1, 1, 50]                (recursive)\n",
              "│    └─LSTM: 2-36                        [1, 1, 128]               (recursive)\n",
              "│    └─Linear: 2-37                      [1, 1807]                 (recursive)\n",
              "│    └─Softmax: 2-38                     [1, 1807]                 --\n",
              "├─Decoder: 1-11                          [1, 1807]                 (recursive)\n",
              "│    └─Embedding: 2-39                   [1, 1, 50]                (recursive)\n",
              "│    └─LSTM: 2-40                        [1, 1, 128]               (recursive)\n",
              "│    └─Linear: 2-41                      [1, 1807]                 (recursive)\n",
              "│    └─Softmax: 2-42                     [1, 1807]                 --\n",
              "==========================================================================================\n",
              "Total params: 597,723\n",
              "Trainable params: 507,773\n",
              "Non-trainable params: 89,950\n",
              "Total mult-adds (Units.MEGABYTES): 5.08\n",
              "==========================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 0.17\n",
              "Params size (MB): 2.39\n",
              "Estimated Total Size (MB): 2.56\n",
              "=========================================================================================="
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Obtenemos la tupla para la primera muestra\n",
        "first_sample_data = data_set[0]\n",
        "\n",
        "# Extraemos las entradas del encoder y del decoder\n",
        "encoder_input_sample = first_sample_data[0]\n",
        "decoder_input_sample = first_sample_data[1]\n",
        "\n",
        "# Agregamos una dimensión de batch (summary usualmente espera un batch)\n",
        "# Usar .to(device) si tu modelo está en CUDA\n",
        "encoder_input_sample = encoder_input_sample.unsqueeze(0).to(device)\n",
        "decoder_input_sample = decoder_input_sample.unsqueeze(0).to(device)\n",
        "\n",
        "# Pasamos la tupla de entrada a summary\n",
        "summary(model, input_data=(encoder_input_sample, decoder_input_sample))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiRBQezZ2agu",
        "outputId": "7b1fa4a7-8f4c-4668-cdec-24dbd3fb2b11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1/500 - Train loss 69.758 - Train accuracy 0.525 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 2/500 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 3/500 - Train loss 69.761 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 4/500 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 5/500 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 6/500 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 7/500 - Train loss 69.760 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 8/500 - Train loss 69.761 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 9/500 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 10/500 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 11/500 - Train loss 69.760 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 12/500 - Train loss 69.759 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 13/500 - Train loss 69.761 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 14/500 - Train loss 69.761 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 15/500 - Train loss 69.760 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 16/500 - Train loss 69.761 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 17/500 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 18/500 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.776 - Valid accuracy 0.523\n",
            "Epoch: 19/500 - Train loss 69.759 - Train accuracy 0.524 - Valid Loss 69.746 - Valid accuracy 0.526\n",
            "Epoch: 20/500 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.757 - Valid accuracy 0.526\n",
            "Epoch: 21/500 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.772 - Valid accuracy 0.523\n",
            "Epoch: 22/500 - Train loss 69.764 - Train accuracy 0.524 - Valid Loss 69.758 - Valid accuracy 0.526\n",
            "Epoch: 23/500 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.754 - Valid accuracy 0.526\n",
            "Epoch: 24/500 - Train loss 69.761 - Train accuracy 0.524 - Valid Loss 69.760 - Valid accuracy 0.526\n",
            "Epoch: 25/500 - Train loss 69.761 - Train accuracy 0.524 - Valid Loss 69.755 - Valid accuracy 0.526\n",
            "Epoch: 26/500 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.760 - Valid accuracy 0.526\n",
            "Epoch: 27/500 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.763 - Valid accuracy 0.523\n",
            "Epoch: 28/500 - Train loss 69.760 - Train accuracy 0.524 - Valid Loss 69.764 - Valid accuracy 0.523\n",
            "Epoch: 29/500 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.758 - Valid accuracy 0.523\n",
            "Epoch: 30/500 - Train loss 69.761 - Train accuracy 0.524 - Valid Loss 69.747 - Valid accuracy 0.526\n",
            "Epoch: 31/500 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.743 - Valid accuracy 0.527\n",
            "Epoch: 32/500 - Train loss 69.759 - Train accuracy 0.524 - Valid Loss 69.749 - Valid accuracy 0.526\n",
            "Epoch: 33/500 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.751 - Valid accuracy 0.527\n",
            "Epoch: 34/500 - Train loss 69.761 - Train accuracy 0.524 - Valid Loss 69.760 - Valid accuracy 0.523\n",
            "Epoch: 35/500 - Train loss 69.761 - Train accuracy 0.524 - Valid Loss 69.750 - Valid accuracy 0.526\n",
            "Epoch: 36/500 - Train loss 69.760 - Train accuracy 0.524 - Valid Loss 69.751 - Valid accuracy 0.526\n",
            "Epoch: 37/500 - Train loss 69.760 - Train accuracy 0.524 - Valid Loss 69.752 - Valid accuracy 0.526\n",
            "Epoch: 38/500 - Train loss 69.760 - Train accuracy 0.524 - Valid Loss 69.747 - Valid accuracy 0.526\n",
            "Epoch: 39/500 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.754 - Valid accuracy 0.526\n",
            "Epoch: 40/500 - Train loss 69.761 - Train accuracy 0.524 - Valid Loss 69.748 - Valid accuracy 0.526\n",
            "Epoch: 41/500 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.754 - Valid accuracy 0.526\n",
            "Epoch: 42/500 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.754 - Valid accuracy 0.526\n",
            "Epoch: 43/500 - Train loss 69.761 - Train accuracy 0.524 - Valid Loss 69.759 - Valid accuracy 0.523\n",
            "Epoch: 44/500 - Train loss 69.761 - Train accuracy 0.524 - Valid Loss 69.754 - Valid accuracy 0.526\n",
            "Epoch: 45/500 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.748 - Valid accuracy 0.526\n",
            "Epoch: 46/500 - Train loss 69.761 - Train accuracy 0.524 - Valid Loss 69.757 - Valid accuracy 0.526\n",
            "Epoch: 47/500 - Train loss 69.764 - Train accuracy 0.524 - Valid Loss 69.752 - Valid accuracy 0.526\n",
            "Epoch: 48/500 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.759 - Valid accuracy 0.523\n",
            "Epoch: 49/500 - Train loss 69.759 - Train accuracy 0.524 - Valid Loss 69.748 - Valid accuracy 0.526\n",
            "Epoch: 50/500 - Train loss 69.761 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 51/500 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 52/500 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 53/500 - Train loss 69.761 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 54/500 - Train loss 69.761 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 55/500 - Train loss 69.761 - Train accuracy 0.524 - Valid Loss 69.771 - Valid accuracy 0.523\n",
            "Epoch: 56/500 - Train loss 69.760 - Train accuracy 0.524 - Valid Loss 69.748 - Valid accuracy 0.526\n",
            "Epoch: 57/500 - Train loss 69.759 - Train accuracy 0.524 - Valid Loss 69.768 - Valid accuracy 0.523\n",
            "Epoch: 58/500 - Train loss 69.761 - Train accuracy 0.524 - Valid Loss 69.761 - Valid accuracy 0.526\n",
            "Epoch: 59/500 - Train loss 69.758 - Train accuracy 0.524 - Valid Loss 69.761 - Valid accuracy 0.523\n",
            "Epoch: 60/500 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.767 - Valid accuracy 0.523\n",
            "Epoch: 61/500 - Train loss 69.761 - Train accuracy 0.524 - Valid Loss 69.767 - Valid accuracy 0.523\n",
            "Epoch: 62/500 - Train loss 69.759 - Train accuracy 0.524 - Valid Loss 69.765 - Valid accuracy 0.523\n",
            "Epoch: 63/500 - Train loss 69.761 - Train accuracy 0.524 - Valid Loss 69.765 - Valid accuracy 0.523\n",
            "Epoch: 64/500 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.767 - Valid accuracy 0.523\n",
            "Epoch: 65/500 - Train loss 69.786 - Train accuracy 0.523 - Valid Loss 69.749 - Valid accuracy 0.526\n",
            "Epoch: 66/500 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.751 - Valid accuracy 0.526\n",
            "Epoch: 67/500 - Train loss 69.760 - Train accuracy 0.524 - Valid Loss 69.775 - Valid accuracy 0.523\n",
            "Epoch: 68/500 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 69/500 - Train loss 69.761 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 70/500 - Train loss 69.760 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 71/500 - Train loss 69.761 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 72/500 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 73/500 - Train loss 69.760 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 74/500 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 75/500 - Train loss 69.759 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 76/500 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 77/500 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 78/500 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.777 - Valid accuracy 0.523\n",
            "Epoch: 79/500 - Train loss 69.761 - Train accuracy 0.524 - Valid Loss 69.776 - Valid accuracy 0.523\n",
            "Epoch: 80/500 - Train loss 69.764 - Train accuracy 0.524 - Valid Loss 69.775 - Valid accuracy 0.523\n",
            "Epoch: 81/500 - Train loss 69.761 - Train accuracy 0.524 - Valid Loss 69.774 - Valid accuracy 0.523\n",
            "Epoch: 82/500 - Train loss 69.761 - Train accuracy 0.524 - Valid Loss 69.775 - Valid accuracy 0.523\n",
            "Epoch: 83/500 - Train loss 69.761 - Train accuracy 0.524 - Valid Loss 69.774 - Valid accuracy 0.523\n",
            "Epoch: 84/500 - Train loss 69.761 - Train accuracy 0.524 - Valid Loss 69.774 - Valid accuracy 0.523\n",
            "Epoch: 85/500 - Train loss 69.760 - Train accuracy 0.524 - Valid Loss 69.773 - Valid accuracy 0.523\n",
            "Epoch: 86/500 - Train loss 69.761 - Train accuracy 0.524 - Valid Loss 69.773 - Valid accuracy 0.523\n",
            "Epoch: 87/500 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.774 - Valid accuracy 0.523\n",
            "Epoch: 88/500 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.773 - Valid accuracy 0.523\n",
            "Epoch: 89/500 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.769 - Valid accuracy 0.523\n",
            "Epoch: 90/500 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.764 - Valid accuracy 0.523\n",
            "Epoch: 91/500 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.750 - Valid accuracy 0.527\n",
            "Epoch: 92/500 - Train loss 69.761 - Train accuracy 0.524 - Valid Loss 69.753 - Valid accuracy 0.526\n",
            "Epoch: 93/500 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.752 - Valid accuracy 0.526\n",
            "Epoch: 94/500 - Train loss 69.760 - Train accuracy 0.524 - Valid Loss 69.751 - Valid accuracy 0.526\n",
            "Epoch: 95/500 - Train loss 69.760 - Train accuracy 0.524 - Valid Loss 69.750 - Valid accuracy 0.527\n",
            "Epoch: 96/500 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.752 - Valid accuracy 0.527\n",
            "Epoch: 97/500 - Train loss 69.760 - Train accuracy 0.524 - Valid Loss 69.756 - Valid accuracy 0.526\n",
            "Epoch: 98/500 - Train loss 69.759 - Train accuracy 0.524 - Valid Loss 69.757 - Valid accuracy 0.526\n",
            "Epoch: 99/500 - Train loss 69.760 - Train accuracy 0.524 - Valid Loss 69.757 - Valid accuracy 0.526\n",
            "Epoch: 100/500 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.752 - Valid accuracy 0.527\n",
            "Epoch: 101/500 - Train loss 69.760 - Train accuracy 0.524 - Valid Loss 69.754 - Valid accuracy 0.526\n",
            "Epoch: 102/500 - Train loss 69.761 - Train accuracy 0.524 - Valid Loss 69.753 - Valid accuracy 0.527\n",
            "Epoch: 103/500 - Train loss 69.764 - Train accuracy 0.524 - Valid Loss 69.753 - Valid accuracy 0.527\n",
            "Epoch: 104/500 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.752 - Valid accuracy 0.527\n",
            "Epoch: 105/500 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.752 - Valid accuracy 0.527\n",
            "Epoch: 106/500 - Train loss 69.761 - Train accuracy 0.524 - Valid Loss 69.752 - Valid accuracy 0.527\n",
            "Epoch: 107/500 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.753 - Valid accuracy 0.526\n",
            "Epoch: 108/500 - Train loss 69.761 - Train accuracy 0.524 - Valid Loss 69.753 - Valid accuracy 0.527\n",
            "Epoch: 109/500 - Train loss 69.760 - Train accuracy 0.524 - Valid Loss 69.752 - Valid accuracy 0.527\n",
            "Epoch: 110/500 - Train loss 69.759 - Train accuracy 0.524 - Valid Loss 69.752 - Valid accuracy 0.527\n",
            "Epoch: 111/500 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.753 - Valid accuracy 0.527\n",
            "Epoch: 112/500 - Train loss 69.761 - Train accuracy 0.524 - Valid Loss 69.752 - Valid accuracy 0.527\n",
            "Epoch: 113/500 - Train loss 69.761 - Train accuracy 0.524 - Valid Loss 69.753 - Valid accuracy 0.526\n",
            "Epoch: 114/500 - Train loss 69.760 - Train accuracy 0.524 - Valid Loss 69.753 - Valid accuracy 0.527\n",
            "Epoch: 115/500 - Train loss 69.758 - Train accuracy 0.525 - Valid Loss 69.753 - Valid accuracy 0.526\n",
            "Epoch: 116/500 - Train loss 69.761 - Train accuracy 0.524 - Valid Loss 69.752 - Valid accuracy 0.527\n",
            "Epoch: 117/500 - Train loss 69.761 - Train accuracy 0.524 - Valid Loss 69.752 - Valid accuracy 0.527\n",
            "Epoch: 118/500 - Train loss 69.758 - Train accuracy 0.525 - Valid Loss 69.753 - Valid accuracy 0.527\n",
            "Epoch: 119/500 - Train loss 69.761 - Train accuracy 0.524 - Valid Loss 69.753 - Valid accuracy 0.527\n",
            "Epoch: 120/500 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.753 - Valid accuracy 0.527\n",
            "Epoch: 121/500 - Train loss 69.761 - Train accuracy 0.524 - Valid Loss 69.753 - Valid accuracy 0.527\n",
            "Epoch: 122/500 - Train loss 69.761 - Train accuracy 0.524 - Valid Loss 69.753 - Valid accuracy 0.527\n",
            "Epoch: 123/500 - Train loss 69.760 - Train accuracy 0.524 - Valid Loss 69.753 - Valid accuracy 0.527\n",
            "Epoch: 124/500 - Train loss 69.761 - Train accuracy 0.524 - Valid Loss 69.749 - Valid accuracy 0.527\n",
            "Epoch: 125/500 - Train loss 69.761 - Train accuracy 0.524 - Valid Loss 69.754 - Valid accuracy 0.527\n",
            "Epoch: 126/500 - Train loss 69.761 - Train accuracy 0.524 - Valid Loss 69.756 - Valid accuracy 0.526\n",
            "Epoch: 127/500 - Train loss 69.760 - Train accuracy 0.524 - Valid Loss 69.754 - Valid accuracy 0.527\n",
            "Epoch: 128/500 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.756 - Valid accuracy 0.526\n",
            "Epoch: 129/500 - Train loss 69.760 - Train accuracy 0.524 - Valid Loss 69.754 - Valid accuracy 0.527\n",
            "Epoch: 130/500 - Train loss 69.761 - Train accuracy 0.524 - Valid Loss 69.755 - Valid accuracy 0.526\n",
            "Epoch: 131/500 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.754 - Valid accuracy 0.527\n",
            "Epoch: 132/500 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.754 - Valid accuracy 0.526\n",
            "Epoch: 133/500 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.754 - Valid accuracy 0.527\n",
            "Epoch: 134/500 - Train loss 69.761 - Train accuracy 0.524 - Valid Loss 69.756 - Valid accuracy 0.526\n",
            "Epoch: 135/500 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.755 - Valid accuracy 0.527\n",
            "Epoch: 136/500 - Train loss 69.757 - Train accuracy 0.525 - Valid Loss 69.755 - Valid accuracy 0.527\n",
            "Epoch: 137/500 - Train loss 69.761 - Train accuracy 0.524 - Valid Loss 69.755 - Valid accuracy 0.527\n",
            "Epoch: 138/500 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.755 - Valid accuracy 0.527\n",
            "Epoch: 139/500 - Train loss 69.761 - Train accuracy 0.524 - Valid Loss 69.755 - Valid accuracy 0.527\n",
            "Epoch: 140/500 - Train loss 69.761 - Train accuracy 0.524 - Valid Loss 69.755 - Valid accuracy 0.527\n",
            "Epoch: 141/500 - Train loss 69.761 - Train accuracy 0.524 - Valid Loss 69.755 - Valid accuracy 0.527\n",
            "Epoch: 142/500 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.756 - Valid accuracy 0.527\n",
            "Epoch: 143/500 - Train loss 69.760 - Train accuracy 0.524 - Valid Loss 69.757 - Valid accuracy 0.523\n",
            "Epoch: 144/500 - Train loss 69.761 - Train accuracy 0.524 - Valid Loss 69.757 - Valid accuracy 0.523\n",
            "Epoch: 145/500 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.758 - Valid accuracy 0.523\n",
            "Epoch: 146/500 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.758 - Valid accuracy 0.523\n",
            "Epoch: 147/500 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.758 - Valid accuracy 0.523\n",
            "Epoch: 148/500 - Train loss 69.761 - Train accuracy 0.524 - Valid Loss 69.759 - Valid accuracy 0.523\n",
            "Epoch: 149/500 - Train loss 69.760 - Train accuracy 0.524 - Valid Loss 69.759 - Valid accuracy 0.523\n",
            "Epoch: 150/500 - Train loss 69.760 - Train accuracy 0.524 - Valid Loss 69.758 - Valid accuracy 0.523\n",
            "Epoch: 151/500 - Train loss 69.764 - Train accuracy 0.524 - Valid Loss 69.759 - Valid accuracy 0.523\n",
            "Epoch: 152/500 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.759 - Valid accuracy 0.523\n",
            "Epoch: 153/500 - Train loss 69.759 - Train accuracy 0.524 - Valid Loss 69.759 - Valid accuracy 0.523\n",
            "Epoch: 154/500 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.759 - Valid accuracy 0.523\n",
            "Epoch: 155/500 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.759 - Valid accuracy 0.523\n",
            "Epoch: 156/500 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.758 - Valid accuracy 0.523\n",
            "Epoch: 157/500 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.761 - Valid accuracy 0.523\n",
            "Epoch: 158/500 - Train loss 69.761 - Train accuracy 0.524 - Valid Loss 69.759 - Valid accuracy 0.523\n",
            "Epoch: 159/500 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.758 - Valid accuracy 0.523\n",
            "Epoch: 160/500 - Train loss 69.764 - Train accuracy 0.524 - Valid Loss 69.759 - Valid accuracy 0.523\n",
            "Epoch: 161/500 - Train loss 69.760 - Train accuracy 0.524 - Valid Loss 69.758 - Valid accuracy 0.523\n",
            "Epoch: 162/500 - Train loss 69.760 - Train accuracy 0.524 - Valid Loss 69.758 - Valid accuracy 0.523\n",
            "Epoch: 163/500 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.758 - Valid accuracy 0.523\n",
            "Epoch: 164/500 - Train loss 69.760 - Train accuracy 0.524 - Valid Loss 69.758 - Valid accuracy 0.523\n",
            "Epoch: 165/500 - Train loss 69.759 - Train accuracy 0.524 - Valid Loss 69.759 - Valid accuracy 0.523\n",
            "Epoch: 166/500 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.758 - Valid accuracy 0.523\n",
            "Epoch: 167/500 - Train loss 69.761 - Train accuracy 0.524 - Valid Loss 69.760 - Valid accuracy 0.523\n",
            "Epoch: 168/500 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.757 - Valid accuracy 0.523\n",
            "Epoch: 169/500 - Train loss 69.759 - Train accuracy 0.524 - Valid Loss 69.757 - Valid accuracy 0.523\n",
            "Epoch: 170/500 - Train loss 69.760 - Train accuracy 0.524 - Valid Loss 69.757 - Valid accuracy 0.523\n",
            "Epoch: 171/500 - Train loss 69.765 - Train accuracy 0.524 - Valid Loss 69.758 - Valid accuracy 0.523\n",
            "Epoch: 172/500 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.757 - Valid accuracy 0.523\n",
            "Epoch: 173/500 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.760 - Valid accuracy 0.523\n",
            "Epoch: 174/500 - Train loss 69.762 - Train accuracy 0.524 - Valid Loss 69.757 - Valid accuracy 0.527\n",
            "Epoch: 175/500 - Train loss 69.763 - Train accuracy 0.524 - Valid Loss 69.755 - Valid accuracy 0.527\n",
            "Epoch: 176/500 - Train loss 69.754 - Train accuracy 0.525 - Valid Loss 69.735 - Valid accuracy 0.527\n",
            "Epoch: 177/500 - Train loss 69.744 - Train accuracy 0.526 - Valid Loss 69.738 - Valid accuracy 0.527\n",
            "Epoch: 178/500 - Train loss 69.746 - Train accuracy 0.526 - Valid Loss 69.739 - Valid accuracy 0.527\n",
            "Epoch: 179/500 - Train loss 69.742 - Train accuracy 0.526 - Valid Loss 69.764 - Valid accuracy 0.523\n",
            "Epoch: 180/500 - Train loss 69.742 - Train accuracy 0.526 - Valid Loss 69.750 - Valid accuracy 0.527\n",
            "Epoch: 181/500 - Train loss 69.744 - Train accuracy 0.526 - Valid Loss 69.765 - Valid accuracy 0.523\n",
            "Epoch: 182/500 - Train loss 69.740 - Train accuracy 0.526 - Valid Loss 69.748 - Valid accuracy 0.527\n",
            "Epoch: 183/500 - Train loss 69.741 - Train accuracy 0.526 - Valid Loss 69.769 - Valid accuracy 0.523\n",
            "Epoch: 184/500 - Train loss 69.581 - Train accuracy 0.543 - Valid Loss 69.488 - Valid accuracy 0.552\n",
            "Epoch: 185/500 - Train loss 69.325 - Train accuracy 0.568 - Valid Loss 69.340 - Valid accuracy 0.566\n",
            "Epoch: 186/500 - Train loss 68.940 - Train accuracy 0.607 - Valid Loss 68.469 - Valid accuracy 0.655\n",
            "Epoch: 187/500 - Train loss 68.298 - Train accuracy 0.671 - Valid Loss 68.332 - Valid accuracy 0.668\n",
            "Epoch: 188/500 - Train loss 68.238 - Train accuracy 0.677 - Valid Loss 68.342 - Valid accuracy 0.666\n",
            "Epoch: 189/500 - Train loss 68.161 - Train accuracy 0.685 - Valid Loss 68.253 - Valid accuracy 0.676\n",
            "Epoch: 190/500 - Train loss 68.091 - Train accuracy 0.692 - Valid Loss 68.273 - Valid accuracy 0.675\n",
            "Epoch: 191/500 - Train loss 68.077 - Train accuracy 0.693 - Valid Loss 68.277 - Valid accuracy 0.673\n",
            "Epoch: 192/500 - Train loss 68.066 - Train accuracy 0.694 - Valid Loss 68.282 - Valid accuracy 0.673\n",
            "Epoch: 193/500 - Train loss 68.063 - Train accuracy 0.694 - Valid Loss 68.277 - Valid accuracy 0.673\n",
            "Epoch: 194/500 - Train loss 68.050 - Train accuracy 0.696 - Valid Loss 68.281 - Valid accuracy 0.673\n",
            "Epoch: 195/500 - Train loss 68.044 - Train accuracy 0.696 - Valid Loss 68.278 - Valid accuracy 0.673\n",
            "Epoch: 196/500 - Train loss 68.040 - Train accuracy 0.697 - Valid Loss 68.281 - Valid accuracy 0.672\n",
            "Epoch: 197/500 - Train loss 68.037 - Train accuracy 0.697 - Valid Loss 68.280 - Valid accuracy 0.672\n",
            "Epoch: 198/500 - Train loss 68.040 - Train accuracy 0.697 - Valid Loss 68.284 - Valid accuracy 0.672\n",
            "Epoch: 199/500 - Train loss 68.033 - Train accuracy 0.697 - Valid Loss 68.281 - Valid accuracy 0.672\n",
            "Epoch: 200/500 - Train loss 68.030 - Train accuracy 0.698 - Valid Loss 68.287 - Valid accuracy 0.671\n",
            "Epoch: 201/500 - Train loss 68.028 - Train accuracy 0.698 - Valid Loss 68.297 - Valid accuracy 0.670\n",
            "Epoch: 202/500 - Train loss 68.027 - Train accuracy 0.698 - Valid Loss 68.295 - Valid accuracy 0.670\n",
            "Epoch: 203/500 - Train loss 68.025 - Train accuracy 0.698 - Valid Loss 68.299 - Valid accuracy 0.670\n",
            "Epoch: 204/500 - Train loss 68.023 - Train accuracy 0.698 - Valid Loss 68.300 - Valid accuracy 0.670\n",
            "Epoch: 205/500 - Train loss 68.022 - Train accuracy 0.698 - Valid Loss 68.296 - Valid accuracy 0.671\n",
            "Epoch: 206/500 - Train loss 68.019 - Train accuracy 0.699 - Valid Loss 68.295 - Valid accuracy 0.671\n",
            "Epoch: 207/500 - Train loss 68.019 - Train accuracy 0.699 - Valid Loss 68.285 - Valid accuracy 0.671\n",
            "Epoch: 208/500 - Train loss 68.007 - Train accuracy 0.700 - Valid Loss 68.280 - Valid accuracy 0.673\n",
            "Epoch: 209/500 - Train loss 68.008 - Train accuracy 0.700 - Valid Loss 68.283 - Valid accuracy 0.672\n",
            "Epoch: 210/500 - Train loss 68.007 - Train accuracy 0.700 - Valid Loss 68.278 - Valid accuracy 0.673\n",
            "Epoch: 211/500 - Train loss 68.009 - Train accuracy 0.700 - Valid Loss 68.277 - Valid accuracy 0.673\n",
            "Epoch: 212/500 - Train loss 68.003 - Train accuracy 0.700 - Valid Loss 68.267 - Valid accuracy 0.674\n",
            "Epoch: 213/500 - Train loss 68.005 - Train accuracy 0.700 - Valid Loss 68.281 - Valid accuracy 0.673\n",
            "Epoch: 214/500 - Train loss 67.980 - Train accuracy 0.703 - Valid Loss 68.236 - Valid accuracy 0.676\n",
            "Epoch: 215/500 - Train loss 67.970 - Train accuracy 0.704 - Valid Loss 68.225 - Valid accuracy 0.678\n",
            "Epoch: 216/500 - Train loss 67.971 - Train accuracy 0.703 - Valid Loss 68.224 - Valid accuracy 0.678\n",
            "Epoch: 217/500 - Train loss 67.969 - Train accuracy 0.704 - Valid Loss 68.220 - Valid accuracy 0.678\n",
            "Epoch: 218/500 - Train loss 67.968 - Train accuracy 0.704 - Valid Loss 68.220 - Valid accuracy 0.678\n",
            "Epoch: 219/500 - Train loss 67.968 - Train accuracy 0.704 - Valid Loss 68.213 - Valid accuracy 0.680\n",
            "Epoch: 220/500 - Train loss 67.966 - Train accuracy 0.704 - Valid Loss 68.218 - Valid accuracy 0.679\n",
            "Epoch: 221/500 - Train loss 67.964 - Train accuracy 0.704 - Valid Loss 68.207 - Valid accuracy 0.680\n",
            "Epoch: 222/500 - Train loss 67.962 - Train accuracy 0.704 - Valid Loss 68.214 - Valid accuracy 0.678\n",
            "Epoch: 223/500 - Train loss 67.961 - Train accuracy 0.705 - Valid Loss 68.210 - Valid accuracy 0.679\n",
            "Epoch: 224/500 - Train loss 67.961 - Train accuracy 0.704 - Valid Loss 68.208 - Valid accuracy 0.680\n",
            "Epoch: 225/500 - Train loss 67.963 - Train accuracy 0.704 - Valid Loss 68.208 - Valid accuracy 0.680\n",
            "Epoch: 226/500 - Train loss 67.960 - Train accuracy 0.705 - Valid Loss 68.208 - Valid accuracy 0.680\n",
            "Epoch: 227/500 - Train loss 67.959 - Train accuracy 0.705 - Valid Loss 68.205 - Valid accuracy 0.680\n",
            "Epoch: 228/500 - Train loss 67.959 - Train accuracy 0.705 - Valid Loss 68.208 - Valid accuracy 0.680\n",
            "Epoch: 229/500 - Train loss 67.954 - Train accuracy 0.705 - Valid Loss 68.171 - Valid accuracy 0.684\n",
            "Epoch: 230/500 - Train loss 67.895 - Train accuracy 0.711 - Valid Loss 68.157 - Valid accuracy 0.684\n",
            "Epoch: 231/500 - Train loss 67.890 - Train accuracy 0.712 - Valid Loss 68.150 - Valid accuracy 0.685\n",
            "Epoch: 232/500 - Train loss 67.830 - Train accuracy 0.718 - Valid Loss 68.069 - Valid accuracy 0.694\n",
            "Epoch: 233/500 - Train loss 67.801 - Train accuracy 0.720 - Valid Loss 68.062 - Valid accuracy 0.695\n",
            "Epoch: 234/500 - Train loss 67.795 - Train accuracy 0.721 - Valid Loss 68.065 - Valid accuracy 0.694\n",
            "Epoch: 235/500 - Train loss 67.791 - Train accuracy 0.722 - Valid Loss 68.074 - Valid accuracy 0.693\n",
            "Epoch: 236/500 - Train loss 67.789 - Train accuracy 0.722 - Valid Loss 68.081 - Valid accuracy 0.692\n",
            "Epoch: 237/500 - Train loss 67.790 - Train accuracy 0.722 - Valid Loss 68.095 - Valid accuracy 0.692\n",
            "Epoch: 238/500 - Train loss 67.786 - Train accuracy 0.722 - Valid Loss 68.089 - Valid accuracy 0.691\n",
            "Epoch: 239/500 - Train loss 67.784 - Train accuracy 0.722 - Valid Loss 68.084 - Valid accuracy 0.693\n",
            "Epoch: 240/500 - Train loss 67.774 - Train accuracy 0.723 - Valid Loss 68.068 - Valid accuracy 0.694\n",
            "Epoch: 241/500 - Train loss 67.759 - Train accuracy 0.725 - Valid Loss 68.035 - Valid accuracy 0.697\n",
            "Epoch: 242/500 - Train loss 67.751 - Train accuracy 0.726 - Valid Loss 68.033 - Valid accuracy 0.698\n",
            "Epoch: 243/500 - Train loss 67.737 - Train accuracy 0.727 - Valid Loss 67.956 - Valid accuracy 0.705\n",
            "Epoch: 244/500 - Train loss 67.665 - Train accuracy 0.734 - Valid Loss 67.956 - Valid accuracy 0.705\n",
            "Epoch: 245/500 - Train loss 67.663 - Train accuracy 0.734 - Valid Loss 67.953 - Valid accuracy 0.707\n",
            "Epoch: 246/500 - Train loss 67.659 - Train accuracy 0.735 - Valid Loss 67.958 - Valid accuracy 0.705\n",
            "Epoch: 247/500 - Train loss 67.659 - Train accuracy 0.735 - Valid Loss 67.967 - Valid accuracy 0.705\n",
            "Epoch: 248/500 - Train loss 67.656 - Train accuracy 0.735 - Valid Loss 67.967 - Valid accuracy 0.705\n",
            "Epoch: 249/500 - Train loss 67.653 - Train accuracy 0.735 - Valid Loss 67.963 - Valid accuracy 0.705\n",
            "Epoch: 250/500 - Train loss 67.650 - Train accuracy 0.736 - Valid Loss 67.960 - Valid accuracy 0.704\n",
            "Epoch: 251/500 - Train loss 67.650 - Train accuracy 0.736 - Valid Loss 67.962 - Valid accuracy 0.705\n",
            "Epoch: 252/500 - Train loss 67.648 - Train accuracy 0.736 - Valid Loss 67.964 - Valid accuracy 0.703\n",
            "Epoch: 253/500 - Train loss 67.648 - Train accuracy 0.736 - Valid Loss 67.963 - Valid accuracy 0.703\n",
            "Epoch: 254/500 - Train loss 67.646 - Train accuracy 0.736 - Valid Loss 67.960 - Valid accuracy 0.705\n",
            "Epoch: 255/500 - Train loss 67.646 - Train accuracy 0.736 - Valid Loss 67.960 - Valid accuracy 0.704\n",
            "Epoch: 256/500 - Train loss 67.644 - Train accuracy 0.736 - Valid Loss 67.956 - Valid accuracy 0.705\n",
            "Epoch: 257/500 - Train loss 67.641 - Train accuracy 0.736 - Valid Loss 67.957 - Valid accuracy 0.705\n",
            "Epoch: 258/500 - Train loss 67.641 - Train accuracy 0.736 - Valid Loss 67.965 - Valid accuracy 0.704\n",
            "Epoch: 259/500 - Train loss 67.639 - Train accuracy 0.737 - Valid Loss 67.955 - Valid accuracy 0.705\n",
            "Epoch: 260/500 - Train loss 67.638 - Train accuracy 0.737 - Valid Loss 67.954 - Valid accuracy 0.706\n",
            "Epoch: 261/500 - Train loss 67.639 - Train accuracy 0.737 - Valid Loss 67.965 - Valid accuracy 0.704\n",
            "Epoch: 262/500 - Train loss 67.635 - Train accuracy 0.737 - Valid Loss 67.960 - Valid accuracy 0.705\n",
            "Epoch: 263/500 - Train loss 67.633 - Train accuracy 0.737 - Valid Loss 67.967 - Valid accuracy 0.703\n",
            "Epoch: 264/500 - Train loss 67.632 - Train accuracy 0.737 - Valid Loss 67.960 - Valid accuracy 0.704\n",
            "Epoch: 265/500 - Train loss 67.635 - Train accuracy 0.737 - Valid Loss 67.954 - Valid accuracy 0.705\n",
            "Epoch: 266/500 - Train loss 67.632 - Train accuracy 0.737 - Valid Loss 67.947 - Valid accuracy 0.706\n",
            "Epoch: 267/500 - Train loss 67.632 - Train accuracy 0.737 - Valid Loss 67.962 - Valid accuracy 0.703\n",
            "Epoch: 268/500 - Train loss 67.633 - Train accuracy 0.737 - Valid Loss 67.950 - Valid accuracy 0.705\n",
            "Epoch: 269/500 - Train loss 67.631 - Train accuracy 0.737 - Valid Loss 67.940 - Valid accuracy 0.705\n",
            "Epoch: 270/500 - Train loss 67.628 - Train accuracy 0.738 - Valid Loss 67.922 - Valid accuracy 0.708\n",
            "Epoch: 271/500 - Train loss 67.621 - Train accuracy 0.738 - Valid Loss 67.933 - Valid accuracy 0.707\n",
            "Epoch: 272/500 - Train loss 67.615 - Train accuracy 0.739 - Valid Loss 67.922 - Valid accuracy 0.709\n",
            "Epoch: 273/500 - Train loss 67.609 - Train accuracy 0.740 - Valid Loss 67.926 - Valid accuracy 0.709\n",
            "Epoch: 274/500 - Train loss 67.610 - Train accuracy 0.740 - Valid Loss 67.936 - Valid accuracy 0.708\n",
            "Epoch: 275/500 - Train loss 67.609 - Train accuracy 0.740 - Valid Loss 67.924 - Valid accuracy 0.708\n",
            "Epoch: 276/500 - Train loss 67.607 - Train accuracy 0.740 - Valid Loss 67.928 - Valid accuracy 0.708\n",
            "Epoch: 277/500 - Train loss 67.606 - Train accuracy 0.740 - Valid Loss 67.924 - Valid accuracy 0.709\n",
            "Epoch: 278/500 - Train loss 67.602 - Train accuracy 0.740 - Valid Loss 67.917 - Valid accuracy 0.709\n",
            "Epoch: 279/500 - Train loss 67.602 - Train accuracy 0.740 - Valid Loss 67.922 - Valid accuracy 0.709\n",
            "Epoch: 280/500 - Train loss 67.603 - Train accuracy 0.740 - Valid Loss 67.918 - Valid accuracy 0.709\n",
            "Epoch: 281/500 - Train loss 67.599 - Train accuracy 0.741 - Valid Loss 67.935 - Valid accuracy 0.708\n",
            "Epoch: 282/500 - Train loss 67.596 - Train accuracy 0.741 - Valid Loss 67.930 - Valid accuracy 0.709\n",
            "Epoch: 283/500 - Train loss 67.597 - Train accuracy 0.741 - Valid Loss 67.934 - Valid accuracy 0.709\n",
            "Epoch: 284/500 - Train loss 67.597 - Train accuracy 0.741 - Valid Loss 67.935 - Valid accuracy 0.707\n",
            "Epoch: 285/500 - Train loss 67.597 - Train accuracy 0.741 - Valid Loss 67.933 - Valid accuracy 0.707\n",
            "Epoch: 286/500 - Train loss 67.595 - Train accuracy 0.741 - Valid Loss 67.929 - Valid accuracy 0.708\n",
            "Epoch: 287/500 - Train loss 67.594 - Train accuracy 0.741 - Valid Loss 67.930 - Valid accuracy 0.709\n",
            "Epoch: 288/500 - Train loss 67.592 - Train accuracy 0.741 - Valid Loss 67.912 - Valid accuracy 0.709\n",
            "Epoch: 289/500 - Train loss 67.590 - Train accuracy 0.741 - Valid Loss 67.922 - Valid accuracy 0.709\n",
            "Epoch: 290/500 - Train loss 67.591 - Train accuracy 0.741 - Valid Loss 67.914 - Valid accuracy 0.709\n",
            "Epoch: 291/500 - Train loss 67.588 - Train accuracy 0.742 - Valid Loss 67.904 - Valid accuracy 0.712\n",
            "Epoch: 292/500 - Train loss 67.590 - Train accuracy 0.742 - Valid Loss 67.918 - Valid accuracy 0.709\n",
            "Epoch: 293/500 - Train loss 67.584 - Train accuracy 0.742 - Valid Loss 67.896 - Valid accuracy 0.711\n",
            "Epoch: 294/500 - Train loss 67.580 - Train accuracy 0.742 - Valid Loss 67.903 - Valid accuracy 0.709\n",
            "Epoch: 295/500 - Train loss 67.576 - Train accuracy 0.743 - Valid Loss 67.894 - Valid accuracy 0.711\n",
            "Epoch: 296/500 - Train loss 67.578 - Train accuracy 0.743 - Valid Loss 67.900 - Valid accuracy 0.710\n",
            "Epoch: 297/500 - Train loss 67.577 - Train accuracy 0.743 - Valid Loss 67.900 - Valid accuracy 0.710\n",
            "Epoch: 298/500 - Train loss 67.578 - Train accuracy 0.743 - Valid Loss 67.898 - Valid accuracy 0.711\n",
            "Epoch: 299/500 - Train loss 67.576 - Train accuracy 0.743 - Valid Loss 67.881 - Valid accuracy 0.713\n",
            "Epoch: 300/500 - Train loss 67.575 - Train accuracy 0.743 - Valid Loss 67.888 - Valid accuracy 0.712\n",
            "Epoch: 301/500 - Train loss 67.573 - Train accuracy 0.743 - Valid Loss 67.870 - Valid accuracy 0.715\n",
            "Epoch: 302/500 - Train loss 67.570 - Train accuracy 0.743 - Valid Loss 67.871 - Valid accuracy 0.713\n",
            "Epoch: 303/500 - Train loss 67.572 - Train accuracy 0.743 - Valid Loss 67.873 - Valid accuracy 0.713\n",
            "Epoch: 304/500 - Train loss 67.569 - Train accuracy 0.744 - Valid Loss 67.882 - Valid accuracy 0.712\n",
            "Epoch: 305/500 - Train loss 67.568 - Train accuracy 0.744 - Valid Loss 67.884 - Valid accuracy 0.713\n",
            "Epoch: 306/500 - Train loss 67.565 - Train accuracy 0.744 - Valid Loss 67.893 - Valid accuracy 0.713\n",
            "Epoch: 307/500 - Train loss 67.564 - Train accuracy 0.744 - Valid Loss 67.888 - Valid accuracy 0.711\n",
            "Epoch: 308/500 - Train loss 67.563 - Train accuracy 0.744 - Valid Loss 67.898 - Valid accuracy 0.711\n",
            "Epoch: 309/500 - Train loss 67.560 - Train accuracy 0.744 - Valid Loss 67.894 - Valid accuracy 0.712\n",
            "Epoch: 310/500 - Train loss 67.560 - Train accuracy 0.745 - Valid Loss 67.895 - Valid accuracy 0.712\n",
            "Epoch: 311/500 - Train loss 67.560 - Train accuracy 0.745 - Valid Loss 67.899 - Valid accuracy 0.711\n",
            "Epoch: 312/500 - Train loss 67.559 - Train accuracy 0.745 - Valid Loss 67.900 - Valid accuracy 0.711\n",
            "Epoch: 313/500 - Train loss 67.557 - Train accuracy 0.745 - Valid Loss 67.876 - Valid accuracy 0.715\n",
            "Epoch: 314/500 - Train loss 67.556 - Train accuracy 0.745 - Valid Loss 67.908 - Valid accuracy 0.709\n",
            "Epoch: 315/500 - Train loss 67.554 - Train accuracy 0.745 - Valid Loss 67.895 - Valid accuracy 0.711\n",
            "Epoch: 316/500 - Train loss 67.550 - Train accuracy 0.746 - Valid Loss 67.883 - Valid accuracy 0.713\n",
            "Epoch: 317/500 - Train loss 67.547 - Train accuracy 0.746 - Valid Loss 67.890 - Valid accuracy 0.712\n",
            "Epoch: 318/500 - Train loss 67.546 - Train accuracy 0.746 - Valid Loss 67.903 - Valid accuracy 0.709\n",
            "Epoch: 319/500 - Train loss 67.546 - Train accuracy 0.746 - Valid Loss 67.891 - Valid accuracy 0.713\n",
            "Epoch: 320/500 - Train loss 67.544 - Train accuracy 0.746 - Valid Loss 67.880 - Valid accuracy 0.713\n",
            "Epoch: 321/500 - Train loss 67.545 - Train accuracy 0.746 - Valid Loss 67.886 - Valid accuracy 0.712\n",
            "Epoch: 322/500 - Train loss 67.538 - Train accuracy 0.747 - Valid Loss 67.887 - Valid accuracy 0.712\n",
            "Epoch: 323/500 - Train loss 67.537 - Train accuracy 0.747 - Valid Loss 67.867 - Valid accuracy 0.715\n",
            "Epoch: 324/500 - Train loss 67.534 - Train accuracy 0.747 - Valid Loss 67.887 - Valid accuracy 0.713\n",
            "Epoch: 325/500 - Train loss 67.534 - Train accuracy 0.747 - Valid Loss 67.885 - Valid accuracy 0.712\n",
            "Epoch: 326/500 - Train loss 67.517 - Train accuracy 0.749 - Valid Loss 67.810 - Valid accuracy 0.719\n",
            "Epoch: 327/500 - Train loss 67.494 - Train accuracy 0.751 - Valid Loss 67.808 - Valid accuracy 0.721\n",
            "Epoch: 328/500 - Train loss 67.488 - Train accuracy 0.752 - Valid Loss 67.845 - Valid accuracy 0.716\n",
            "Epoch: 329/500 - Train loss 67.487 - Train accuracy 0.752 - Valid Loss 67.829 - Valid accuracy 0.718\n",
            "Epoch: 330/500 - Train loss 67.484 - Train accuracy 0.752 - Valid Loss 67.790 - Valid accuracy 0.723\n",
            "Epoch: 331/500 - Train loss 67.487 - Train accuracy 0.752 - Valid Loss 67.798 - Valid accuracy 0.721\n",
            "Epoch: 332/500 - Train loss 67.481 - Train accuracy 0.752 - Valid Loss 67.797 - Valid accuracy 0.720\n",
            "Epoch: 333/500 - Train loss 67.481 - Train accuracy 0.753 - Valid Loss 67.821 - Valid accuracy 0.720\n",
            "Epoch: 334/500 - Train loss 67.477 - Train accuracy 0.753 - Valid Loss 67.820 - Valid accuracy 0.719\n",
            "Epoch: 335/500 - Train loss 67.475 - Train accuracy 0.753 - Valid Loss 67.816 - Valid accuracy 0.719\n",
            "Epoch: 336/500 - Train loss 67.474 - Train accuracy 0.753 - Valid Loss 67.806 - Valid accuracy 0.720\n",
            "Epoch: 337/500 - Train loss 67.473 - Train accuracy 0.753 - Valid Loss 67.816 - Valid accuracy 0.720\n",
            "Epoch: 338/500 - Train loss 67.472 - Train accuracy 0.753 - Valid Loss 67.844 - Valid accuracy 0.715\n",
            "Epoch: 339/500 - Train loss 67.473 - Train accuracy 0.753 - Valid Loss 67.857 - Valid accuracy 0.715\n",
            "Epoch: 340/500 - Train loss 67.473 - Train accuracy 0.753 - Valid Loss 67.825 - Valid accuracy 0.719\n",
            "Epoch: 341/500 - Train loss 67.470 - Train accuracy 0.754 - Valid Loss 67.807 - Valid accuracy 0.720\n",
            "Epoch: 342/500 - Train loss 67.470 - Train accuracy 0.754 - Valid Loss 67.818 - Valid accuracy 0.719\n",
            "Epoch: 343/500 - Train loss 67.468 - Train accuracy 0.754 - Valid Loss 67.830 - Valid accuracy 0.716\n",
            "Epoch: 344/500 - Train loss 67.468 - Train accuracy 0.754 - Valid Loss 67.822 - Valid accuracy 0.719\n",
            "Epoch: 345/500 - Train loss 67.467 - Train accuracy 0.754 - Valid Loss 67.824 - Valid accuracy 0.718\n",
            "Epoch: 346/500 - Train loss 67.463 - Train accuracy 0.754 - Valid Loss 67.807 - Valid accuracy 0.720\n",
            "Epoch: 347/500 - Train loss 67.465 - Train accuracy 0.754 - Valid Loss 67.821 - Valid accuracy 0.717\n",
            "Epoch: 348/500 - Train loss 67.466 - Train accuracy 0.754 - Valid Loss 67.814 - Valid accuracy 0.721\n",
            "Epoch: 349/500 - Train loss 67.463 - Train accuracy 0.754 - Valid Loss 67.800 - Valid accuracy 0.720\n",
            "Epoch: 350/500 - Train loss 67.462 - Train accuracy 0.754 - Valid Loss 67.820 - Valid accuracy 0.718\n",
            "Epoch: 351/500 - Train loss 67.453 - Train accuracy 0.755 - Valid Loss 67.801 - Valid accuracy 0.720\n",
            "Epoch: 352/500 - Train loss 67.451 - Train accuracy 0.755 - Valid Loss 67.794 - Valid accuracy 0.722\n",
            "Epoch: 353/500 - Train loss 67.447 - Train accuracy 0.756 - Valid Loss 67.813 - Valid accuracy 0.719\n",
            "Epoch: 354/500 - Train loss 67.449 - Train accuracy 0.756 - Valid Loss 67.812 - Valid accuracy 0.720\n",
            "Epoch: 355/500 - Train loss 67.449 - Train accuracy 0.756 - Valid Loss 67.795 - Valid accuracy 0.721\n",
            "Epoch: 356/500 - Train loss 67.445 - Train accuracy 0.756 - Valid Loss 67.779 - Valid accuracy 0.723\n",
            "Epoch: 357/500 - Train loss 67.443 - Train accuracy 0.756 - Valid Loss 67.796 - Valid accuracy 0.721\n",
            "Epoch: 358/500 - Train loss 67.443 - Train accuracy 0.756 - Valid Loss 67.788 - Valid accuracy 0.722\n",
            "Epoch: 359/500 - Train loss 67.439 - Train accuracy 0.757 - Valid Loss 67.792 - Valid accuracy 0.721\n",
            "Epoch: 360/500 - Train loss 67.436 - Train accuracy 0.757 - Valid Loss 67.788 - Valid accuracy 0.722\n",
            "Epoch: 361/500 - Train loss 67.439 - Train accuracy 0.757 - Valid Loss 67.797 - Valid accuracy 0.721\n",
            "Epoch: 362/500 - Train loss 67.437 - Train accuracy 0.757 - Valid Loss 67.789 - Valid accuracy 0.723\n",
            "Epoch: 363/500 - Train loss 67.436 - Train accuracy 0.757 - Valid Loss 67.792 - Valid accuracy 0.722\n",
            "Epoch: 364/500 - Train loss 67.435 - Train accuracy 0.757 - Valid Loss 67.800 - Valid accuracy 0.720\n",
            "Epoch: 365/500 - Train loss 67.432 - Train accuracy 0.757 - Valid Loss 67.784 - Valid accuracy 0.723\n",
            "Epoch: 366/500 - Train loss 67.436 - Train accuracy 0.757 - Valid Loss 67.780 - Valid accuracy 0.723\n",
            "Epoch: 367/500 - Train loss 67.433 - Train accuracy 0.757 - Valid Loss 67.785 - Valid accuracy 0.722\n",
            "Epoch: 368/500 - Train loss 67.435 - Train accuracy 0.757 - Valid Loss 67.801 - Valid accuracy 0.722\n",
            "Epoch: 369/500 - Train loss 67.431 - Train accuracy 0.757 - Valid Loss 67.785 - Valid accuracy 0.722\n",
            "Epoch: 370/500 - Train loss 67.431 - Train accuracy 0.757 - Valid Loss 67.793 - Valid accuracy 0.722\n",
            "Epoch: 371/500 - Train loss 67.432 - Train accuracy 0.757 - Valid Loss 67.802 - Valid accuracy 0.720\n",
            "Epoch: 372/500 - Train loss 67.431 - Train accuracy 0.757 - Valid Loss 67.775 - Valid accuracy 0.723\n",
            "Epoch: 373/500 - Train loss 67.432 - Train accuracy 0.757 - Valid Loss 67.778 - Valid accuracy 0.723\n",
            "Epoch: 374/500 - Train loss 67.423 - Train accuracy 0.758 - Valid Loss 67.732 - Valid accuracy 0.728\n",
            "Epoch: 375/500 - Train loss 67.405 - Train accuracy 0.760 - Valid Loss 67.712 - Valid accuracy 0.728\n",
            "Epoch: 376/500 - Train loss 67.401 - Train accuracy 0.761 - Valid Loss 67.757 - Valid accuracy 0.725\n",
            "Epoch: 377/500 - Train loss 67.398 - Train accuracy 0.761 - Valid Loss 67.697 - Valid accuracy 0.731\n",
            "Epoch: 378/500 - Train loss 67.397 - Train accuracy 0.761 - Valid Loss 67.689 - Valid accuracy 0.731\n",
            "Epoch: 379/500 - Train loss 67.398 - Train accuracy 0.761 - Valid Loss 67.699 - Valid accuracy 0.730\n",
            "Epoch: 380/500 - Train loss 67.395 - Train accuracy 0.761 - Valid Loss 67.715 - Valid accuracy 0.728\n",
            "Epoch: 381/500 - Train loss 67.390 - Train accuracy 0.762 - Valid Loss 67.716 - Valid accuracy 0.730\n",
            "Epoch: 382/500 - Train loss 67.390 - Train accuracy 0.762 - Valid Loss 67.739 - Valid accuracy 0.727\n",
            "Epoch: 383/500 - Train loss 67.383 - Train accuracy 0.762 - Valid Loss 67.734 - Valid accuracy 0.728\n",
            "Epoch: 384/500 - Train loss 67.384 - Train accuracy 0.762 - Valid Loss 67.715 - Valid accuracy 0.729\n",
            "Epoch: 385/500 - Train loss 67.381 - Train accuracy 0.763 - Valid Loss 67.723 - Valid accuracy 0.729\n",
            "Epoch: 386/500 - Train loss 67.376 - Train accuracy 0.763 - Valid Loss 67.732 - Valid accuracy 0.727\n",
            "Epoch: 387/500 - Train loss 67.376 - Train accuracy 0.763 - Valid Loss 67.725 - Valid accuracy 0.727\n",
            "Epoch: 388/500 - Train loss 67.374 - Train accuracy 0.763 - Valid Loss 67.725 - Valid accuracy 0.727\n",
            "Epoch: 389/500 - Train loss 67.370 - Train accuracy 0.764 - Valid Loss 67.748 - Valid accuracy 0.727\n",
            "Epoch: 390/500 - Train loss 67.368 - Train accuracy 0.764 - Valid Loss 67.756 - Valid accuracy 0.725\n",
            "Epoch: 391/500 - Train loss 67.365 - Train accuracy 0.764 - Valid Loss 67.738 - Valid accuracy 0.727\n",
            "Epoch: 392/500 - Train loss 67.366 - Train accuracy 0.764 - Valid Loss 67.731 - Valid accuracy 0.727\n",
            "Epoch: 393/500 - Train loss 67.362 - Train accuracy 0.764 - Valid Loss 67.726 - Valid accuracy 0.728\n",
            "Epoch: 394/500 - Train loss 67.363 - Train accuracy 0.764 - Valid Loss 67.745 - Valid accuracy 0.726\n",
            "Epoch: 395/500 - Train loss 67.360 - Train accuracy 0.765 - Valid Loss 67.753 - Valid accuracy 0.724\n",
            "Epoch: 396/500 - Train loss 67.359 - Train accuracy 0.765 - Valid Loss 67.748 - Valid accuracy 0.727\n",
            "Epoch: 397/500 - Train loss 67.359 - Train accuracy 0.764 - Valid Loss 67.743 - Valid accuracy 0.727\n",
            "Epoch: 398/500 - Train loss 67.357 - Train accuracy 0.765 - Valid Loss 67.745 - Valid accuracy 0.725\n",
            "Epoch: 399/500 - Train loss 67.357 - Train accuracy 0.765 - Valid Loss 67.738 - Valid accuracy 0.728\n",
            "Epoch: 400/500 - Train loss 67.355 - Train accuracy 0.765 - Valid Loss 67.743 - Valid accuracy 0.727\n",
            "Epoch: 401/500 - Train loss 67.355 - Train accuracy 0.765 - Valid Loss 67.733 - Valid accuracy 0.728\n",
            "Epoch: 402/500 - Train loss 67.354 - Train accuracy 0.765 - Valid Loss 67.737 - Valid accuracy 0.728\n",
            "Epoch: 403/500 - Train loss 67.354 - Train accuracy 0.765 - Valid Loss 67.740 - Valid accuracy 0.727\n",
            "Epoch: 404/500 - Train loss 67.354 - Train accuracy 0.765 - Valid Loss 67.780 - Valid accuracy 0.721\n",
            "Epoch: 405/500 - Train loss 67.352 - Train accuracy 0.765 - Valid Loss 67.765 - Valid accuracy 0.723\n",
            "Epoch: 406/500 - Train loss 67.353 - Train accuracy 0.765 - Valid Loss 67.759 - Valid accuracy 0.724\n",
            "Epoch: 407/500 - Train loss 67.352 - Train accuracy 0.765 - Valid Loss 67.764 - Valid accuracy 0.723\n",
            "Epoch: 408/500 - Train loss 67.351 - Train accuracy 0.765 - Valid Loss 67.744 - Valid accuracy 0.726\n",
            "Epoch: 409/500 - Train loss 67.350 - Train accuracy 0.766 - Valid Loss 67.757 - Valid accuracy 0.727\n",
            "Epoch: 410/500 - Train loss 67.348 - Train accuracy 0.766 - Valid Loss 67.752 - Valid accuracy 0.726\n",
            "Epoch: 411/500 - Train loss 67.350 - Train accuracy 0.766 - Valid Loss 67.751 - Valid accuracy 0.725\n",
            "Epoch: 412/500 - Train loss 67.350 - Train accuracy 0.766 - Valid Loss 67.757 - Valid accuracy 0.726\n",
            "Epoch: 413/500 - Train loss 67.349 - Train accuracy 0.766 - Valid Loss 67.746 - Valid accuracy 0.727\n",
            "Epoch: 414/500 - Train loss 67.348 - Train accuracy 0.766 - Valid Loss 67.749 - Valid accuracy 0.726\n",
            "Epoch: 415/500 - Train loss 67.350 - Train accuracy 0.766 - Valid Loss 67.742 - Valid accuracy 0.727\n",
            "Epoch: 416/500 - Train loss 67.349 - Train accuracy 0.766 - Valid Loss 67.737 - Valid accuracy 0.728\n",
            "Epoch: 417/500 - Train loss 67.352 - Train accuracy 0.765 - Valid Loss 67.717 - Valid accuracy 0.730\n",
            "Epoch: 418/500 - Train loss 67.350 - Train accuracy 0.766 - Valid Loss 67.745 - Valid accuracy 0.727\n",
            "Epoch: 419/500 - Train loss 67.351 - Train accuracy 0.765 - Valid Loss 67.740 - Valid accuracy 0.726\n",
            "Epoch: 420/500 - Train loss 67.349 - Train accuracy 0.766 - Valid Loss 67.752 - Valid accuracy 0.725\n",
            "Epoch: 421/500 - Train loss 67.350 - Train accuracy 0.766 - Valid Loss 67.705 - Valid accuracy 0.730\n",
            "Epoch: 422/500 - Train loss 67.349 - Train accuracy 0.766 - Valid Loss 67.742 - Valid accuracy 0.726\n",
            "Epoch: 423/500 - Train loss 67.353 - Train accuracy 0.765 - Valid Loss 67.759 - Valid accuracy 0.724\n",
            "Epoch: 424/500 - Train loss 67.355 - Train accuracy 0.765 - Valid Loss 67.720 - Valid accuracy 0.730\n",
            "Epoch: 425/500 - Train loss 67.351 - Train accuracy 0.766 - Valid Loss 67.710 - Valid accuracy 0.730\n",
            "Epoch: 426/500 - Train loss 67.354 - Train accuracy 0.766 - Valid Loss 67.744 - Valid accuracy 0.727\n",
            "Epoch: 427/500 - Train loss 67.352 - Train accuracy 0.766 - Valid Loss 67.693 - Valid accuracy 0.733\n",
            "Epoch: 428/500 - Train loss 67.348 - Train accuracy 0.766 - Valid Loss 67.694 - Valid accuracy 0.732\n",
            "Epoch: 429/500 - Train loss 67.341 - Train accuracy 0.767 - Valid Loss 67.719 - Valid accuracy 0.730\n",
            "Epoch: 430/500 - Train loss 67.339 - Train accuracy 0.767 - Valid Loss 67.734 - Valid accuracy 0.728\n",
            "Epoch: 431/500 - Train loss 67.334 - Train accuracy 0.767 - Valid Loss 67.687 - Valid accuracy 0.734\n",
            "Epoch: 432/500 - Train loss 67.335 - Train accuracy 0.767 - Valid Loss 67.737 - Valid accuracy 0.727\n",
            "Epoch: 433/500 - Train loss 67.331 - Train accuracy 0.767 - Valid Loss 67.719 - Valid accuracy 0.728\n",
            "Epoch: 434/500 - Train loss 67.329 - Train accuracy 0.768 - Valid Loss 67.717 - Valid accuracy 0.729\n",
            "Epoch: 435/500 - Train loss 67.330 - Train accuracy 0.768 - Valid Loss 67.715 - Valid accuracy 0.728\n",
            "Epoch: 436/500 - Train loss 67.330 - Train accuracy 0.768 - Valid Loss 67.703 - Valid accuracy 0.731\n",
            "Epoch: 437/500 - Train loss 67.331 - Train accuracy 0.767 - Valid Loss 67.730 - Valid accuracy 0.729\n",
            "Epoch: 438/500 - Train loss 67.329 - Train accuracy 0.768 - Valid Loss 67.712 - Valid accuracy 0.730\n",
            "Epoch: 439/500 - Train loss 67.328 - Train accuracy 0.768 - Valid Loss 67.734 - Valid accuracy 0.727\n",
            "Epoch: 440/500 - Train loss 67.324 - Train accuracy 0.768 - Valid Loss 67.753 - Valid accuracy 0.723\n",
            "Epoch: 441/500 - Train loss 67.319 - Train accuracy 0.769 - Valid Loss 67.716 - Valid accuracy 0.730\n",
            "Epoch: 442/500 - Train loss 67.320 - Train accuracy 0.769 - Valid Loss 67.709 - Valid accuracy 0.730\n",
            "Epoch: 443/500 - Train loss 67.320 - Train accuracy 0.769 - Valid Loss 67.725 - Valid accuracy 0.730\n",
            "Epoch: 444/500 - Train loss 67.319 - Train accuracy 0.769 - Valid Loss 67.729 - Valid accuracy 0.729\n",
            "Epoch: 445/500 - Train loss 67.317 - Train accuracy 0.769 - Valid Loss 67.749 - Valid accuracy 0.725\n",
            "Epoch: 446/500 - Train loss 67.317 - Train accuracy 0.769 - Valid Loss 67.731 - Valid accuracy 0.726\n",
            "Epoch: 447/500 - Train loss 67.317 - Train accuracy 0.769 - Valid Loss 67.745 - Valid accuracy 0.725\n",
            "Epoch: 448/500 - Train loss 67.316 - Train accuracy 0.769 - Valid Loss 67.715 - Valid accuracy 0.729\n",
            "Epoch: 449/500 - Train loss 67.313 - Train accuracy 0.769 - Valid Loss 67.711 - Valid accuracy 0.731\n",
            "Epoch: 450/500 - Train loss 67.310 - Train accuracy 0.770 - Valid Loss 67.737 - Valid accuracy 0.725\n",
            "Epoch: 451/500 - Train loss 67.309 - Train accuracy 0.770 - Valid Loss 67.736 - Valid accuracy 0.727\n",
            "Epoch: 452/500 - Train loss 67.308 - Train accuracy 0.770 - Valid Loss 67.714 - Valid accuracy 0.730\n",
            "Epoch: 453/500 - Train loss 67.307 - Train accuracy 0.770 - Valid Loss 67.669 - Valid accuracy 0.735\n",
            "Epoch: 454/500 - Train loss 67.308 - Train accuracy 0.770 - Valid Loss 67.696 - Valid accuracy 0.733\n",
            "Epoch: 455/500 - Train loss 67.307 - Train accuracy 0.770 - Valid Loss 67.697 - Valid accuracy 0.731\n",
            "Epoch: 456/500 - Train loss 67.308 - Train accuracy 0.770 - Valid Loss 67.693 - Valid accuracy 0.732\n",
            "Epoch: 457/500 - Train loss 67.304 - Train accuracy 0.770 - Valid Loss 67.719 - Valid accuracy 0.729\n",
            "Epoch: 458/500 - Train loss 67.303 - Train accuracy 0.770 - Valid Loss 67.683 - Valid accuracy 0.734\n",
            "Epoch: 459/500 - Train loss 67.304 - Train accuracy 0.770 - Valid Loss 67.689 - Valid accuracy 0.734\n",
            "Epoch: 460/500 - Train loss 67.301 - Train accuracy 0.770 - Valid Loss 67.697 - Valid accuracy 0.732\n",
            "Epoch: 461/500 - Train loss 67.301 - Train accuracy 0.770 - Valid Loss 67.713 - Valid accuracy 0.731\n",
            "Epoch: 462/500 - Train loss 67.301 - Train accuracy 0.770 - Valid Loss 67.696 - Valid accuracy 0.732\n",
            "Epoch: 463/500 - Train loss 67.300 - Train accuracy 0.771 - Valid Loss 67.709 - Valid accuracy 0.730\n",
            "Epoch: 464/500 - Train loss 67.298 - Train accuracy 0.771 - Valid Loss 67.705 - Valid accuracy 0.730\n",
            "Epoch: 465/500 - Train loss 67.297 - Train accuracy 0.771 - Valid Loss 67.708 - Valid accuracy 0.730\n",
            "Epoch: 466/500 - Train loss 67.296 - Train accuracy 0.771 - Valid Loss 67.712 - Valid accuracy 0.730\n",
            "Epoch: 467/500 - Train loss 67.298 - Train accuracy 0.771 - Valid Loss 67.703 - Valid accuracy 0.731\n",
            "Epoch: 468/500 - Train loss 67.296 - Train accuracy 0.771 - Valid Loss 67.715 - Valid accuracy 0.730\n",
            "Epoch: 469/500 - Train loss 67.296 - Train accuracy 0.771 - Valid Loss 67.702 - Valid accuracy 0.731\n",
            "Epoch: 470/500 - Train loss 67.294 - Train accuracy 0.771 - Valid Loss 67.691 - Valid accuracy 0.732\n",
            "Epoch: 471/500 - Train loss 67.296 - Train accuracy 0.771 - Valid Loss 67.697 - Valid accuracy 0.732\n",
            "Epoch: 472/500 - Train loss 67.294 - Train accuracy 0.771 - Valid Loss 67.703 - Valid accuracy 0.732\n",
            "Epoch: 473/500 - Train loss 67.293 - Train accuracy 0.771 - Valid Loss 67.695 - Valid accuracy 0.731\n",
            "Epoch: 474/500 - Train loss 67.293 - Train accuracy 0.771 - Valid Loss 67.711 - Valid accuracy 0.730\n",
            "Epoch: 475/500 - Train loss 67.294 - Train accuracy 0.771 - Valid Loss 67.707 - Valid accuracy 0.731\n",
            "Epoch: 476/500 - Train loss 67.292 - Train accuracy 0.771 - Valid Loss 67.699 - Valid accuracy 0.732\n",
            "Epoch: 477/500 - Train loss 67.293 - Train accuracy 0.771 - Valid Loss 67.707 - Valid accuracy 0.730\n",
            "Epoch: 478/500 - Train loss 67.293 - Train accuracy 0.771 - Valid Loss 67.703 - Valid accuracy 0.730\n",
            "Epoch: 479/500 - Train loss 67.293 - Train accuracy 0.771 - Valid Loss 67.712 - Valid accuracy 0.730\n",
            "Epoch: 480/500 - Train loss 67.296 - Train accuracy 0.771 - Valid Loss 67.708 - Valid accuracy 0.730\n",
            "Epoch: 481/500 - Train loss 67.300 - Train accuracy 0.771 - Valid Loss 67.693 - Valid accuracy 0.732\n",
            "Epoch: 482/500 - Train loss 67.309 - Train accuracy 0.770 - Valid Loss 67.666 - Valid accuracy 0.734\n",
            "Epoch: 483/500 - Train loss 67.312 - Train accuracy 0.770 - Valid Loss 67.690 - Valid accuracy 0.733\n",
            "Epoch: 484/500 - Train loss 67.315 - Train accuracy 0.769 - Valid Loss 67.758 - Valid accuracy 0.725\n",
            "Epoch: 485/500 - Train loss 67.313 - Train accuracy 0.770 - Valid Loss 67.717 - Valid accuracy 0.728\n",
            "Epoch: 486/500 - Train loss 67.316 - Train accuracy 0.769 - Valid Loss 67.726 - Valid accuracy 0.729\n",
            "Epoch: 487/500 - Train loss 67.313 - Train accuracy 0.769 - Valid Loss 67.709 - Valid accuracy 0.730\n",
            "Epoch: 488/500 - Train loss 67.310 - Train accuracy 0.770 - Valid Loss 67.752 - Valid accuracy 0.723\n",
            "Epoch: 489/500 - Train loss 67.306 - Train accuracy 0.770 - Valid Loss 67.740 - Valid accuracy 0.726\n",
            "Epoch: 490/500 - Train loss 67.305 - Train accuracy 0.770 - Valid Loss 67.695 - Valid accuracy 0.732\n",
            "Epoch: 491/500 - Train loss 67.300 - Train accuracy 0.771 - Valid Loss 67.710 - Valid accuracy 0.730\n",
            "Epoch: 492/500 - Train loss 67.296 - Train accuracy 0.771 - Valid Loss 67.698 - Valid accuracy 0.731\n",
            "Epoch: 493/500 - Train loss 67.289 - Train accuracy 0.772 - Valid Loss 67.695 - Valid accuracy 0.730\n",
            "Epoch: 494/500 - Train loss 67.290 - Train accuracy 0.772 - Valid Loss 67.675 - Valid accuracy 0.734\n",
            "Epoch: 495/500 - Train loss 67.288 - Train accuracy 0.772 - Valid Loss 67.670 - Valid accuracy 0.735\n",
            "Epoch: 496/500 - Train loss 67.284 - Train accuracy 0.772 - Valid Loss 67.669 - Valid accuracy 0.734\n",
            "Epoch: 497/500 - Train loss 67.285 - Train accuracy 0.772 - Valid Loss 67.699 - Valid accuracy 0.730\n",
            "Epoch: 498/500 - Train loss 67.278 - Train accuracy 0.773 - Valid Loss 67.710 - Valid accuracy 0.728\n",
            "Epoch: 499/500 - Train loss 67.272 - Train accuracy 0.773 - Valid Loss 67.714 - Valid accuracy 0.729\n",
            "Epoch: 500/500 - Train loss 67.273 - Train accuracy 0.773 - Valid Loss 67.691 - Valid accuracy 0.732\n"
          ]
        }
      ],
      "source": [
        "# Se entrena el modelo por 500 epochs\n",
        "history1 = train(model,\n",
        "                train_loader,\n",
        "                valid_loader,\n",
        "                optimizer,\n",
        "                criterion,\n",
        "                epochs=500\n",
        "                )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualizamos la curva de entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "1r0u-4VI2agu",
        "outputId": "a2e4a5c9-698a-4a50-a289-f3032b851c8a"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "epoch_count = range(1, len(history1['accuracy']) + 1)\n",
        "sns.lineplot(x=epoch_count,  y=history1['accuracy'], label='train')\n",
        "sns.lineplot(x=epoch_count,  y=history1['val_accuracy'], label='valid')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "No me queda del todo claro por qué tiene unas curvas tan extrañas. Se puede ver que hasta la epoch ~200 no comienza realmente a mejorar.\n",
        "\n",
        "Esto fue comprobado en múltiples reentrenamientos con distintos hiperparámetros."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LS4auqBHVDr"
      },
      "source": [
        "### Guardado de resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KoEALodGwYS",
        "outputId": "6abda0e9-3b2a-44e5-b53d-2ff8d8bd7393"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Pesos del modelo guardados en model_weights.pth\n"
          ]
        }
      ],
      "source": [
        "# Definimos una ruta para guardar los pesos\n",
        "filepath = 'model_weights.pth'\n",
        "\n",
        "# Guardamos el diccionario de estados\n",
        "torch.save(model.state_dict(), filepath)\n",
        "\n",
        "print(f\"✅ Pesos del modelo guardados en {filepath}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uO8l7mT4HH5D",
        "outputId": "3552fd68-9b16-4ad0-a2e1-9b19ce34ab3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Historial de entrenamiento guardado en training_history.csv\n"
          ]
        }
      ],
      "source": [
        "# Definimos la ruta para guardar el historial en un archivo CSV\n",
        "history_filepath = 'training_history.csv'\n",
        "\n",
        "# Creamos un DataFrame de pandas\n",
        "df_history = pd.DataFrame(history1)\n",
        "\n",
        "# Guardamos el DataFrame en un archivo CSV\n",
        "df_history.to_csv(history_filepath, index=False)\n",
        "\n",
        "print(f\"✅ Historial de entrenamiento guardado en {history_filepath}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zbwn0ekDy_s2"
      },
      "source": [
        "## 5. Inferencia\n",
        "Se experimenta el funcionamiento del modelo. Se realiza la inferencia de los modelos por separado de encoder y decoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4PV_ZRdy76H",
        "outputId": "d1239ef9-3a0a-4259-d8dc-1e61ac54dc10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Pesos del modelo cargados desde model_weights.pth\n"
          ]
        }
      ],
      "source": [
        "# Cargamos los pesos del modelo guardados\n",
        "model_weights_path = 'model_weights.pth'\n",
        "model.load_state_dict(torch.load(model_weights_path))\n",
        "\n",
        "print(f\"✅ Pesos del modelo cargados desde {model_weights_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "Tr0TK6522agu",
        "outputId": "80f65210-7566-4c24-de52-aeeae35341fb"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nStep 1:\\nA deal is a deal -> Encoder -> enc(h1,c1)\\n\\nenc(h1,c1) + <sos> -> Decoder -> Un + dec(h1,c1)\\n\\nstep 2:\\ndec(h1,c1) + Un -> Decoder -> trato + dec(h2,c2)\\n\\nstep 3:\\ndec(h2,c2) + trato -> Decoder -> es + dec(h3,c3)\\n\\nstep 4:\\ndec(h3,c3) + es -> Decoder -> un + dec(h4,c4)\\n\\nstep 5:\\ndec(h4,c4) + un -> Decoder -> trato + dec(h5,c5)\\n\\nstep 6:\\ndec(h5,c5) + trato. -> Decoder -> <eos> + dec(h6,c6)\\n'"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "Step 1:\n",
        "A deal is a deal -> Encoder -> enc(h1,c1)\n",
        "\n",
        "enc(h1,c1) + <sos> -> Decoder -> Un + dec(h1,c1)\n",
        "\n",
        "step 2:\n",
        "dec(h1,c1) + Un -> Decoder -> trato + dec(h2,c2)\n",
        "\n",
        "step 3:\n",
        "dec(h2,c2) + trato -> Decoder -> es + dec(h3,c3)\n",
        "\n",
        "step 4:\n",
        "dec(h3,c3) + es -> Decoder -> un + dec(h4,c4)\n",
        "\n",
        "step 5:\n",
        "dec(h4,c4) + un -> Decoder -> trato + dec(h5,c5)\n",
        "\n",
        "step 6:\n",
        "dec(h5,c5) + trato. -> Decoder -> <eos> + dec(h6,c6)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnS51WFF2agu"
      },
      "outputs": [],
      "source": [
        "# Armamos lo conversores de indice a palabra:\n",
        "idx2word_input = {v:k for k, v in word2idx_inputs.items()}\n",
        "idx2word_target = {v:k for k, v in word2idx_outputs.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "OUEdMsga2agv"
      },
      "outputs": [],
      "source": [
        "def translate_sentence(input_seq):\n",
        "    # Se transforma la sequencia de entrada a los stados \"h\" y \"c\" de la LSTM\n",
        "    # para enviar la primera vez al decoder\"\n",
        "    prev_state = model.encoder(encoder_sequence_test_tensor.to(device))\n",
        "\n",
        "    # Se inicializa la secuencia de entrada al decoder como \"<sos>\"\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = word2idx_outputs['<sos>']\n",
        "    target_seq_tensor = torch.from_numpy(target_seq.astype(np.int32))\n",
        "\n",
        "    # Se obtiene el indice que finaliza la inferencia\n",
        "    eos = word2idx_outputs['<eos>']\n",
        "\n",
        "    output_sentence = []\n",
        "    for _ in range(max_out_len):\n",
        "        # Predicción del próximo elemento\n",
        "        output, new_prev_state = model.decoder(target_seq_tensor.to(device), prev_state)\n",
        "        top1 = output.argmax(1).view(-1, 1)\n",
        "        idx = int(top1.cpu())\n",
        "\n",
        "        # Si es \"end of sentece <eos>\" se acaba\n",
        "        if eos == idx:\n",
        "            break\n",
        "\n",
        "        # Transformar ídx a palabra\n",
        "        word = ''\n",
        "        if idx > 0:\n",
        "            word = idx2word_target[idx]\n",
        "            output_sentence.append(word)\n",
        "\n",
        "        # Actualizar los estados dado la ultimo prediccion\n",
        "        prev_state = new_prev_state\n",
        "\n",
        "        # Actualizar secuencia de entrada con la salida (re-alimentacion)\n",
        "        target_seq_tensor = top1\n",
        "\n",
        "    return ' '.join(output_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LrxZnms0GLu"
      },
      "source": [
        "### Test 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHbAli6b2agv",
        "outputId": "99a481fe-7055-4831-ee8e-85684ecfe7fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "▪ Input: My mother say hi.\n",
            "▪ Representacion en vector de tokens de ids [36, 356, 113, 11]\n",
            "▪ Padding del vector: [[  0   0   0   0   0  36 356 113  11]]\n",
            "\n",
            "1. Etapa Encoder:\n",
            "▪ Index/token de salida: 3\n",
            "▪ Palabra de salida: i\n",
            "\n",
            "2. Etapa Decoder:\n",
            "▪ Response: i love to read\n"
          ]
        }
      ],
      "source": [
        "input_test = \"My mother say hi.\"\n",
        "print('▪ Input:', input_test)\n",
        "integer_seq_test = input_tokenizer.texts_to_sequences([input_test])[0]\n",
        "print(\"▪ Representacion en vector de tokens de ids\", integer_seq_test)\n",
        "encoder_sequence_test = pad_sequences([integer_seq_test], maxlen=max_input_len)\n",
        "print(\"▪ Padding del vector:\", encoder_sequence_test)\n",
        "\n",
        "# ENCODER STEP\n",
        "encoder_sequence_test_tensor = torch.from_numpy(encoder_sequence_test.astype(np.int32))\n",
        "\n",
        "# Se obtiene la salida del encoder (el estado oculto para el decoder)\n",
        "prev_state = model.encoder(encoder_sequence_test_tensor.to(device))\n",
        "\n",
        "# Se inicializa la secuencia de entrada al decoder como \"<sos>\"\n",
        "target_seq = np.zeros((1, 1))\n",
        "target_seq[0, 0] = word2idx_outputs['<sos>']\n",
        "target_seq_tensor = torch.from_numpy(target_seq.astype(np.int32))\n",
        "\n",
        "# Se obtiene la primera palabra de la secuencia de salida del decoder\n",
        "output, prev_state = model.decoder(target_seq_tensor.to(device), prev_state)\n",
        "\n",
        "print(\"\\n1. Etapa Encoder:\")\n",
        "top1 = output.argmax(1).view(-1, 1)\n",
        "idx = int(top1.cpu())\n",
        "print(\"▪ Index/token de salida:\", idx)\n",
        "\n",
        "word = idx2word_target[idx]\n",
        "print(\"▪ Palabra de salida:\", word)\n",
        "\n",
        "print(\"\\n2. Etapa Decoder:\")\n",
        "\n",
        "integer_seq_test = input_tokenizer.texts_to_sequences([input_test])[0]\n",
        "encoder_sequence_test = pad_sequences([integer_seq_test], maxlen=max_input_len)\n",
        "encoder_sequence_test_tensor = torch.from_numpy(encoder_sequence_test.astype(np.int32))\n",
        "\n",
        "translation = translate_sentence(encoder_sequence_test)\n",
        "print('▪ Response:', translation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZgJsNjS0xu1"
      },
      "source": [
        "### Test 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXxMjybI0xu2",
        "outputId": "0a9fa985-48e8-4c19-d1f0-5073d0401f93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "▪ Input: Your mother say bye.\n",
            "▪ Representacion en vector de tokens de ids [21, 356, 113, 58]\n",
            "▪ Padding del vector: [[  0   0   0   0   0  21 356 113  58]]\n",
            "\n",
            "1. Etapa Encoder:\n",
            "▪ Index/token de salida: 3\n",
            "▪ Palabra de salida: i\n",
            "\n",
            "2. Etapa Decoder:\n",
            "▪ Response: i like to read\n"
          ]
        }
      ],
      "source": [
        "input_test = \"Your mother say bye.\"\n",
        "print('▪ Input:', input_test)\n",
        "integer_seq_test = input_tokenizer.texts_to_sequences([input_test])[0]\n",
        "print(\"▪ Representacion en vector de tokens de ids\", integer_seq_test)\n",
        "encoder_sequence_test = pad_sequences([integer_seq_test], maxlen=max_input_len)\n",
        "print(\"▪ Padding del vector:\", encoder_sequence_test)\n",
        "\n",
        "# ENCODER STEP\n",
        "encoder_sequence_test_tensor = torch.from_numpy(encoder_sequence_test.astype(np.int32))\n",
        "\n",
        "# Se obtiene la salida del encoder (el estado oculto para el decoder)\n",
        "prev_state = model.encoder(encoder_sequence_test_tensor.to(device))\n",
        "\n",
        "# Se inicializa la secuencia de entrada al decoder como \"<sos>\"\n",
        "target_seq = np.zeros((1, 1))\n",
        "target_seq[0, 0] = word2idx_outputs['<sos>']\n",
        "target_seq_tensor = torch.from_numpy(target_seq.astype(np.int32))\n",
        "\n",
        "# Se obtiene la primera palabra de la secuencia de salida del decoder\n",
        "output, prev_state = model.decoder(target_seq_tensor.to(device), prev_state)\n",
        "\n",
        "print(\"\\n1. Etapa Encoder:\")\n",
        "top1 = output.argmax(1).view(-1, 1)\n",
        "idx = int(top1.cpu())\n",
        "print(\"▪ Index/token de salida:\", idx)\n",
        "\n",
        "word = idx2word_target[idx]\n",
        "print(\"▪ Palabra de salida:\", word)\n",
        "\n",
        "print(\"\\n2. Etapa Decoder:\")\n",
        "\n",
        "integer_seq_test = input_tokenizer.texts_to_sequences([input_test])[0]\n",
        "encoder_sequence_test = pad_sequences([integer_seq_test], maxlen=max_input_len)\n",
        "encoder_sequence_test_tensor = torch.from_numpy(encoder_sequence_test.astype(np.int32))\n",
        "\n",
        "translation = translate_sentence(encoder_sequence_test)\n",
        "print('▪ Response:', translation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpoxEbnH1IG8"
      },
      "source": [
        "### Test 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4EYrXuD1IG9",
        "outputId": "c009824e-54ea-4df4-e56f-51eb83c25759"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "▪ Input: What do you want from me?\n",
            "▪ Representacion en vector de tokens de ids [4, 3, 2, 92, 39, 35]\n",
            "▪ Padding del vector: [[ 0  0  0  4  3  2 92 39 35]]\n",
            "\n",
            "1. Etapa Encoder:\n",
            "▪ Index/token de salida: 3\n",
            "▪ Palabra de salida: i\n",
            "\n",
            "2. Etapa Decoder:\n",
            "▪ Response: i like to read\n"
          ]
        }
      ],
      "source": [
        "input_test = \"What do you want from me?\"\n",
        "print('▪ Input:', input_test)\n",
        "integer_seq_test = input_tokenizer.texts_to_sequences([input_test])[0]\n",
        "print(\"▪ Representacion en vector de tokens de ids\", integer_seq_test)\n",
        "encoder_sequence_test = pad_sequences([integer_seq_test], maxlen=max_input_len)\n",
        "print(\"▪ Padding del vector:\", encoder_sequence_test)\n",
        "\n",
        "# ENCODER STEP\n",
        "encoder_sequence_test_tensor = torch.from_numpy(encoder_sequence_test.astype(np.int32))\n",
        "\n",
        "# Se obtiene la salida del encoder (el estado oculto para el decoder)\n",
        "prev_state = model.encoder(encoder_sequence_test_tensor.to(device))\n",
        "\n",
        "# Se inicializa la secuencia de entrada al decoder como \"<sos>\"\n",
        "target_seq = np.zeros((1, 1))\n",
        "target_seq[0, 0] = word2idx_outputs['<sos>']\n",
        "target_seq_tensor = torch.from_numpy(target_seq.astype(np.int32))\n",
        "\n",
        "# Se obtiene la primera palabra de la secuencia de salida del decoder\n",
        "output, prev_state = model.decoder(target_seq_tensor.to(device), prev_state)\n",
        "\n",
        "print(\"\\n1. Etapa Encoder:\")\n",
        "top1 = output.argmax(1).view(-1, 1)\n",
        "idx = int(top1.cpu())\n",
        "print(\"▪ Index/token de salida:\", idx)\n",
        "\n",
        "word = idx2word_target[idx]\n",
        "print(\"▪ Palabra de salida:\", word)\n",
        "\n",
        "print(\"\\n2. Etapa Decoder:\")\n",
        "\n",
        "integer_seq_test = input_tokenizer.texts_to_sequences([input_test])[0]\n",
        "encoder_sequence_test = pad_sequences([integer_seq_test], maxlen=max_input_len)\n",
        "encoder_sequence_test_tensor = torch.from_numpy(encoder_sequence_test.astype(np.int32))\n",
        "\n",
        "translation = translate_sentence(encoder_sequence_test)\n",
        "print('▪ Response:', translation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QIUJy4P1vkF"
      },
      "source": [
        "### Test 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prwZ3vK11R5L",
        "outputId": "9173aab5-e6fe-47bf-de7c-625ba47c191c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "▪ Input: i am from ohio how about you \n",
            "▪ Response: i am good\n"
          ]
        }
      ],
      "source": [
        "i = np.random.choice(len(input_sentences))\n",
        "input_seq = encoder_input_sequences[i:i+1]\n",
        "encoder_sequence_test_tensor = torch.from_numpy(input_seq.astype(np.int32))\n",
        "translation = translate_sentence(encoder_sequence_test_tensor)\n",
        "print('▪ Input:', input_sentences[i])\n",
        "print('▪ Response:', translation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4J739D-c2agw",
        "outputId": "03d118a0-ccc2-4f3d-97b2-ede9a8246143"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "▪ Input: bye\n",
            "▪ Response: i love to read \n",
            "\n",
            "▪ Input: coffee is the best thing\n",
            "▪ Response: i like to \n",
            "\n",
            "▪ Input: how are you \n",
            "▪ Response: i am good \n",
            "\n",
            "▪ Input: what do you do for a living \n",
            "▪ Response: i am a girl i am a girl \n",
            "\n",
            "▪ Input: where from the book i issue \n",
            "▪ Response: i am a girl \n",
            "\n",
            "▪ Input:  \n",
            "▪ Response: i love to read \n",
            "\n",
            "▪ Input: what\n",
            "▪ Response: i love to read \n",
            "\n",
            "▪ Input: what do you do for a living \n",
            "▪ Response: i am a girl i am a girl \n",
            "\n",
            "▪ Input: i do not know what to say\n",
            "▪ Response: i love to read \n",
            "\n",
            "▪ Input: pursuing bba\n",
            "▪ Response: i like to read \n",
            "\n",
            "▪ Input: nice\n",
            "▪ Response: i love to read \n",
            "\n",
            "▪ Input: do you cook papayas_\n",
            "▪ Response: i like to read \n",
            "\n",
            "▪ Input: same for you\n",
            "▪ Response: i like to read \n",
            "\n",
            "▪ Input: fine \n",
            "▪ Response: i love to read \n",
            "\n",
            "▪ Input: what city are you from \n",
            "▪ Response: i am a girl i am a girl \n",
            "\n",
            "▪ Input: i have already told u\n",
            "▪ Response: i am a girl i am a girl \n",
            "\n",
            "▪ Input: how r you\n",
            "▪ Response: i am a girl i am a girl \n",
            "\n",
            "▪ Input: ntg\n",
            "▪ Response: i love to read \n",
            "\n",
            "▪ Input: what do you do for fun \n",
            "▪ Response: i am a girl \n",
            "\n",
            "▪ Input: do you have any hobbies \n",
            "▪ Response: i am a girl \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Se realizan 20 inferencias de modo aleatorio\n",
        "\n",
        "for n in range(20):\n",
        "  i = np.random.choice(len(input_sentences))\n",
        "  input_seq = encoder_input_sequences[i:i+1]\n",
        "  encoder_sequence_test_tensor = torch.from_numpy(input_seq.astype(np.int32))\n",
        "  translation = translate_sentence(encoder_sequence_test_tensor)\n",
        "  print('▪ Input:', input_sentences[i])\n",
        "  print('▪ Response:', translation,\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Análisis de resultados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Observaciones del resultado:\n",
        "- Se ha conseguido que el output sean oraciones coherentes *per se*, pero no así dentro del contexto del diálogo.\n",
        "- El resultado es bastante determinista, todas las oraciones comienzan con \"i\" y ofrece un pequeño repertorio de frases para múltiples inputs:\n",
        "    - \"i love to read\" o \"i like to read\"\n",
        "    - \"i am a girl\" o \"i am a girl i am a girl\"\n",
        "    - \"i am good\"\n",
        "\n",
        "En comparación con los otros modelos entrenados, el resultado es órdenes de magnitud mejor, dado que los otros modelos simplemente respondían siempre con una misma frase (por ejemplo, \"i am vegan\", \"i am\" o simplemente \"i i i i i i i i i i\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Contra-ejemplo hilarante"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Esta fue una de las pruebas realizadas con los mismmos hiperparámetros pero entrenando por el doble de épocas (1000 en total). El resultado fue claramente peor…"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "▪ Input: so what \n",
            "▪ Response: i am a vegan \n",
            "\n",
            "▪ Input: do you have any pets \n",
            "▪ Response: i am a vegan \n",
            "\n",
            "▪ Input: you ask me my favorite color\n",
            "▪ Response: i am a vegan \n",
            "\n",
            "▪ Input: what is your favorite game \n",
            "▪ Response: i am a vegan \n",
            "\n",
            "▪ Input: i take photos of many things\n",
            "▪ Response: i am a vegan \n",
            "\n",
            "▪ Input: me to\n",
            "▪ Response: i am a vegan \n",
            "\n",
            "▪ Input: yes i m do you like bees \n",
            "▪ Response: i am a vegan \n",
            "\n",
            "▪ Input: what do you do for a living \n",
            "▪ Response: i am a vegan \n",
            "\n",
            "▪ Input: what is your favorite book \n",
            "▪ Response: i am a vegan \n",
            "\n",
            "▪ Input: ok\n",
            "▪ Response: i am a vegan \n",
            "\n",
            "▪ Input: hi how are you today\n",
            "▪ Response: i am a vegan \n",
            "\n",
            "▪ Input: nice\n",
            "▪ Response: i am a vegan \n",
            "\n",
            "▪ Input: what do you do for a living \n",
            "▪ Response: i am a vegan \n",
            "\n",
            "▪ Input: where it is\n",
            "▪ Response: i am a vegan \n",
            "\n",
            "▪ Input: what do you do for a living \n",
            "▪ Response: i am a vegan \n",
            "\n",
            "▪ Input: what do you do for a living \n",
            "▪ Response: i am a vegan \n",
            "\n",
            "▪ Input: currently nthg\n",
            "▪ Response: i am a vegan \n",
            "\n",
            "▪ Input: oh i see so you are a girl \n",
            "▪ Response: i am a vegan \n",
            "\n",
            "▪ Input: nice\n",
            "▪ Response: i am a vegan \n",
            "\n",
            "▪ Input: complain\n",
            "▪ Response: i am a vegan \n",
            "\n"
          ]
        }
      ],
      "source": [
        "for n in range(20):\n",
        "  i = np.random.choice(len(input_sentences))\n",
        "  input_seq = encoder_input_sequences[i:i+1]\n",
        "  encoder_sequence_test_tensor = torch.from_numpy(input_seq.astype(np.int32))\n",
        "  translation = translate_sentence(encoder_sequence_test_tensor)\n",
        "  print('▪ Input:', input_sentences[i])\n",
        "  print('▪ Response:', translation,\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Detalle del entrenamiento\n",
        "[Ver Notebook](https://github.com/MLsound/procesamiento_lenguaje_natural/blob/main/clase_6/ejercicios/1000epochs/Desafio_4.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "epoch_count = range(1, len(history1['accuracy']) + 1)\n",
        "sns.lineplot(x=epoch_count,  y=history1['accuracy'], label='train')\n",
        "sns.lineplot(x=epoch_count,  y=history1['val_accuracy'], label='valid')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Detalle de hiperparámetros:\n",
        " - lstm_size = 128\n",
        " - num_layers = 1\n",
        " - Entrenado por 1000 epochs\n",
        " - GloveEmbeddings()\n",
        "    - N_FEATURES = 50\n",
        "    - WORD_MAX_SIZE = 60\n",
        "- MAX_VOCAB_SIZE = 8000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xNBT4KA2agw"
      },
      "source": [
        "# Conclusiones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9kZnJ7D2agw"
      },
      "source": [
        "El modelo presentado es el que alcanzó los mejores resultados entre todas las pruebas realizadas. Así y todo, como puede verse en las pruebas de inferencia el resultado es pobre.\n",
        "\n",
        "Entrenando sólo durante 50 epochs accuracy para train y val daban casi lo mismo y se movían muy poco entre épocas, evidenciando que el modelo no estaba aprendiendo realmente. Por eso fui incrementando progresivamente el número de epochs hasta llegar a 500, y luego a 1000.\n",
        "\n",
        "Se experimento además:\n",
        " - incrementando el learning rate sin llegar a mejores resultados.\n",
        " - implementando un scheduler para reducir el learning rate, intentnado evitar ese lapsus inicial.\n",
        " - incorporando dropout a la salida de las capas.\n",
        " - incrementando la cantidad de capas y el número neuronas.\n",
        " \n",
        " Los resultados emeporaban en todos los casos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La conclusión final luego de haber realizado los distintos proyectos para la materia, es que entrenar modelos de NLP que aporten resultados útiles a fines prácticos es una tarea compleja, que demanda muchos recursos de hardware (RAM, CPU y GPU) y es altamente sensible a la clidad de los datos de entrada. Si el dataset está sesgado o es imcompleto la calidad de los resultados decae abruptamente."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv310",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
